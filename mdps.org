#+TITLE: Markov Decision Processes
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+startup: beamer
#+LaTeX_CLASS: beamer
#+LaTeX_HEADER: \usepackage{algorithm,algorithmic}
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \usepackage{tikzsymbols}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \callcset[2] {\left\{#1 ~\middle|~ #2 \right\}}
#+LaTeX_HEADER: \newcommand \pol {\pi}
#+LaTeX_HEADER: \newcommand \Pols {\Pi}
#+LaTeX_HEADER: \newcommand \mdp {\mu}
#+LaTeX_HEADER: \newcommand \MDPs {\mathcal{M}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Bels {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \Unif {\textrm{Unif}}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Mult}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Dir {\textrm{Dir}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \Simplex {\mathbb{\Delta}}
#+LaTeX_HEADER: \newcommand \pn {\param^{(n)}}
#+LaTeX_HEADER: \newcommand \pnn {\param^{(n+1)}}
#+LaTeX_HEADER: \newcommand \pnp {\param^{(n-1)}}
#+LaTeX_HEADER: \newcommand{\indep}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}
#+LaTeX_HEADER: \usetikzlibrary{shapes.geometric}
#+LaTeX_HEADER: \usetikzlibrary{arrows.meta, positioning, quotes}
#+LaTeX_HEADER: \tikzstyle{utility}=[diamond,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=8mm]
#+LaTeX_HEADER: \tikzstyle{select}=[rectangle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{hidden}=[dashed,draw=black,fill=red!10]
#+LaTeX_HEADER: \tikzstyle{RV}=[circle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{con}=[rectangle,draw=white,fill=gray,inner sep=0mm, minimum size=6mm]
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* Markov decision processes
#+TOC: headlines [currentsection]
** The Markov process
*** Markov processes
[[./figures/Snakes_and_Ladders.jpeg]]
*** Markov Process
A sequence of random variables 
$s_1, s_2, \ldots, s_t$ is a *Markov process* if the nest variable $s_{t+1}$ only depends on the current value $s_t$
\[
P(s_{t+1} \mid s_{t}, \ldots, s_1) = P(s_{t+1} \mid s_{t}) \qquad \forall t
\]
#+BEAMER: \pause
- The variable $s_t \in S$ is the current *state* of the process.
- For finite $S$, the matrix $p_{i,j} \defn P(s_t = j \mid s_{t-1} = i)$ is called the *transition matrix*.
#+BEAMER: \pause
**** Bayesian network
It is sometimes called  *Markov chain* because of the structure formed by the RVs:
\begin{tikzpicture}
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \draw[->] (stp) -- (st);
  \draw[->] (st) -- (stn);
\end{tikzpicture}
#+BEAMER: \pause
**** Snakes and ladders
- What is the state?
- What is the transition matrix?

*** Markov Decision Process
**** Variables                                                        :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.4
     :END:
- The *state* $s_t \in S$.
- The *action* $a_t \in A$.
- The *reward* $r_t \in \Reals$.
#+BEAMER: \pause
**** Bayesian network                                                 :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.6
     :END:
\begin{tikzpicture}
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \node[select] at (0,2) (atp) {$a_{t-1}$};
  \node[select] at (2,2) (at) {$a_t$};
  \node[RV] at (2,1) (rtp) {$r_{t}$};
  \node[RV] at (4,1) (rt) {$r_{t+1}$};
  \draw[->] (atp) -- (st);
  \draw[->] (atp) -- (rtp);
  \draw[->] (stp) -- (rtp);
  \draw[->] (at) -- (rt);
  \draw[->] (st) -- (rt);
  \draw[->] (at) -- (stn);
  \draw[->] (stp) -- (st);
  \draw[->] (st) -- (stn);
\end{tikzpicture}

**** Markov Decision Process :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
A Markov decision process $\mdp$ on $(S, A)$ has the property that for any sequence of actions $a_1, \ldots$
\begin{align*}
P_\mdp(s_{t+1} \mid s_t, a_t, r_{t}, s_{t-1}, a_{t-1}, \ldots) = P_\mdp(s_{t+1} \mid s_t, a_t)
\\
P_\mdp(r_{t+1} \mid s_t, a_t, r_{t}, s_{t-1}, a_{t-1}, \ldots) = P_\mdp(r_{t+1} \mid s_t, a_t)
\end{align*}
The goal in a *finite-horizon* MDP is to maximise the $T$-horizon utility:
\[
U = \sum_{t=1}^T r_t
\]
* Backwards induction

** Utility and value functions
*** Utility and cost

