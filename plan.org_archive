#    -*- mode: org -*-


Archived entries from file /home/olethros/teaching/AI_BSc/plan.org


* Outline
  :PROPERTIES:
  :ARCHIVE_TIME: 2024-02-17 Sa 07:27
  :ARCHIVE_FILE: ~/teaching/AI_BSc/plan.org
  :ARCHIVE_CATEGORY: plan
  :END:
** AI agents and the environment
*** Decisions and learning
**** Decision making and interaction
- Recommend a route.
- Buy and sell stocks.
- Make a move in chess.
**** Learning.
- Decisions depend on information.
- How do we incorporate new information?
*** The view from statistics  
**** Optimal decisions
- Problem-dependent
- Require defining a cost- or utility function
- The optimal solution has the lowest cost or maximal utility

**** Optimisation: algorithms for optimal solutions
- Discrete optimisation.
- Linear optimisation.
- Non-linear optimisation.

**** Statistics and Machine Learning
- How to learn from data and interactions.
- Summarising knowledge into a model.
- Using the knowledge to make decisions.

*** Agent-environment interface
**** Agent
- Obtains stimuli/observations $x_t$
- Generates actions/decisions $a_t$
**** Environment
- Reacts to agent's actions
- Generates observations
**** The mind/body interface
- The body can be seen as part of the mind's environment
**** Policy and history
- The agent's next action $a_{t+1}$ depends on previous observation's and actions.
- The policy is implemented through an *algorithm*
*** Environment types
**** Observability
- Known/Unknown environment
- Fully/Partially observable
**** Agents
- Single/Multi
**** Randomness
- Deterministic/Stochastic
**** Temporal structure
- One-shot/Sequential
- Static/Dynamic
**** Variable structure
- Discrete/Continuous
*** Example environments
|-------------+---------+--------+------+-------+------|
| Environment | Observ. | Agents | Rand | Temp. | Var  |
|-------------+---------+--------+------+-------+------|
| Crossword   | Full    |      1 | Det  | One   | Disc |
| Chess       | Full    |      2 | Det  | Seq   | Disc |
|-------------+---------+--------+------+-------+------|
| Poker       | Partial |      n | Stoc | Seq   | Disc |
| Backgammon  | Full    |      2 | Stoc | Seq   | Disc |
|-------------+---------+--------+------+-------+------|
| Driving     | Partial |      n | Stoc | Seq   | Cont |
| Diagnosis   | Partial |      1 | Stoc | Seq   | Cont |
|-------------+---------+--------+------+-------+------|
| Image Rec.  | Partial |      1 | Det  | One   | Cont |
| Robot Chef. | Partial |      1 | Stoc | Seq   | Cont |
|-------------+---------+--------+------+-------+------|

*** Example policies
**** Reactive maze policy
- Ordered actions $A = \{\textrm{Up}, \textrm{Right}, \textrm{Down}, \textrm{Left}\}$
- Take action $a_{t+1} = a_t$ unless there is a wall in front.
- If there is a wall, take the next action, $a_{t+1} = a_t + 1$.

**** Problems with this policy
- Can it solve any maze?
- Why yes/no?

*** Agent structures
**** Simple reflex agents
- The agent takes an action based on just the current observations:
\[
\pi(a_t | x_t)
\]
**** Model-based reflex agents
- Maintains a model of the environment
- The agent has an internal state $\bel_t$ describing what it knows
\[
\pol(a_t | \bel_t)
\]
**** Planning agents
- The agent policy is not a simply function of observations and internal state
- At each step, the agent uses an algorithm to select its next action
**** Utility-based agents
- The agent has a specific utility function that it wishes to maximise

*** Learning and memory
**** Belief state
- Memory
- A summary of the agent's knowledge
- The state in a state machine
- The contents of the tape and read/write heads on a Turing machine.
**** Belief transitions
- A (possibly randomised) function $f : B \times A \times X \to B$ 
\[
b_{t+1} = f(b_t, a_t, x_t)
\]
- $b_t \in S$: Belief at time $t$.
- $a_t \in A$: Action at time $t$
- $x_t \in X$: Observation at time $t$.
- $f$ is implemented by the agent's algorithm
*** Example belief state
**** A known maze, known location
- What does the optimal policy look like?
**** An unknown maze, known location
- Observations: nearby walls, co-ordinates.
- What should the agent remember?
- What does the optimal policy look like?
**** A known maze, unknown location
- Observations: nearby walls
- What should the agent remember?
- What does the optimal policy look like?

*** Exercises and Assignments
**** Exercises (From AI3e, 2.7)
- 1. Representations
- 2. Top-level controller.
- 3. Obstacle avoidance.
- 4. Robot trap.
- 10. Autonomous cars: driver preferences
**** Assignments (From AI3e, 2.7)
- 5. Moving targets
- 7. Sensing
- 8. Batteries
- 9. Which functions?
- 11. Autonomous cars: state of the art.
  
*** Readings
**** Artificial Intelligence: FoCA 3E
- Chapter 1: Artificial Intelligence and Agents
- Chapter 2: Agent Architectures and Hierarchical Control

**** Artificial Intelligence: aMA 
- Chapter 1: Introduction
- Chapter 2: Intelligent Agents

** Elementary Decision Theory
*** Preferences
**** Types of rewards                                               :example:
- For e.g. a student: Tickets to concerts.
- For e.g. an investor: A basket of stocks, bonds and currency.
- For everybody: Money.

**** Preferences among rewards
For any rewards $x, y \in R$, we either
- (a) Prefer $x$ at least as much as $y$ and write $x \preceq^* y$.
- (b) Prefer $x$ not more than $y$ and write $x \succeq^* y$.
- (c) Prefer $x$ about the same as $y$ and write $x \eqsim^* y$.
- (d) Similarly define $\succ^*$ and $\prec^*$
  
*** Utility and Cost

**** Utility function
To make it easy, assign a utility $U(x)$ to every reward through a
utility function $U : R \to \Reals$.

**** Utility-derived preferences
We prefer items with higher utility, i.e.
- (a) $U(x) \geq U(y)$ $\Leftrightarrow$ $x \succeq^* y$
- (b) $U(x) \leq U(y)$ $\Leftrightarrow$ $y \succeq^* x$

**** Cost
It is sometimes more convenient to define a cost function $C: R \to \Reals$ so that we prefer items with lower cost, i.e.
- $C(x) \geq C(y)$ $\Leftrightarrow$ $y \succeq^* x$

**** Decision making as an optimisation problem
How can we find the decision maximising utility / minimising cost?

*** Choice of the utility function
**** Designer input
- The AI designer selects the utility (or goals)
- The choice is not always obvious!
**** The *value-alignment* problem
- The designer selects a utility they *think* is the best choice
- However, their choice results in unintended behaviour
- Example: Autonomous vehicles

**** The value-alignment in *populations*
- 
*** The basic decision problem
If we have defined a utility function assigning a value to every possible choice, then by assumption the *optimal* choice /maximises/ utility. So, we need to solve this problem:

\[
x^* = \argmax_x U(x)
\]
*** Brute Force
In the simplest case, we need to choose among a (small) finite number of options,
so finding the maximum is easy:

**** Python function
#+BEGIN_SRC python
  # returns the first element maximising U
  import numpy as np
  x_star = np.argmax(U)
#+END_SRC
**** Implementation
#+BEGIN_SRC python
  def argmax(U):
	arg_max = 0
	U_max = U[0]
	for x in range(1, len(U)):
	  if (U[x] > U_max):
		U_max = U[x]
		arg_max = x
  return arg_max
#+END_SRC
*** Large or infinite choices
**** Large, structured choices
- Shortest path: Sequences of places to pass along a route.
- Matching: Assign items to individuals.
- Theorem proving: Prove a mathematical theorem in the simplest possible way.
**** Infinite choices
- Control: Drive a car.

*** Readings
**** Artificial Intelligence: FoCA 3E
- Chapter 12.1: Preferences and Utility

**** Artificial Intelligence: aMA 
- Chapter 2: Intelligent Agents

