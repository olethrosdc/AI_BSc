{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Definition (Already Implemented)\n",
    "\n",
    "actions = {\n",
    "    \"up\": (0, 1),\n",
    "    \"left\": (-1, 0),\n",
    "    \"right\": (1, 0),\n",
    "    \"down\": (0, -1),\n",
    "}\n",
    "\n",
    "class DungeonEnvironment:\n",
    "    def __init__(self):\n",
    "        self.start_pos = (0,0)\n",
    "        self.goal_pos = (3,4)\n",
    "        self.holes = [(0,4), (3,2)]\n",
    "        self.walls = [(0,2), (2,0), (2,2), (2,3)]\n",
    "        self.current_position = (0,0)\n",
    "\n",
    "    def get_light(self):\n",
    "        curr_x, curr_y = self.current_position\n",
    "        goal_x, goal_y = self.goal_pos\n",
    "        return curr_x == goal_x or curr_y == goal_y\n",
    "\n",
    "    def get_echo(self):\n",
    "        curr_x, curr_y = self.current_position\n",
    "        for adjacent in [\n",
    "            (curr_x, curr_y + 1),\n",
    "            (curr_x, curr_y - 1),\n",
    "            (curr_x + 1, curr_y),\n",
    "            (curr_x - 1, curr_y),\n",
    "        ]:\n",
    "            if adjacent in self.holes:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Called at the start of the game.\n",
    "        \"\"\"\n",
    "        self.current_position = self.start_pos\n",
    "\n",
    "    def step(self, action: str):\n",
    "        \"\"\"\n",
    "        Updates the environment with the action of the agent.\n",
    "        :returns: new observation for the agent, as well as if the game ended and the outcome.\n",
    "        \"\"\"\n",
    "        act_x, act_y = actions[action]\n",
    "        curr_x, curr_y = self.current_position\n",
    "\n",
    "        new_x = curr_x + act_x\n",
    "        new_y = curr_y + act_y\n",
    "\n",
    "        # we bump if we go into a wall, or if we go out of bound\n",
    "        if (new_x, new_y) in self.walls or not (0<=new_x<=4 and 0<=new_y<=4):\n",
    "            bump = True\n",
    "        else:\n",
    "            # if we do not bump, update the position.\n",
    "            bump = False\n",
    "            self.current_position = (new_x, new_y)\n",
    "\n",
    "        observation = {\n",
    "            \"bump\" : bump,\n",
    "            \"echo\" : self.get_echo(),\n",
    "            \"light\": self.get_light()\n",
    "        }\n",
    "\n",
    "        if self.current_position == self.goal_pos:\n",
    "            outcome = \"Escaped\"\n",
    "            terminated = True\n",
    "        elif self.current_position in self.holes:\n",
    "            outcome = \"Fell into hole\"\n",
    "            terminated = True\n",
    "\n",
    "        else:\n",
    "            outcome = None\n",
    "            terminated = False\n",
    "\n",
    "        return observation, outcome, terminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Belief State (To be completed)\n",
    "\n",
    "\n",
    "class BeliefState:\n",
    "    \"\"\"\n",
    "    Maintains what we believe, using the knowledge base.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.sensed_walls = []  # list of positions where we bumped\n",
    "        self.safe_tiles = [] # list of tiles that are not walls and safe\n",
    "        self.sensed_light = []  # list of positions where we sensed light\n",
    "        self.sensed_echoes = []  # list of positions where we sensed echoes\n",
    "        self.current_position = (0, 0)\n",
    "\n",
    "    def update(self, taken_action: str, echo: bool, bump: bool, light: bool):\n",
    "        \"\"\"\n",
    "        :param taken_action: action taken by the agent before sensing (up, left, right or down).\n",
    "        :param echo: did we hear an echo ?\n",
    "        :param bump: did we bump into a wall ?\n",
    "        :param light: did we see light ?\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: implement logic here\n",
    "        # we must update the position, and sensed components.\n",
    "\n",
    "    def infer_goal_position(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        :return: The list of possible positions for the goal\n",
    "        \"\"\"\n",
    "        # TODO: implement logic here\n",
    "\n",
    "    def infer_hole_positions(self) -> List[Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        :return: The list of positions where we could have holes\n",
    "        \"\"\"\n",
    "        # TODO: implement logic here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for running any policy on the dungeon environment (Already implemented)\n",
    "def play_game(policy):\n",
    "    belief_state = BeliefState()\n",
    "    terminated = False\n",
    "    environment = DungeonEnvironment()\n",
    "    environment.reset()\n",
    "    num_steps = 0\n",
    "    trajectory = [environment.current_position]\n",
    "    while not terminated:\n",
    "        action = policy(belief_state)\n",
    "        observation, outcome, terminated = environment.step(action)\n",
    "        belief_state.update(action, **observation)\n",
    "        trajectory.append(environment.current_position)\n",
    "        num_steps += 1\n",
    "\n",
    "        if num_steps > 200:\n",
    "            print(\"Check your code, probably your agent is stuck somewhere\")\n",
    "            outcome = \"Timeout\"\n",
    "            terminated = True\n",
    "\n",
    "    print(f\"The game ended in {num_steps} with the following outcome: {outcome}.\")\n",
    "    print(f\"The path taken was: {trajectory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent definitions\n",
    "\n",
    "# Example\n",
    "def random_agent(belief_state) -> str:\n",
    "    \"\"\"\n",
    "    This is a naive agent, picking actions randomly.\n",
    "    \"\"\"\n",
    "    action_names = list(actions.keys())\n",
    "    return np.random.choice(action_names)\n",
    "\n",
    "def your_agent(belief_state) -> str:\n",
    "    \"\"\"\n",
    "    This function selects a new action based on the belief state.\n",
    "    :param belief_state: Current belief state.\n",
    "    :return: action picked by the policy:\n",
    "    \"\"\"\n",
    "    # TODO: try implementing a smarter agent\n",
    "    # This should at least exploit inferred goal and hole positions.\n",
    "    # (Does not have to be optimal, but must always find the exit and never fall into holes).\n",
    "    \n",
    "    return \"up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_game(random_agent)\n",
    "play_game(your_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
