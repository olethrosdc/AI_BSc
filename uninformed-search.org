#+TITLE: Uninformed search
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{algorithm,algorithmic}
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \cset[2] {\left\{#1 ~\middle|~ #2 \right\}}
#+LaTeX_HEADER: \newcommand \pol {\pi}
#+LaTeX_HEADER: \newcommand \Pols {\Pi}
#+LaTeX_HEADER: \newcommand \mdp {\mu}
#+LaTeX_HEADER: \newcommand \MDPs {\mathcal{M}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Bels {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \Unif {\textrm{Unif}}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Mult}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Dir {\textrm{Dir}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \Simplex {\mathbb{\Delta}}
#+LaTeX_HEADER: \newcommand \pn {\param^{(n)}}
#+LaTeX_HEADER: \newcommand \pnn {\param^{(n+1)}}
#+LaTeX_HEADER: \newcommand \pnp {\param^{(n-1)}}
#+LaTeX_HEADER: \newcommand \parent {\texttt{parent}}
#+LaTeX_HEADER: \usetikzlibrary{shapes.geometric}
#+LaTeX_HEADER: \usetikzlibrary{arrows.meta, positioning, quotes}
#+LaTeX_HEADER: \tikzstyle{utility}=[diamond,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=8mm]
#+LaTeX_HEADER: \tikzstyle{select}=[rectangle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{hidden}=[dashed,draw=black,fill=red!10]
#+LaTeX_HEADER: \tikzstyle{RV}=[circle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* Introduction
** Introduction
*** Search to find a solution
- Input: Problem specification (e.g. route-finding)
- Output: Solution (e.g. a policy)

**** Algorithms for finding solutions
- Must *search* through solution space
- Will ideally return an *optimal* solution
*** Graphs in Search Algorithms
**** Problem Graph
- Specifies the problem.
- An abstraction of a real-world problem
**** Search Graph
- Saves the state of the search.

** Graphs

*** Graph definitions
**** Graph $G = \langle N, E \rangle$
A graph $G$ is defined by:
- Set of *nodes* $N$
- Set of *edges* $E$, with $\langle x,y \rangle \in E$ and $x, y \in N$
**** Labels and costs
- Nodes can be labelled as e.g. start and goal states.
- Arcs can be labelled according to *actions*.
**** Paths and cycles
- A path $h$ from $x$ to $y$ in $N$ is a sequence $\langle n_0, \ldots, n_k \rangle$ so that
  $n_0 = x, n_k = y$ and $\langle n_{t}, n_{t+1} \rangle \in A$.
- We write $h_j$ for the $j$-th element of the path.
- A cycle is a path $\langle n_0, \ldots, n_k \rangle$ where $n_0 = n_k$.
- If a graph has no cycles, it is *acyclic*
- A state with no outgoing edges to other states is *terminal*

*** Graph labels
\begin{tikzpicture}[every edge quotes/.style = {auto, font=\footnotesize, sloped}]
      \node[RV,label=below:{start}] at (0,0) (0) {0};
      \node[RV] at (0,2) (1) {1};
      \node[RV] at (4,2) (2) {2};
      \node[RV,label=below:{goal}] at (4,0) (3) {3};
      \draw[->] (0) edge["a=1~c=1"] (1);
      \draw[->] (1) edge["a=0~c=2"] (2);
      \draw[->] (0) edge["a=0~c=5"] (3);
      \draw[->] (2) edge["a=0~c=1"] (3);
\end{tikzpicture}
- Action labels $a$: Tell you which action traverses an edge
- Edge costs $c$: Optionally, $c$
- Node labels: Some nodes are *goal* or *start* nodes.
**** Notation
- For a node $i \in N$, we write $g(i) = 1$ if it's a goal label.
- For an edge $(i,j) \in E$, we write $c(i, j) \in \Reals$ for its cost.
- The total cost for a path $h = \langle x, \ldots, y \rangle$ from node $x$ to a node $y$ is
\[
\sum_{k=1}^{|p|-1} c(p_k, p_{k+1}), \qquad \textrm{where $|p|$ is the length of $p$}
\]

*** Tree example
\begin{tikzpicture}
      \node[RV] at (0,0) (0) {0};
      \node[RV] at (1,1) (1) {1};
      \node[RV] at (1,-1) (2) {2};
      \node[RV] at (2,1.5) (3) {3};
      \node[RV] at (2,0.5) (4) {4};
      \node[RV] at (2,-0.5) (5) {5};
      \node[RV] at (2,-1.5) (6) {6};
      \draw[->] (0) to (1);
      \draw[->] (0) to (2);
      \draw[->] (1) to (3);
      \draw[->] (1) to (4);
      \draw[->] (2) to (5);
      \draw[->] (2) to (6);
\end{tikzpicture}
- No matter where the goal is, there is a unique path to it.
- Depending on how the edges are ordered, finding it may require up to 6 steps.
- For binary trees of depth $D$, the worst-case complexity is $2^D$.
- The *branching factor* is the number of children that each node can have.

*** Shortcut example
\begin{tikzpicture}
      \node[RV] at (0,0) (0) {0};
      \node[RV] at (1,0) (1) {1};
      \node[RV] at (2,0) (2) {2};
      \node[RV] at (3,0) (3) {3};
      \node[RV] at (1,1) (4) {4};
      \node[RV] at (1,-1) (5) {5};
      \draw[->] (0) to (1);
      \draw[->] (1) to (2);
      \draw[->] (2) to (3);
      \draw[->] (1) to (5);
      \draw[->] (1) to (4);
      \draw[->] (4) to (3);
\end{tikzpicture}
- There are two paths to node 3.
- Depending on how we search, we may find the longest path first.
- Why would we *prefer* one path to another?
- How should we search to find the /best/ path?
** Search algorithms
*** State-space graphs
- $s \in S$: *state* space
- $a \in A$: *action* space (with $A_s \subset S$ *available* actions in state $s$)
- $\tau: S \times A \to A$: *transition* model (deterministic)
- When we reach a *terminal* state, we stop. 
**** Graph specification
- Nodes: $S$
- Edges from node $i$: $\{(i, \tau(i, a) | a \in A_s\}$
**** Problem specifications
One or more of the following:
- $g : S \to \{0, 1\}$: goal indicator
- $c : S \times A \to \Reals$: step cost or constraint.
- $r : S \times A \to \Reals$: step reward.
**** Solution specification
- $\pi : S \to A$ deterministic policy
- If both the problem and the policy are deterministic, the policy is *open loop*
*** The search graph $S'$
- Node $0$ is *root* of the search graph.
- Each node $i \in S'$ corresponds to a state $s^i \in S$.
- It also corresponds to a  *path* $s^0, \ldots, \parent(\parent(s^i)), \parent(s^i), s^{i}$.
- Node depth: $d_i = 1 + d_{\parent(i)}$, with $d_0 = 0$.
**** Frontier: Keeping track of what to search next
At step $0$, the frontier is $F_0 = \{0\}$ and set of searched nodes $S'_0 = \emptyset$. 

At step $k = 0, 1, \ldots$:
- The frontier is $F_k$, and searched nodes $S'_k$.
- Select a node $i$, where $s^i \notin S'_k$.
- We select action $a$ in node $i$, and observe $s' = \tau(s^i, a)$.
- $i + 1$ is now a child of $i$, with $s^{i+1} = s'$.
- Update the frontier $F_{k+1} = F_k \cup \{i + 1\} \setminus \{i\}$.
- In the end, no more nodes can be added: $F_k = \emptyset$ and $S'_k = S'_{k+1}$

*** Graph visualisation

* Uninformed search
** Depth-first search
*** Depth-first search
**** Generic depth-first search
\begin{algorithmic}
\STATE \textbf{global} $F = \emptyset$ : Nodes searched
\STATE \textbf{input} $G = \langle N, E \rangle$: Graph.
\STATE \textbf{input} $n$ : Current node
\STATE \textbf{function} \texttt{DepthFirst}($G, n$)
  \STATE $F = F \cup \{n\}$ : mark $n$ as searched
  \FOR {$c \notin F: \langle c,j \rangle \in E$}
     \IF {$\texttt{DepthFirst}(G, j, S)$}
          \RETURN 1.
     \ENDIF
\ENDFOR
\end{algorithmic}
**** Discussion
- This function goes through all the nodes in the graph
- How can we use it to identify a paths to the goal?
- How can we modify it to identify all paths to the goal?
- How can we modify it to identify the shortest path to the goal?
*** Goals and DFS
**** Main idea
- Go through all nodes, marking searched nodes.
- Return true for nodes that reach the goal.
**** Goal-DFS
\begin{algorithmic}
\STATE \textbf{global} $S = \emptyset$ : Nodes searched
\STATE \textbf{input} $G = \langle N, A \rangle$: Graph.
\STATE \textbf{input} $n$ : Current node
\STATE \textbf{input} $X$ : Set of goals
\STATE \textbf{function} \texttt{DepthFirst}($G, n, X$)
\STATE $S = S \cup \{n\}$ : mark $n$ as searched
\IF {$n \in X$}
    \RETURN 1
\ENDIF
\FOR {$c \notin F: \langle c,j \rangle \in E$}
     \IF {$\texttt{DepthFirst}(G, j, X)$}
          \RETURN 1.
     \ENDIF
\ENDFOR
\end{algorithmic}

*** BFS
\begin{algorithmic}
\STATE \textbf{function} \texttt{breadthFirst}($G, n, S$)
\STATE $S_{k+1} = S_k \cup \{n\}$ : mark $n$ as searched
\FOR {$a \in A_s \notin F: \langle c,j \rangle \in E$}
     \IF {$\texttt{DepthFirst}(G, j, S)$}
          \RETURN 1.
     \ENDIF
\ENDFOR
\end{algorithmic}

** Minimum-cost search    
** The shortest path problem
*** The shortest path problem
#+ATTR_BEAMER: :overlay <+->
- Traversing arc $\langle x, y \rangle$ incurs *costs* $c(\langle x,y \rangle)$
- Following a *path* $h$ has a total cost $C(h) = \sum_{\langle x,y \rangle \in h} c(\langle x,y \rangle)$
- We can equivalently consider state-action *costs* $c(s,a)$.
- A policy $\pi$ specifies a path $x_1, \ldots$ with $x_{k+1} = \tau(x_k, \pi(x_k))$
- Following a *policy* $\pi$ from state $x_1 = x$ has a total cost $C^\pi(x_1) = \sum_{k=1}^{t} c(x_k, \pi(x_k))$.
#+BEAMER: \pause
**** The shortest path problem
- Input:  *start* nodes $X$ and *goal* nodes $Y$ and edge costs $c: A \to \Reals$.
- Output: Find a path $h$ from $X$ to $Y$ so that $C(h) \leq C(h')$ for all $h'$ 
#+BEAMER: \pause
**** Notes
- If the path/policy does not reach a goal, the cost is infinite.
- We can maximise rewards instead of minimising costs.


*** Formalising the shortest path problem
The cost from state $x$ of a policy that reaches a goal is
\[
C^\pi(s) \defn \sum_{i=1}^\infty c[s_{t}, \pi(s_t)], \qquad s_{t+1} = \tau[s_t, \pi(s_t)], \quad s_{1} = s
\]
where for every $s \in Y$,  $c(s, a) = 0$ and $\tau(s,a) = s$ for all actions.
- We can calculate this recursively (from the goal state)
\begin{align}
C^\pi(s)
& = \sum_{i=1}^\infty c[s_{t}, \pi(s_t)]\\
& = c[s, \pi(s)] + \sum_{i=2}^\infty c[s_{t}, \pi(s_t)]\\
& = c[s, \pi(s)] + C^\pi\{\tau[s, \pi(s)]\}.
\end{align}
- The same idea applies for the *shortest* path
\begin{align}
C^*(s) 
\defn \min_\pi C^\pi(s)
= \min_a \left\{c[s, a] + C^*[\tau(s, a)]\right\}.
\end{align}

*** The shortest path algorithm: backward search
**** Shortest path algorithm
\begin{algorithmic}
\STATE Input: Goal states $Y$, starting state $x$.
\STATE Set $C(s) = 0$ for all states $s \in Y$, $F_0 = Y$.
\FOR {$t = 0,1, \ldots$}
\FOR {$s' \in F_t$}
\STATE $\pol(s) = \argmin_a c(s,a) + C(\tau(s, a))$
\STATE $C(s) = \min_a c(s,a) + C(\tau(s, a))$
\ENDFOR
\STATE $F_{t+1} = \parent(F_t)$.
\IF {$F_{t+1} = \emptyset$ or $x \in F_t$}
\RETURN $\pol, C$
\ENDIF
\ENDFOR
\end{algorithmic}
**** Algorithm idea
- Start from goal states
- Go back one step each time, adding the cost.
- Stop whenever there are no more states to go back to, or if we reach the start state.

*** Optimality proof
**** Theorem
$C(s) = C^*(s)$
**** Proof
- If $s \in Y$, then $C(s) = 0 = C^*(s)$.
- For any other $s', s = \parent(s')$: we will show that:
 if $C(s') \leq C^*(s')$ then $C(s) \leq C^*(s)$.
\begin{align*}
C(s)
&=
\min_a \left\{c(s,a) + C(\tau(s,a))\right\}
\tag{by definition}
\\
&\leq
\min_a \left\{ c(s,a) + C^*(\tau(s,a)) \right\}
\tag{by induction}
\\
&\leq
\min_a \left\{ c(s,a) + C^{\pi'}(\tau(s,a)) \right\},
\qquad \forall \pi'
\tag{by optimality}
\\
&\leq
C^\pi(s), \qquad \forall \pi.
\end{align*}
For the optimal policy $\pi^*$, $C^{\pi^*}(s) = C^*(s)$, so $C(s) \leq C^*(s)$. Finally,
\[
C^*(s) \leq C^{\pi}(s) = C(s) \geq C^*(s),
\]
since $C^{\pi}(s) = C(s)$ for the policy returned by the algorithm.
