#+TITLE: Inference
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \input{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSubsection[]{\begin{frame}<beamer>\tableofcontents[currentsubsection]\end{frame}}
* Logical inference
  #+TOC: headlines [currentsection]
** Set theory and logic
*** Set theory
- First, consider some universal set $\Omega$.
- A set $A$ is a collection of points $x$ in $\Omega$.
- $\{x \in \Omega : f(x)\}$: the set of points in $\Omega$ with the property that $f(x)$ is true.

**** Unary operators
- $\neg A =  \{x \in \Omega : x \notin A\}$.
**** Binary operators
- $A \cup B$ if $\{x \in \Omega : x \in A \vee x \in B\}$ - (c.f. $A \vee B$)
- $A \cap B$ if $\{x \in \Omega : x \in A \wedge x \in B\}$ - (c.f. $A \wedge B$)
**** Binary relations
- $A \subset B$ if $x \in A \Rightarrow x \in B$ - (c.f. $A \implies B$)
- $A = B$ if $x \in A \Leftrightarrow x \in B$ - (c.f. $A \Leftrightarrow B$)

** Logical inference
*** The inference problem

- Given statements $A_1, \ldots, A_n$ we know to be true (i.e. a knowledge base), is another statement $B$ true?

The following statements are equivalent:
- $A \implies B$ iff $(A \cap \neg B) = \emptyset$.
- $A \implies B$ iff $A \subset B$.

In addition
- If $(A \Rightarrow B ) \wedge A$ then $B$.
- If $(A \wedge B)$ then $A$.
*** Illustration  
[[./figures/logical-inference.pdf]]

* Probability background
  #+TOC: headlines [currentsection]
** Probability facts
*** Events as sets
**** The universe and random outcomes
- The $\Omega$ contains all events that can happen.
- When something happens, we observe an  element $\omega \in \Omega$.
**** Events in the universe
- An event is true if $\omega \in A$, and false if $\omega \notin A$.
- The negative event $\neg A = \Omega \setminus A$ is the set
- The possible events are a collection of subsets $\Sigma$ of $\Omega$ so that
(i) $\Omega \in \Sigma$, (ii) $A, B \in \Sigma \Rightarrow A \cup B in \Sigma$ (iii) $A \in \Sigma \Rightarrow \neg A \in \Sigma$
**** Example: Traffic violation
- A car is moving with speed $\omega \in [0,\infty)$ in front of the speed camera.
- $A_0 = [0,50]$: below the speed limit
- $A_1 = (50,60]$: low fine
- $A_2 = (60,\infty]$: high fine
- $A_3 = (100, \infty)$: Suspension of license
- All combinations of the above events are interesting.
*** Probability fundamentals

**** Probability measure $P$
Probability can be seen as an area-like function assigning a likelihood to sets.
- $P : \Sigma \to [0,1]$  gives the likelihood $P(A)$ of an event $A \in \Sigma$.
- $P(\Omega) = 1$
- For $A, B \subset \Omega$, if $A \cap B = \emptyset$ then $P(A \cup B) = P(A) + P(B)$.
*** Marginalisation
**** Partition
If $A_1, \ldots, A_n$ are a partition of $B$ then:
- $A_j \cap A_i = \emptyset$ for $i \neq j$
- $\bigcup_{i=1}^n A_i = B$.
**** Marginalisation
If $A_1, \ldots, A_n \subset \Omega$ are a partition of $\Omega$
\[
P(B) = \sum_{i = 1}^n P(B \cap A_i).
\]


** Conditional probability and independence
*** Conditional probability
**** Conditional probability
    :PROPERTIES:
    :BEAMER_env: definition
    :END:
The conditional probability of an event $A$ given an event $B$ is defined as 
\[
P(A | B) \defn \frac{P(A \cap B)}{P(B)}
\]
The above definition requires $P(B)$ to exist and be positive.

**** Conditional probabilities as a collection of probabilities
More generally, we can define conditional probabilities as simply a
collection of probability distributions:
\[
\{P_\param : \theta \in \Param\},
\]
where $\Param$ is indexing possible values of $\theta$.
-  $\theta$ is sometimes called the *model* or *parameter*

*** The theorem of Bayes
**** Bayes's theorem
    :PROPERTIES:
    :BEAMER_env: theorem
    :END:
\[
P(A | B) = \frac{P(B | A) P(A)}{P(B)} 
\]
#+BEAMER: \pause

**** The general case
If $A_1, \ldots, A_n$ are a partition of $\Omega$, meaning that they
are mutually exclusive events (i.e. $A_i \cap A_j = \emptyset$ for $i
\neq j$) such that one of them must be true (i.e. $\bigcup_{i=1}^n A_i =
\Omega$), then
\[
P(B) = \sum_{i=1}^n P(B | A_i) P(A_i)
\]
and 
\[
P(A_j | B) = \frac{P(B | A_j)}{\sum_{i=1}^n P(B | A_i) P(A_i)}
\]

** Posterior distributions and model estimation
*** Bayes's theorem
**** As a conditional measure
\[
P(A \mid B)
= 
\frac{P(B \mid A) P(A)}{P(B)}
= 
\frac{P(B \mid A) P(A)}{P(B \mid A) P(A) + P(B \mid \neg A) P(\neg A)}
\]

#+BEAMER: \pause
**** As a causal explanation
\[
\Pr(\textrm{cause} \mid \textrm{effect})
= 
\frac{\Pr(\textrm{effect} \mid \textrm{cause}) \Pr(\textrm{cause})}{\Pr(\textrm{effect})}
\]
#+BEAMER: \pause
**** As model inference
- Prior $\bel(\param)$
- Model class $\{P_\param(\bel) : \param \in \Param\}$
- Data $x$
\[
\bel(\param \mid x)
= 
\frac{P_\param(x) \bel(\param)}{\Pr_\bel(x)}
= 
\frac{P_\param(x) \bel(x)}{\sum_{\param' \in \Param} P_{\param'}(x) \bel(\param')}
\]

*** Example: COVID symptoms
**** Activity (with playing cards or dice)
- Pick two ($x, y$) from 1 to 10.
- If ($x = 1$ and $y < 9$), *or* ($x$ is even and $y \geq 9$), you have *symptoms*.
- Do you have COVID?
#+BEAMER: \pause
**** Information
- 20% of people have COVID
- 50% of people  *with* COVID have symptoms.
- 10% of people with *no* COVID have symptoms.
- If you *do* have symptoms, what are the chances you have COVID?
#+BEAMER: \pause
**** Formalisation
- Prior $P(C)=  0.1$:
- Likelihood: $P(S | C) = 0.5$, $P(S | \neg C) = 0.1$
- Posterior: 
\[
P(C | S) = \frac{P(S | C) P(C)}{P(S | C) P(C) + P(S | \neg C) P( \neg C)}
\]

*** Example: The k-meteorologists problem (set notation)
- $R_t$: The *event* that it rains at time $t$.
- A set of stations $\Param$, with $\param \in \Param$ making weather predictions:
  \[
  P(R_{t+1} \mid R_1, \ldots, R_{t}, \param), 
  \]
- A *prior probability* $P(\param)$ on the stations.
- The *marginal* probability
\[
P(R_1 \cup \cdots \cup R_t) = \sum_{\param \in \Param} P(R_1 \cap \cdots \cap R_t  \mid \param) P(\param)
\]
- The *posterior* probability
\begin{align*}
P(\param \mid R_1\cap \cdots \cap R_t)
&= \frac{P(R_1\cap \cdots \cap R_t \mid \param) P(\param)}{P(R_1\cap \cdots \cap R_t)}
= \frac{\prod_{i=1}^t P(R_t \mid R_1 \cap \cdots \cap R_{t-1}, \param)  P(\param)}{P(R_1 \cap \cdots \cap R_t)}\\
&= \frac{P(R_t \mid R_1 \cap \cdots \cap R_{t-1} \mid \param)  P(\param \mid R_1 \cap \cdots \cap R_{t-1})}{P(R_t \mid R_1 \cap \cdots \cap R_{t-1})}
\end{align*}
- The *marginal posterior* probability
\[
P(R_{t+1} \mid R_1\cap \cdots \cap R_t) = \sum_{\param \in \Param} P(R_{t+1} \mid R_1\cap \cdots \cap R_t, \param) P(\param \mid R_1\cap \cdots \cap R_t)
\]

*** Example: The k-meteorologists problem (stat notation)
- $x_t \in \{0,1\}$: A *random variable*, telling us whether it rains at time $t$.
- A set of stations $\Param$, with $\param \in \Param$ making weather predictions:
  \[
  P_\param(x_{t+1} \mid x_1, \ldots, x_{t})
  \]
- A *prior probability* $\bel(\param)$ on the stations.
- The *marginal* probability
\[
\Pr_\bel(x_1, \ldots, x_t) = \sum_{\param \in \Param} P_\param(x_1, \ldots, x_t) \bel(\param)
\]
- The *posterior* probability
\begin{align*}
\bel(\param \mid x_1, \ldots, x_t)
&= \frac{P_\param(x_1, \ldots, x_t) \bel(\param)}{\Pr_\bel(x_1, \ldots, x_t)}
= \frac{\prod_{i=1}^t P_\param(x_t \mid x_1, \ldots, x_{t-1})  \bel(\param)}{\Pr_\bel(x_1, \ldots, x_t)}\\
&= \frac{P_\param(x_t \mid x_1, \ldots, x_{t-1})  \bel(\param \mid x_1, \ldots, x_{t-1})}{\Pr_\bel(x_t \mid x_1, \ldots, x_{t-1})}
\end{align*}
- The *marginal posterior* probability
\[
\Pr_\bel(x_{t+1} \mid x_1, \ldots, x_t) = \sum_{\param \in \Param} P_\param(x_{t+1} \mid x_1, \ldots, x_t) \bel(\param \mid x_1, \ldots, x_t)
\]

** Random variables, expectation and variance
*** Random variables
A random variable $f : \Omega \to \Reals$ is a real-valued *function*, with $\omega \sim P$.
**** The distribution of $f$
The probability that $f$ lies in some subset $A \subset \Reals$ is
\[
P_f(A) \defn P(\{\omega \in \Omega : f(\omega) \in A\}),
\]
and we write $f \sim P_f$. 
**** Shorthands for RV
- For RVs $f : \Omega \to \Reals$, we write $P(f \in A)$ to mean $P_f(A)$.
- For RVs $f : \Omega \to X$, where $X$ is a finite set e.g. $\{1, 2, \ldots, n\}$, we write $P(f = x) = P_f(\{x\})$ for any $x \in X$.
*** Independence of random variables

Two RVs $f,g$ are independent in the same way that events are independent. 
\[
P(f \in A \wedge g \in B) = P(f \in A) P(g \in B) = P_f(A) P_g(B).
\]
In that sense, $f \sim P_f$ and $g \sim P_g$. 
**** Formal definition
More specifically, we are measuring the set of \(\omega\) values for which $f(\omega) \in A$ and $g(\omega) \in B$:
\[
P(\{\omega : f(\omega) \in A, g(\omega) \in B\}) = P_f(A) P_g(B).
\]
**** Shorthand notation
Since the above is very cumbersome, we usually just write that
\[
P(f, g) = P(f) P(g)
\]
for any two independent random variables $f, g$.
*** Expectation
For any real-valued random variable $f: \Omega \to \Reals$, the expectation with respect to a probability measure $P$ is
\[
\E_P(f) = \sum_{\omega \in \Omega} f(\omega) P(\omega).
\]
When $\Omega$ is continuous, we can use a density $p$
\[
\E_P(f) = \int_{\Omega} f(\omega) p(\omega) d\omega.
\]

**** Linearity of expectations
For any RVs $x, y$:
\[
\E_P(x + y) = \E_P(x) + \E_P(y)
\]
*** Multiple variables
**** The joint distribution $P(x,y)$
For two (or more) RVs $x : \Omega \to \Reals$, and $y : \Omega \to
 \Reals$, this is a *shorthand* for the distribution of $(x(\omega),
 y(\omega))$ when $\omega \sim P$. We can also use $P(x = i, y = j)$ for the probability that the two variables assume the values $i, j$ respectively.
**** Independence
If $x,y$ are independent RVs then $P(x,y) = P_x(x) P_y(y)$.
**** Correlation
If $x,y$ are *not* correlated then $\E_P(xy) = \E(x)\E(y)$.
**** IID (Independent and Identically Distributed) random variables
A sequence $x_t$ of r.v.s is IID if $x_t \sim P$
so that
\[
(x_1, \ldots, x_t, \ldots, x_T) \sim P^T
\]
i.e. a \(T\)-length sample is drawn from the product distribution $P^T = P \times P \times \cdots \times P$.
*** Conditional expectation
The conditional expectation of a random variable $f: \Omega \to \Reals$, with respect to a probability measure $P$ conditioned on some event $B$ is simply
\[
\E_P(f | B) = \sum_{\omega \in \Omega} f(\omega) P(\omega | B).
\]
Conditional expectations are similar to conditional probabilities.
*** Conditional probabilities of RVs
Similarly to the notation over sets,
\[
P(A \cap B) = P(A \mid B) P(B),
\]
when dealing with RVs, it is common to use the notation
\[
P(x, y) = P(x | y) P(y)
\]
This equation works for all possible values of $x, y$ e.g.
\[
P(x = 1, y = 0) = P(x = 1 | y = 0) P(y = 0)
\]
which then denotes the probability msas of each


* Graphical models

** Graphical model
*** Independence
**** Independent events $A \indep B$
-  $A, B$ are *independent* iff $P(A \cap B) = P(A) P(B)$.
- Knowing if $A$ happened, does not tell us anything about whether $B$ happened

**** Conditional independence $A \indep B \mid C$
- $A, B$ are *conditionally independent* given $C$ iff $P(A \cap B | C) = P(A | C) P(B | C)$.
- Knowing if $C$ happened tells us all we need to know about $A$ and $B$.

**** For random variables
- Independence: $P(x, y) = P(x) P(y)$.
- Conditional independence:  $P(x, y | z) = P(x | z) P(y |z)$.


*** Model specification: Independent
\begin{center}
      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$x_1$};
        \node[RV] at (2,0) (x2) {$x_2$};
      \end{tikzpicture}
\end{center}

**** Specification                                                    :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :END:
    \begin{align*}
      f &= \Ber(1/2)\\
      g &= \Ber(0.8)\\
      x_1 &\sim f\\
      x_2 &\sim g
    \end{align*}
**** Code                                                             :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.7
     :END:

#+BEGIN_SRC python
def f():
  return np.random.choice(2)
def g: 
  return np.random.choice(2, [0.2, 0.8])
x1 = f()
x2 = g()
#+END_SRC 


*** Model specification: Gaussian Dependent variables
\begin{center}
      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$x_1$};
        \node[RV] at (2,0) (x2) {$x_2$};
	\draw[->] (x1)--(x2);
      \end{tikzpicture}
\end{center}

**** Specification                                                    :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :END:
    \begin{align*}
      f &= \Normal(0, 1)\\
      g(a) &= \Normal(a, 1)\\
      x_1 &\sim f\\
      x_2 | x_1 = a &\sim g(a)
    \end{align*}
**** Code                                                             :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.7
     :END:

#+BEGIN_SRC python
def f():
  return np.random.normal(0,1)
def g(a): 
  return np.random.normal(a)
x1 = f()
x2 = g(x1)
#+END_SRC 




*** Model specification: Bernoulli Dependent variables
\begin{center}
      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$x_1$};
        \node[RV] at (2,0) (x2) {$x_2$};
	\draw[->] (x1)--(x2);
      \end{tikzpicture}
\end{center}

**** Specification                                                    :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.3
     :END:
    \begin{align*}
      f &= \Ber(1/2)\\
      g(a) &= \Ber(\theta_a)\\
      x_1 &\sim f\\
      x_2 | x_1 = a &\sim g(a)\\
       \theta &= (0.6, 0.5)
    \end{align*}
**** Code                                                             :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.7
     :END:

#+BEGIN_SRC python
def f():
  return np.random.choice(2)
def g(a): 
  theta = [0.6, 0.5]
  return np.random.choice(2,
 [1 - theta[a], theta[a]])
x1 = f()
x2 = g(x1)
#+END_SRC 




*** Graphical models
\begin{center}
    \begin{tikzpicture}
      \node[RV] at (2,0) (xi) {$x_3$};
      \node[RV] at (0,0) (xB) {$x_1$};
      \node[RV] at (1,1) (xD) {$x_2$};
      \draw[->] (xB) to (xD);
      \draw[->] (xD) to (xi);
      \draw[->] (xB) to (xi);
    \end{tikzpicture}
\end{center}
- Variables: $x_1, x_2, x_3$
- Arrows denote dependencies between variables.

*** Conditional independence
**** Example
    \begin{tikzpicture}
      \node[RV] at (2,0) (xi) {$x_3$};
      \node[RV] at (0,0) (xB) {$x_1$};
      \node[RV] at (1,1) (xD) {$x_2$};
      \draw[->] (xB)--(xD);
      \draw[->] (xD)--(xi);
    \end{tikzpicture}
    Graphical model for the factorisation $\Pr(x_3 \mid x_2) \Pr(x_2 \mid x_1) \Pr(x_1)$.
**** Definition
    - Consider variables $x_1, \ldots, x_n$.
    - Let $B, D$ be subsets of $[n]$.

    We say $x_i$ is *conditionally independent* of $\bx_B$ given $\bx_D$ and write 
    \[x_i \indep \bx_B \mid \bx_D\]
    if and only if:
    \[
    \Pr(x_i, \bx_B \mid \bx_D)
    =
    \Pr(x_i \mid \bx_D)
    \Pr(\bx_B \mid \bx_D).
    \]
*** Directed graphical model
A collection of $n$ random variables $x_i : \Omega \to X_i$, and let $X \defn \prod_i X_i$, with underlying probability measure $P$ on $\Omega$.
      Let $\bx = (x_i)_{i=1}^n$ and for any subset $B \subset[n]$ let
      \begin{align}
        \bx_B &\defn (x_i)_{i \in B}\\
        \bx_{-j} &\defn (x_i)_{i \neq i}
      \end{align}

*** Model specification: Chain
\begin{center}
      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$x_1$};
        \node[RV] at (2,0) (x2) {$x_2$};
        \node[RV] at (4,0) (x3) {$x_3$};
        \draw[->] (x1)--(x2);
        \draw[->] (x2)--(x3);
      \end{tikzpicture}
\end{center}

**** Specification                                                    :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
    \begin{align}
      \label{eq:factored-model}
      x_1 &\sim f\\
      x_2 \mid x_1 = a &\sim g(a)\\
      x_3 \mid x_2 = b &\sim h(b),
    \end{align}
**** Code                                                             :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:

#+BEGIN_SRC python
def f():
  return np.random.uniform()
def g(a): 
  return np.random.uniform() + a
def h(b)
  return np.random.uniform * b
x1 = f()
x2 = g(x1)
x3 = h(x2)
#+END_SRC 

*** Smoking and lung cancer
      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$S$};
        \node[RV] at (2,0) (x2) {$C$};
        \node[RV] at (4,0) (x3) {$A$};
        \draw[->] (x1)--(x2);
        \draw[->] (x3)--(x2);
      \end{tikzpicture}
      
Smoking and lung cancer graphical model, where $S$: Smoking, $C$: cancer, $A$: asbestos exposure.

*** Time of arrival at work
      \begin{tikzpicture}
        \node[RV] at (0,0) (x1) {$x_1$};
        \node[RV] at (1,1) (x2) {$T$};
        \node[RV] at (2,0) (x3) {$x_2$};
        \draw[->] (x2)--(x3);
        \draw[->] (x2)--(x1);
      \end{tikzpicture}
     
Time of arrival at work graphical model where $T$ is a traffic jam and $x_1$ is the time John arrives at the office and $x_2$ is the time Jane arrives at the office.

*Conditional independence:
- Even though $x_1, x_2$ are *not independent*, they become independent once you know $T$.

*** School admission
|--------+------+--------|
| School | Male | Female |
|--------+------+--------|
| A      |   62 |     82 |
| B      |   63 |     68 |
| C      |   37 |     34 |
| D      |   33 |     35 |
| E      |   28 |     24 |
| F      |    6 |      7 |
|--------+------+--------|

- $z$: gender
- $s$: school applied to
- $a$: admission

**** Col A                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
        \begin{tikzpicture}
          \node[RV] at (0,0) (z) {$z$};
          \node[RV] at (1,1) (s) {$s$};
          \node[RV] at (2,0) (a) {$a$};
          \draw[->] (z)--(s);
          \draw[->] (z)--(s);
          \draw[->] (s)--(a);
        \end{tikzpicture}
	Is admission independent of gender?
**** Col B                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:



        \begin{tikzpicture}
          \node[RV] at (0,0) (z) {$z$};
          \node[RV] at (1,1) (s) {$s$};
          \node[RV] at (2,0) (a) {$a$};
          \draw[->] (z)--(s);
          \draw[->] (z)--(s);
          \draw[->] (s)--(a);
          \draw[->] (z)--(a);
        \end{tikzpicture}

	How about here?
** Exercises
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (b)--(c);
          \draw[->] (c)--(d);
        \end{tikzpicture}
\[
P(a, b, c, d) = \cdots
\]
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (b)--(c);
          \draw[->] (c)--(d);
          \draw[->] (b)--(d);
        \end{tikzpicture}
\[
P(a, b, c, d) = 
\]
*** What is the model for this graph?
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
          \draw[->] (a)--(b);
          \draw[->] (a)--(c);
        \end{tikzpicture}
\[
P(a, b, c, d) = 
\]
*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (c | b) P(d | b)
\]

*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (d | c) P(c)
\]

*** Draw the graph for this model
       \begin{tikzpicture}
          \node[RV] at (0,0) (a) {$a$};
          \node[RV] at (0,2) (b) {$b$};
          \node[RV] at (2,0) (c) {$c$};
          \node[RV] at (2,2) (d) {$d$};
        \end{tikzpicture}
\[
P(a, b, c, d) = P(a) P(b | a) P (c | a) P(d | b, c)
\]








*** Conditional independence

For any set of events $A_1, A_2, A_3, \ldots$, we can write their co-occurence probability as
$\prod_i P(A_i  \mid \cap A_1 \cap A_2 \cap \cdots \cap A_{i-1})$. However,
we can use a *Bayesian network* to define conditional independence structures.
**** Bayesian Network
:PROPERTIES:
:BEAMER_col: 0.5
:END:
    \begin{tikzpicture}
      \node[RV] at (0,0) (A1) {$A_1$};
      \node[RV] at (0,1) (A2) {$A_2$};
      \node[RV] at (1,0.5) (B) {$B$};
      \node[RV] at (2,0) (C1) {$C_1$};
      \node[RV] at (2,1) (C2) {$C_2$};
      \draw[->] (A1) to (B);
      \draw[->] (A2) to (B);
      \draw[->] (B) to (C1);
      \draw[->] (B) to (C2);
    \end{tikzpicture}
**** Network rule
:PROPERTIES:
:BEAMER_col: 0.5
:END:
If $A$ is a parent of $B$ and $C$ is a child of $B$, and there are *no other paths* from $A$ to $C$ then the following conditional independence holds:
\[
P(C \mid B, A) = P(C \mid B)
\]
i.e. $C$ is conditionally independent of $A$ given $B$. 
**** Conditional probability tables
We can now write the distribution of the above example as
\[
P( B, C_1, C_2) = 
P(A_1) P(A_2) P(B | A_1 \cap A_2) P(C_1 | B) P(C_2 | B).
\]
*** Example: COVID test 
**** Information
- 10% of people have COVID
- 50% of people with COVID have a positive *test*
- 50% of people with COVID have *symptoms*
- 10% of people without COVID have a positive *test*
- 20% of people without COVID have *symptoms*
#+BEAMER: \pause
**** Formalisation
- Prior: $P(C = 1) = 0.1$
- Likelihood: $P(T, S | C) = P(T | C) P(S | C)$, $P(T, S | \neg C)$ for all va43lues of $T, S, C$.
- Posterior: 
\[
P(C | T, S) = 
\frac{P(S | C)P (T | C) P(C)}
{\sum_{i=0}^1 P(S | C = i)P (T | C = i) P(C = i)}
\]
*** Example: Naive Bayes models
Sometimes we observe multiple effects that have a common cause, but which are otherwise independent:
\[
\Pr(\textrm{effect}_1, \ldots \textrm{effect}_n \mid \textrm{cause})
=
\prod_{i=1}^n \Pr(\textrm{effect}_i \mid \textrm{cause})
\]
**** Naive Bayes model
- Observations $(\bx_t, y_t)_{t=1}^T$ with $\bx_t = (x_{t,1}, \ldots, x_{t,n})$.
- Probability *models* $P_\param(y \mid \bx) = \prod_{i=1}^n P_\param(y \mid x_i)$.
*** Example: Wumpus world
**** World 1
:PROPERTIES:
:BEAMER_col: 0.25
:END:
  \begin{tikzpicture}[scale=0.8]
        \draw[help lines] (0,0) grid (3,2);
        \node at (1.5,0.5) (agent) {\Strichmaxerl};
  \end{tikzpicture}
**** World 2
:PROPERTIES:
:BEAMER_col: 0.25
:END:
  \begin{tikzpicture}[scale=0.8]
        \draw[help lines] (0,0) grid (3,2);
        \node at (1.5,0.5) (agent) {\Strichmaxerl};
        \node at (1.5,1.5) (wumpus) {O};
  \end{tikzpicture}
**** World 3
:PROPERTIES:
:BEAMER_col: 0.25
:END:
  \begin{tikzpicture}[scale=0.8]
        \draw[help lines] (0,0) grid (3,2);
        \node at (1.5,0.5) (agent) {\Strichmaxerl};
        \node at (2.5,0.5) (hole) {O};
  \end{tikzpicture}
**** World 4
:PROPERTIES:
:BEAMER_col: 0.25
:END:
  \begin{tikzpicture}[scale=0.8]
        \draw[help lines] (0,0) grid (3,2);
        \node at (1.5,0.5) (agent) {\Strichmaxerl};
        \node at (1.5,1.5) (wumpus) {O};
        \node at (2.5,0.5) (hole) {O};
  \end{tikzpicture}
**** Details
- Probability of each world $A_i$ being true: 1/4
- Probability of each hole generating a breeze: $P(B_1 | A_2 \cup A_4) = P(B_2 | A_3 \cup A_4)$ with $B_1, B_2$ conditionally independent given $A$.
**** Questions
- What is the probability of feeling a breeze $B = B_1 \cup B_2$ in each world? 
- What is the probability of a hole above if you *feel* a breeze?
- What is the probability of a hole above f you *don't* feel a breeze?



