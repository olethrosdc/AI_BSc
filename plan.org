#+TITLE: Artificial Intelligence
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{algorithm,algorithmic}
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \cset[2] {\left\{#1 ~\middle|~ #2 \right\}}
#+LaTeX_HEADER: \newcommand \pol {\pi}
#+LaTeX_HEADER: \newcommand \Pols {\Pi}
#+LaTeX_HEADER: \newcommand \mdp {\mu}
#+LaTeX_HEADER: \newcommand \MDPs {\mathcal{M}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Bels {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \Unif {\textrm{Unif}}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Mult}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Dir {\textrm{Dir}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \Simplex {\mathbb{\Delta}}
#+LaTeX_HEADER: \newcommand \pn {\param^{(n)}}
#+LaTeX_HEADER: \newcommand \pnn {\param^{(n+1)}}
#+LaTeX_HEADER: \newcommand \pnp {\param^{(n-1)}}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* Introduction

** About the course  
*** Aims
This course will focus on algorithms and models for Artificial
Intelligence.  We will concentrate mainly on the decision making,
rather than the learning, side of artificial intelligence. Learning is
already addressed in statistics courses, as well as the machine
learning course in the final year.

*** Philosophy
The philosophy of this course is as follows: 
- We give example problems.
- We use theory to explain and generalise from those examples to general problems.
- We describe algorithms to /solve/ general problems.
- We implement algorithms to solve the specific examples.

In general, the course will start from the simplest problems and
slowly progress to the more complex ones.

*** How to use this notebook
- This notebook contains a summary of all the foundational material in the course.
- For details, use the links to external resources.

** What is Artificial Intelligence?

*** Decision making and interaction
- Recommend a route.
- Buy and sell stocks.
- Make a move in chess.

*** Learning.
- Decisions depend on information.
- How do we incorporate new information?
  
*** Optimisation and Decision Theory
**** Optimal decisions
- Problem-dependent
- Require defining a cost- or utility function
- The optimal solution has the lowest cost or maximal utility

**** Optimisation: algorithms for optimal solutions
- Discrete optimisation.
- Linear optimisation.
- Non-linear optimisation.


*** Statistics and Machine Lerning
- How to learn from data and interactions.
* Single agent problems with no uncertainty
** The agent and the environment
*** Agent
- Obtains stimuli/observations
- Generates actions/decisions
*** Environment
- Reacts to agent's actions
- Generates observations
*** The mind/body interface
- The body can be seen as part of the mind's environment
** Learning and memory
*** Belief state
- Memory
- A summary of the agent's knowledge
- The state in a state machine
- The contents of the tape and read/write heads on a Turing machien.
*** Belief transitions
- 

*** Exercises and Assignments
**** Exercises (From AI3e, 2.7)
- 1. Representations
- 2. Top-level controller.
- 3. Obstacle avoidance.
- 4. Robot trap.
- 10. Autonomous cars: driver preferences
**** Assignments (From AI3e, 2.7)
- 5. Moving targets
- 7. Sensing
- 8. Batteries
- 9. Which functions?
- 11. Autonomous cars: state of the art.
  


** Elementary Decision Theory
*** Preferences
**** Types of rewards                                               :example:
- For e.g. a student: Tickets to concerts.
- For e.g. an investor: A basket of stocks, bonds and currency.
- For everybody: Money.

**** Preferences among rewards
For any rewards $x, y \in R$, we either
- (a) Prefer $x$ at least as much as $y$ and write $x \preceq^* y$.
- (b) Prefer $x$ not more than $y$ and write $x \succeq^* y$.
- (c) Prefer $x$ about the same as $y$ and write $x \eqsim^* y$.
- (d) Similarly define $\succ^*$ and $\prec^*$
  
*** Utility and Cost

**** Utility function
To make it easy, assign a utility $U(x)$ to every reward through a
utility function $U : R \to \Reals$.

**** Utility-derived preferences
We prefer items with higher utility, i.e.
- (a) $U(x) \geq U(y)$ $\Leftrightarrow$ $x \succeq^* y$
- (b) $U(x) \leq U(y)$ $\Leftrightarrow$ $y \succeq^* x$

**** Cost
It is sometimes more convenient to define a cost function $C: R \to \Reals$ so that we prefer items with lower cost, i.e.
- $C(x) \geq C(y)$ $\Leftrightarrow$ $y \succeq^* x$

**** Decision making as an optimisation problem
How can we find the decision maximising utility / minimising cost?

** Few choices and beyond
*** Brute Force
In the simplest case, we need to choose among a finite number of options:
\[
x^* = \argmax_x U(x)
\]
**** Python function
#+BEGIN_SRC python
  # returns the first element maximising U
  import numpy as np
  x_star = np.argmax(U)
#+END_SRC
**** Implementation
#+BEGIN_SRC python
  def argmax(U):
	arg_max = 0
	U_max = U[0]
	for x in range(1, len(U)):
	  if (U[x] > U_max):
		U_max = U[x]
		arg_max = x
  return arg_max
#+END_SRC
*** Large or infinite choices
**** Large, structured choices
- Shortest path: Sequences of places to pass along a route.
- Matching: Assign items to individuals.
- Theorem proving: Prove a mathematical theorem in the simplest possible way.
**** Infinite choices
- Control: Drive a car.


** Structured choices
*** Graph definitions
**** Graph $G = \langle N, A \rangle$
A graph $G$ is defined by:
- Set of *nodes* $N$
- Set of *arcs* $A$, with $\langle x,y \rangle \in A$ and $x, y \in N$
**** Labels and costs
- Nodes can be labelled as e.g. start and goal states.
- Arcs can be labelled according to *actions*
**** Paths and cycles
- A path from $x$ to $y$ in $N$ is a sequence $\langle n_0, \ldots, n_k \rangle$ so that
  $n_0 = x, n_k = y$ and $\langle n_{t}, n_{t+1} \rangle \in A$.
- A cycle is a path $\langle n_0, \ldots, n_k \rangle$ where $n_0 = n_k$.
- If a graph has no cycles, it is *acyclic*
*** Shortest path

**** Costs
- Traversing arc $\langle x,y \rangle$ incurs *costs* $c(\langle x,y \rangle)$
- Following a path $p$ has a total cost
\[
  C(p) = \sum_{\langle x,y \rangle \in p} c(\langle x,y \rangle)
\]

**** The shortest path problem
- Input: a set of *start* nodes $X$ and *goal* nodes $Y$ and edge costs $c: A \to \Reals$.
- Output: Find a path $p$ from $X$ to $Y$ so that $C(p) \leq C(p')$ for all $p$ 

**** Notes
- In the simplest cast $c(a) = 1$ for all arcs.
- We can maximise rewards instead of minimising costs.

*** Depth-first search
  \begin{algorithmic}
	\STATE \textbf{function} \texttt{DepthFirst}($V, F$)
	\FOR {$n \in F$}
	\FOR {$j : \langle n,j \rangle \in A \AND $}
	\STATE \texttt{DepthFirst}($S, F$)
	\ENDFOR 
	\ENDFOR
	\STATE La
  \end{algorithmic}
*** Breath-first search
*** Dynamic programming

** Logic 
*** Logic
**** Statements
- A statement $A$ may be true or false

**** Unary operators
- negation: $\neg A$ is true if $A$ is false (and vice-versa).

**** Binary operators
- or: $A \vee B$ ($A$ or $B$) is true if either $A$ or $B$ are true.
- and: $A \wedge B$ is true if both $A$ and $B$ are true.
- implies: $A \Rightarrow B$: is false if $A$ is true and $B$ is false.
- iff: $A \Leftrightarrow B$: is true if $A,B$ have equal truth values.

**** Operator precedence
$\neg, \wedge, \vee, \Rightarrow, \Leftrightarrow$


*** Set theory
- First, consider some universal set $\Omega$.
- A set $A$ is a collection of points $x$ in $\Omega$.
- $\{x \in \Omega : f(x)\}$: the set of points in $\Omega$ with the property that $f(x)$ is true.

**** Unary operators
- $\neg A =  \{x \in \Omega : x \notin A\}$.
**** Binary operators
- $A \cup B$ if $\{x \in \Omega : x \in A \vee x \in B\}$ - (c.f. $A \vee B$)
- $A \cap B$ if $\{x \in \Omega : x \in A \wedge x \in B\}$ - (c.f. $A \wedge B$)
**** Binary relations
- $A \subset B$ if $x \in A \Rightarrow x \in B$ - (c.f. $A \implies B$)
- $A = B$ if $x \in A \Leftrightarrow x \in B$ - (c.f. $A \Leftrightarrow B$)

*** Knowledge base
**** Syntax and Semtantics
- Syntax: How to construct sentences
- Semantix: What sentences mean
**** Truth
- A statement $A$ is either true or false in any model $m$.
**** Model
- $M(A)$ the set of all models where $A$ is true.
**** Entailment
- $A \models B$ means that $B$ is true whenever $A$ is true.
- $A \models B$ if and only if $M(A) \subseteq M(B)$.
**** Knowledge-Base
- A set of sentences that are true.
**** Inference
- $KB \vdash_i A$: Algorithm $i$ can derive $A$ from KB.
*** Propositional logic syntax
-Sentence $\to$ Atomic | Complex
-Atomic \to True | False | A | B | C | \ldots
-Complex \to (Sentence) | [Sentence]
- | $\neg$  Sentence (not)
- | Sentence $\wedge$ Sentence (and)
- | Sentence $\vee$ Sentence (or)
- | Sentence $\Rightarrow$ Sentence (implies)
- | Sentence $\Leftrightarrow$ Sentence (if and only if)

Precedence: $\neg, \wedge, \vee, \Rightarrow, \Leftrightarrow$

*** Difference between Meta-Logic and Propositional Logic
**** Meta-Logic
- $\alpha \models \beta$: $(\alpha \Rightarrow \beta)$ in every model.
- $\alpha \equiv \beta$: $(\alpha \Leftrightarrow \beta)$ in every model.
**** Propositional Logic
- $A \Rightarrow B$: $A$ implies $B$
- $A \Leftrightarrow B$, $A$ is true iff $B$ is true.
*** Proposition logic semantics
- $A \Rightarrow B \equiv (\neg B \Rightarrow \neg A)$
- $\neg (\neg A) \equiv A$
- $(A \Rightarrow B) \equiv (\neg B \Rightarrow \neg A)$
- $(A \Rightarrow B) \equiv (\neg A \vee B)$


**** For any model $m$:
- $\neg P$ is true iff $P$ is false in $m$.
- $P \wedge Q$ is true iff $P, Q$ are true in $m$.
- $P \vee Q$ is true iff either $P$ or $Q$ is true in $m$.
- $P \Rightarrow Q$ is true unless $P$ is true and $Q$ is false in $m$.
- $P \Leftrightarrow Q$ if $P,Q$ are both true or both false in $m$.

**** Inference Rules
- If $a \Rightarrow b$ and $a$ is true then $b$ is true.
- If $a$ and $b$ is true then $a$ is true.

*** Set theory semantics of propositional logic
**** Atoms as sets 
- Let $\Omega$ be the universal set.
- Any atom $A$ is a subset of $\Omega$.
- Any model $\omega$ is an element of $\Omega$.
**** Definitions
- $A \Rightarrow B$ is equivalent to $A \supset B$.
- $\neg (\neg A) \equiv A$
- $(A \Rightarrow B) \equiv (\neg B \Rightarrow \neg A)$
- $(A \Rightarrow B) \equiv (\neg A \vee B)$

**** For any model $m$:
- $\neg P$ is true iff $P$ is false in $m$.
- $P \wedge Q$ is true iff $P, Q$ are true in $m$.
- $P \vee Q$ is true iff either $P$ or $Q$ is true in $m$.
- $P \Rightarrow Q$ is true unless $P$ is true and $Q$ is false in $m$.
- $P \Leftrightarrow Q$ if $P,Q$ are both true or both false in $m$.


- If $A \subset B$ then, for every $\omega \in A$,  $\omega \in B$.
- If $\omega \in A \cap B$ then $\omega \in A$.
*** Conjunctive Normal Forms
**** Equivalence
Every sentence is equivalent to a conjunction
*** Inference
Let's check if $KB \models A$, i.e. if what we know implies $A$.
From entailment, this means that if our $KB$ is correct, then $A$ must be true.

** Deterministic planning
*** [[https://artint.info/3e/html/ArtInt3e.Ch6.S1.html][States, actions and goals]]
- States $s \in S$
- Actions $a \in A$
- Transition function $\tau : S \times A \to S$
**** STRIPS represnetation
- State: $S \subset \{0,1\}^n$ of $n$ atoms.
- Precondition: $c(s, a) = 1$ if $a$ can be performed in $s$.
- Effect: Assigns values to *some* atoms.
**** Feature-based
- Effect: Transition function $\tau_i: S \times A \to \{0,1\}$ for each $i \in [n]$.

** Infinite choices
*** Lipschitz search
If we know the function $f$ is Lipschitz-smooth, i.e.
\[
\exists L > 0 : |f(x) - f(y)| \leq L |x  - y|,
\]
then we also know that for any point $z$:
\[
f(z) < f(x) + L |x - z|,
\qquad
f(z) < f(y) + L |y - z|
\]
**** Schubert's Algorithm [[https://www.jstor.org/stable/2156138][(Schubert, 1972)]]
\begin{algorithmic}
\STATE \textbf{Input:} $L > 0$, $X$, $x_0 \in X$.
\FOR {$t=1, \ldots, T$}
\STATE $x_{t} = \argmax_{x \in X} \min \cset{f(x_k) + L|x_k - x|}{k=0, \ldots, t-1}$
\ENDFOR
\end{algorithmic}
**** Discussion
- This is guaranteed to \alert{converge} to the optimal solution.
- If $L$ is \alert{unknown}, DIRECT [[http://www.planchet.net/EXT/ISFA/1226.nsf/769998e0a65ea348c1257052003eb94f/f9ca730ca27def69c12576d8002ed895/$FILE/Jones93.pdf][(Jones et al. 1993)]] can be used.
- If $f$ is noisy, the problem becomes a \alert{continuum bandit} problem.
*** First-order gradient methods
- Gradient descent
- Stochastic gradient descent
**** Properties  
- Incremental algorithms
- Can converge to a *local* optimum

*** Single-variable gradient descent
**** Setting
- Input: $f : \Reals \to \Reals$
- Problem: $\max_x f(x)$
- Derivative: $\frac{d}{dx} f(x) \defn \lim_{\Delta \to 0} \frac{f(x + \Delta)  - f(x)}{\Delta}$.
**** Algorithm
1) Input: $x^{(0)}$, f
2) For $t = 1, \ldots$:
3) Calculate direction $g_t = \frac{d}{dx} f(x_{t-1})$
4) Select step-size $\alpha_t$
5) Update $x^{(t)} = x^{(t-1)} + \alpha_t g_t$.
   
*** Multiple-variable gradient descent
**** Setting
- Input: $f : \Reals^d \to \Reals$, $x = (x_1, \ldots, x_d)$
- Problem: $\max_x f(x)$
- Partial Derivative: $\frac{\partial}{\partial x_i} f(x) \defn \lim_{\Delta \to 0} \frac{f(x_1, \ldots, x_i + \Delta, \ldots, x_d)  - f(x)}{\Delta}$.
- Gradient $\nabla_x f(x) = \left[\frac{\partial}{\partial x_1} f(x), \ldots, \frac{\partial}{\partial x_i} f(x), \ldots, \frac{\partial}{\partial x_d} f(x)\right]^\top$.
**** Algorithm
1) Input: $x_0$, f
2) For $t = 1, \ldots$:
3) Calculate direction $g_t = \nabla_x f(x_{t-1})$
4) Select step-size $\alpha_t$
5) Update $x_{t} = x_{t-1} + \alpha_t g_t$.
   
*** Stochastic gradient descent
**** As gradient descent with errors
- Calculate direction $g_t = \nabla_x f(x_{t-1}) + \epsilon_t$
- $\epsilon_t$ is typically zero-mean noise.
**** In learning from data
The gradient can be broken up into a sum of gradients:
\[
f(x) = \sum_t v(x, z_t),
\qquad
\nabla_x f(x) = \sum_t \nabla_x v(x, z_t),
\]
$x_t = x_{t-1} + \alpha_t \nabla_x v(x, z_t)$.
**** In Bayesian quadrature
The function is an expectation:
\[
f(x) = \int_Z v(x, z) p(z) dz.
\qquad
\nabla_x f(x) \approx \sum_t \nabla_x v(x, z_t), 
\]
where $z_t \sim p(z)$ are samples from $p$.

* single agent problems with uncertainty
** Probability
*** Probability fundamentals
**** Probability measure $P$
- Defined on a universe $\Omega$
- $P : \Sigma \to [0,1]$ is a function of subsets of $\Omega$.
- A subset $A \subset \Omega$ is an *event* and $P$ measures its likelihood.
**** Axioms of probability
- $P(\Omega) = 1$
- For $A, B \subset \Omega$, if $A \cap B = \emptyset$ then $P(A \cup B) = P(A) + P(B)$.
**** Marginalisation
If $A_1, \ldots, A_n \subset \Omega$ are a partition of $\Omega$
\[
P(B) = \sum_{i = 1}^n P(B \cap A_i).
\]
** Conditional probability and independence
*** Conditional probability
**** Conditional probability
    :PROPERTIES:
    :BEAMER_env: definition
    :END:
The conditional probability of an event $A$ given an event $B$ is defined as 
\[
P(A | B) \defn \frac{P(A \cap B)}{P(B)}
\]
The above definition requires $P(B)$ to exist and be positive.

**** Conditional probabilities as a collection of probabilities
More generally, we can define conditional probabilities as simply a
collection of probability distributions:
\[
\cset{P_\param(A)}{\theta \in \Param},
\]
where $\Param$ is an arbitrary set. 

*** The theorem of Bayes
**** Bayes's theorem
    :PROPERTIES:
    :BEAMER_env: theorem
    :END:
\[
P(A | B) = \frac{P(B | A)}{P(B)} 
\]
#+BEAMER: \pause

**** The general case
If $A_1, \ldots, A_n$ are a partition of $\Omega$, meaning that they
are mutually exclusive events (i.e. $A_i \cap A_j = \emptyset$ for $i
\neq j$) such that one of them must be true (i.e. $\bigcup_{i=1}^n A_i =
\Omega$), then
\[
P(B) = \sum_{i=1}^n P(B | A_i) P(A_i)
\]
and 
\[
P(A_j | B) = \frac{P(B | A_j)}{\sum_{i=1}^n P(B | A_i) P(A_i)}
\]

*** Independence
**** Independent events
$A, B$ are independent iff $P(A \cap B) = P(A) P(B)$.
**** Conditional independence
 $A, B$ are conditionally independent given $C$ iff $P(A \cap B | C) = P(A | C) P(B | C)$.
** Random variables and expectation 
*** Random variables
A random variable $f : \Omega \to \Reals$ is a real-value function measurable with respect to the underlying probability measure $P$, and we write $f \sim P$.
**** The distribution of $f$
The probability that $f$ lies in some subset $A \subset \Reals$ is
\[
P_f(A) \defn P(\{\omega \in \Omega : f(\omega) \in A\}).
\]
**** Independence
Two RVs $f,g$ are independent in the same way that events are independent:
\[
P(f \in A \wedge g \in B) = P(f \in A) P(g \in B) = P_f(A) P_g(B).
\]
In that sense, $f \sim P_f$ and $g \sim P_g$.

*** Expectation
For any real-valued random variable $f: \Omega \to \Reals$, the expectation with respect to a probability measure $P$ is
\[
\E_P(f) = \sum_{\omega \in \Omega} f(\omega) P(\omega).
\]
**** Linearity of expectations
For any RVs $x, y$:
\[
\E_P(x + y) = \E_P(x) + \E_P(y)
\]
**** Independence
If $x,y$ are independent RVs then $\E_P(xy) = \E(x)\E(y)$.
**** Correlation
If $x,y$ are *not* correlated then $\E_P(xy) = \E(x)\E(y)$.
**** IID (Independent and Identically Distributed) random variables
A sequence $x_t$ of r.v.s is IID if $x_t \sim P$
$(x_1, \ldots, x_t, \ldots, x_T) \sim P^T$.

*** Conditional expectation
The conditional expectation of a random variable $f: \Omega \to \Reals$, with respect to a probability measure $P$ conditioned on some event $B$ is simply
\[
\E_P(f | B) = \sum_{\omega \in \Omega} f(\omega) P(\omega | B).
\]


** Satatistical Decision Theory
*** Expected utility
**** Actions, outcomes and utility
In this setting, we obtain random outcomes that depend on our actions.
- Actions $a \in A$
- Outcomes $\omega \in \Omega$.
- Probability of outcomes $P(\omega \mid a)$
- Utility $U : \Omega \to \Reals$
**** Expected utility
The expected utility of an action is:
\[
\E_P[U \mid a] = \sum_{\omega \in \Omega} U(\omega) P(\omega \mid a).
\]

**** The expected utility hypothesis
We prefer $a$ to $a'$ if and only if
\[
\E_P[U \mid a] \geq \E_P[U \mid a']
\]

** Supervised learning
*** Supervised learning



** Markov decision processes
*** Markov decision process
- Action space $A$.
- State space $S$.
- Transition kernel $s_{t+1} = j \mid s_t = s, a_t = a \sim P_\mdp(j \mid s, a)$.
- Reward $r_t = \rho(s_t, a_t)$ (can also be random).
- Utility
\[
U_t = \sum_{k=t}^T r_t.
\]
*** Value functions
**** The state value function
For any given MDP $\mdp$ and policy $\pol$ we define
\[
V^\pol_{\mdp, t}(s) \defn \E^\pol_{\mdp, t} \left[ U_t ~\middle|~ s_t = s \right]
\]
**** The state-action value function
\[
Q^\pol_{\mdp, t}(s, a) \defn \E^\pol_{\mdp, t} \left[ U_t ~\middle|~ s_t = s, a_t = a \right]
\]
**** The optimal value functions
For an optimal policy $\pol^*$
\[
V^*_{\mdp, t}(s) \defn V^{\pol^*}_{\mdp, t}(s) \geq V^\pol_{\mdp, t}(s),
\qquad
Q^*_{\mdp, t}(s,a) \defn Q^{\pol^*}_{\mdp, t}(s,a) \geq V^\pol_{\mdp, t}(s,a) 
\]
*** The Bellman equations
**** State value function
\begin{align*}
V^\pol_{\mdp, t}(s)
& \defn \E^\pol_{\mdp}[U_{t}\mid s_t = s] \\
& = \E^\pol_{\mdp}[r_t + U_{t+1}\mid s_t = s] \\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \E^\pol_{\mdp}[U_{t+1} \mid s_t = s]\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} \E^\pol_{\mdp}[U_{t+1} \mid s_{t+1} = j] \Pr^\pol_\mdp(s_{t+1} = j \mid s_t = s)\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} V^\pol_{\mdp, t+1}(j)  \Pr^\pol_\mdp(s_{t+1} = j \mid s_t = s)\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} V^\pol_{\mdp, t+1}(j) \sum_{a \in A} P_\mdp(j \mid s, a) \pol(a_t \mid s_t).
\end{align*}
**** State-action value function
\begin{align*}
Q^\pol_{\mdp, t}(s)
&= \rho(s,a) +  \sum_{j \in S} V^\pol_{\mdp, t+1}(j) P_\mdp(j \mid s, a)
\end{align*}

*** Optimal policies

**** Bellman optimality condition
The value function of the optimal policy satisfies this:
\begin{align*}
V^*_{\mdp, t}(s)
& = 
\max_{a}  [\rho(s,a) +  \sum_{j \in S} V^*_{\mdp, t+1}(j) P_\mdp(j \mid s, a)]
\end{align*}
**** Dynamic programming 
To find $V^*, Q^*$, first initialise $V^*_{\mdp, T}(s) = \max_a \rho(s,a)$. 
Then for $t = T-1, T-2, \ldots, 1$:
\begin{align*}
Q^*_{\mdp, t}(s,a) &= \rho(s,a) +  \sum_{j \in S} V^*_{\mdp, t+1}(j) P_\mdp(j \mid s, a).\\
V^*_{\mdp, t}(s) &= \max_a Q^*_{\mdp, t}(s,a).
\end{align*}
**** The optimal policy
The optimal policy is deterministic with:
\[
a_t = \argmax_a Q^*(s_t, a)
\]



* Multiple agent problems with no uncertainty
** Introduction
*** Multi-agent decision making
- *Two* versus $n$-player games
- *Co-operative* games
- *Zero-sum* games
- General-sum games
- *Stochastic* games
- Partial information games

** Two-Player zero-sum Games
*** Extensive-form alternating-move games
- At time $t$:
- Player chooses action $a_t$, which is revealed.
- Player chooses action $b_t$.
- Player $a$ receives $\rho(a_t, b_t)$ and $b$ receives $-\rho(a_t, b_t)$.
The utility for each player is 
$U = \sum_t \rho(a_t, b_t)$.
*** Backwards induction for ZSG 
\begin{algorithmic}
\FOR {$t=T, T-1, \ldots, 1$} 
\STATE x
\ENDFOR
\end{algorithmic}
*** Normal-form simultaneous-move games
- Player $a$ chooses action $a$ in secret.
- Player $b$ chooses action $b$ in secret.
- Players observe both actions
- Player $a$ receives $U(a,b)$, and $b$ receives $-U(a,b)$. 


* Summary of Optimisation methods
** Gradient Descent
$d_t = \nabla_x f(x_t)$.

** Stochastic Gradient Descent
$d_t = \nabla_x f(x_t) + \epsilon_t$.
** Newton's Method

** Simulated Annealing

** Monte-Carlo Methods
** Dynamic Programming and Backwards Induction
** Linear Programming
*** The linear programming problem
Linear programming is a constrained minimisation problem where the objective and the constraints are both linear.
\begin{align*}
\min_x~ & \theta^\top x\\
\textrm{s.t.~} & c^\top x \geq 0.
\end{align*}
We can have
* Books and schedule

Artificial Intelligence: Foundations of Computational Agents, 3rd Edition
Artificial Intelligence: a Modern Approach, 4th Edition

|--------+----------------------------+---------------------------------+------------------------------------|
| Module | Topics                     | AI:FoCA                         | AI: aMA                            |
|--------+----------------------------+---------------------------------+------------------------------------|
|      1 | - Preferences              | 1. AI and Agents                | 2.1. Agents and environments       |
|        | - Utility                  | 1.2. Complexity                 | 2.2 Rationality                    |
|        | - States                   | 1.3. Application domains        | 2.3. Environments                  |
|        | - Actions                  | 1.4. Knowledge representation   | 2.4. Agents                        |
|        | - Beliefs                  | 2. Architecture                 | 2.4.1 Programs                     |
|        | - Fairness                 | 2.1. Control                    | 2.4.2-3 Reflex agents              |
|        |                            | 2.2. Hierarchical control       | 2.4.4. Goals                       |
|        |                            | 2.3. Moral machines             | 2.4.5. Utility                     |
|        |                            |                                 | 2.4.6. Learning                    |
|--------+----------------------------+---------------------------------+------------------------------------|
|      2 | Depth-First Search         | 3. Search                       | 3.1 Problem-solving                |
|        | Breadth-First Search       | 3.1. Search in graphs           | 3.2. Examples.                     |
|        |                            |                                 | 3.3. Best-First search             |
|        |                            |                                 | 3.4.1. Breadth-first               |
|        |                            |                                 | 3.4.3. Depth-first search          |
|--------+----------------------------+---------------------------------+------------------------------------|
|      3 | Heuristic Search           | 3.2. Uninformed search          | 3.5.2. A*                          |
|        | A* Search                  | 3.3. Heuristic search           | 3.6. Heuristic Functions           |
|        |                            |                                 |                                    |
|--------+----------------------------+---------------------------------+------------------------------------|
|      4 | Dynamic Programming        |                                 | 3.4.2 Dijkstra                     |
|        |                            | 3.4. Dynamic programming        |                                    |
|        | Branch and bound           | 3.5. Branch and bound           |                                    |
|--------+----------------------------+---------------------------------+------------------------------------|
|      5 | Constraint programming     | 4. Reasoning with constraints   | 6. CSP                             |
|        |                            | 4.1. Variables and Constraints  |                                    |
|        |                            | 4.2. CSPs and Search            |                                    |
|        |                            | 4.6. Local Search               |                                    |
|        | Deterministic planning     | 4.8. Optimization               |                                    |
|        |                            |                                 |                                    |
|--------+----------------------------+---------------------------------+------------------------------------|
|      6 | Logical reasoning          | 6.1. States, Actions, Goals     | 7. Logical Agents                  |
|        | Deterministic planning     | 6.2. Forward Planning           | 11. Automated planning             |
|        |                            | 6.2. Regressoin Planning        |                                    |
|        |                            | 6.4. Planning as CSP            |                                    |
|        |                            | 6.5. Partial Order Planning     |                                    |
|        |                            |                                 |                                    |
|--------+----------------------------+---------------------------------+------------------------------------|
|      7 | Probability Theory         | 9. Reasoning with Uncertainty   | 12.1. Acting under uncertaint      |
|        | Bayes Theorem              | 9.1. Probability                | 12.2. Basic probability notation   |
|        |                            | 9.2. Independence               | 12.3. Inference                    |
|        |                            |                                 | 12.4. Independence                 |
|        |                            |                                 | 12.5. Bayes's theorem              |
|--------+----------------------------+---------------------------------+------------------------------------|
|      8 | Belief networks            | 9.3. Belief Networks            | 13.1. Representing knowledge       |
|        |                            | 9.4. Probabilistic Inference    | 13.2. Bayesian Networks            |
|        |                            |                                 | 13.3. Exact inference in BNs       |
|--------+----------------------------+---------------------------------+------------------------------------|
|      9 | Expected Utility Theory    | 12.1 Preferences and Utility    | 16.1. Beliefs and Desires          |
|        |                            | 12.2 One-off decisions          | 16.2. utility theory               |
|        |                            |                                 | 16.3. Utility functions            |
|        |                            |                                 | 16.5. Decision networks            |
|--------+----------------------------+---------------------------------+------------------------------------|
|     10 | Markov Decision Processes  | 12.3 Sequential Decisions       | 17.1. Sequential decision problems |
|        | Dynamic Programming        | 12.4 The value of information   | 17.2. Algorithms for MDPs          |
|        |                            | 12.5 Decision processes         |                                    |
|--------+----------------------------+---------------------------------+------------------------------------|
|     11 | Alternating Zero-Sum Games | 14.1. Multi-agent framework     | 5.1. Game Theory                   |
|        | Stochastic Zero-Sum Games  | 14.2. Representations of games  | 5.2. Zero-Sum Games                |
|        |                            | 14.3. Perfect information games | 5.3. Alpha-Beta Search             |
|        |                            |                                 | 5.5. Stochastic Games              |
|--------+----------------------------+---------------------------------+------------------------------------|
|     12 | General Games              | 14.4. Imperfect information     | 18.1. Multiagent environments      |
|        |                            | 14.5. Group decision making     | 18.2. Non-cooperative games        |
|        |                            | 14.6. Mechanism design          | 18.3. Co-operative games           |
|--------+----------------------------+---------------------------------+------------------------------------|




* Book chapters

1. 1-2 Introduction
2. 3.1-3.5 Search, State Spaces, Graphs, Uniformed Search.
3. 3.6. Heuristic Search
4. 4.1-4.2, 4.8 Constrained Search
5. 5.1. Propositions, Constraints, 6.1. Representations, 6.2-6.4. Planning
6. 9.1 Probability, Independence, Belief Networks
7. 12.1. Preferences and Utility, 12.5 Decision Processes
8. Alternative move games
9. Simultaneous games
10. Supervised Learning
11. Reinforcement Learning


