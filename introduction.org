#+TITLE: Artificial Intelligence
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{algorithm,algorithmic}
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \cset[2] {\left\{#1 ~\middle|~ #2 \right\}}
#+LaTeX_HEADER: \newcommand \pol {\pi}
#+LaTeX_HEADER: \newcommand \Pols {\Pi}
#+LaTeX_HEADER: \newcommand \mdp {\mu}
#+LaTeX_HEADER: \newcommand \MDPs {\mathcal{M}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Bels {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \Unif {\textrm{Unif}}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Mult}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Dir {\textrm{Dir}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \Simplex {\mathbb{\Delta}}
#+LaTeX_HEADER: \newcommand \pn {\param^{(n)}}
#+LaTeX_HEADER: \newcommand \pnn {\param^{(n+1)}}
#+LaTeX_HEADER: \newcommand \pnp {\param^{(n-1)}}
#+LaTeX_HEADER: \usetikzlibrary{shapes.geometric}
#+LaTeX_HEADER: \tikzstyle{utility}=[diamond,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=8mm]
#+LaTeX_HEADER: \tikzstyle{select}=[rectangle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{hidden}=[dashed,draw=black,fill=red!10]
#+LaTeX_HEADER: \tikzstyle{RV}=[circle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* Introduction

** About the course  
*** Aims
This course will focus on algorithms and models for Artificial
Intelligence.  We will concentrate mainly on the decision making,
rather than the learning, side of artificial intelligence. Learning is
already addressed in statistics courses, as well as the machine
learning course in the final year.

*** Philosophy
The philosophy of this course is as follows: 
- We give example problems.
- We use theory to explain and generalise from those examples to general problems.
- We describe algorithms to /solve/ general problems.
- We implement algorithms to solve the specific examples.

In general, the course will start from the simplest problems and
slowly progress to the more complex ones.

*** How to use this notebook
- This notebook contains a summary of all the foundational material in the course.
- For details, use the links to external resources.

** What is Artificial Intelligence?

*** Decisions and learning
**** Decision making and interaction
- Recommend a route.
- Buy and sell stocks.
- Make a move in chess.
**** Learning.
- Decisions depend on information.
- How do we incorporate new information?
*** The view from statistics  
**** Optimal decisions
- Problem-dependent
- Require defining a cost- or utility function
- The optimal solution has the lowest cost or maximal utility

**** Optimisation: algorithms for optimal solutions
- Discrete optimisation.
- Linear optimisation.
- Non-linear optimisation.

**** Statistics and Machine Learning
- How to learn from data and interactions.
- Summarising knowledge into a model.
- Using the knowledge to make decisions.

* The agent and the environment
** The agent and the environment
*** Agent-environment interface
**** Agent
- Obtains stimuli/observations $x_t$
- Generates actions/decisions $a_t$
**** Environment
- Reacts to agent's actions
- Generates observations
**** The mind/body interface
- The body can be seen as part of the mind's environment
**** Policy and history
- The agent's next action $a_{t+1}$ depends on previous observation's and actions.
- The policy is implemented through an *algorithm*
** Policies
*** Example policies
**** Reactive maze policy
- Ordered actions $A = \{\textrm{Up}, \textrm{Right}, \textrm{Down}, \textrm{Left}\}$
- Take action $a_{t+1} = a_t$ unless there is a wall in front.
- If there is a wall, take the next action, $a_{t+1} = a_t + 1$.

**** Problems with this policy
- Can it solve any maze?
- Why yes/no?

** Agent structures
*** Example: taking an exam
**** High-level policy
- Study for exam
- Prepare exam materials
- Get to exam on time
- Write 
**** Mid-level policy for getting to the exam:
- Check starting time.
- Check location.
- Select transport option
- Set alarm clock.
- Go to the exam.
**** Low-level policy: Go to the exam.
- Get dressed
- Pick up things
- Get transport
- Go to exam room.
**** Reactive policy: Go to exam room
- Navigate to the exam room.
- Move feet, don't fall down.
- Look around to avoid obstacles.
**** Autonomous policy
- Breathe with lungs.
- Pump blood with heart.

*** Hierarchical control
**** High-level plan

**** Low-level control

*** Learning and memory
**** Belief state
- Memory
- A summary of the agent's knowledge
- The state in a state machine
- The contents of the tape and read/write heads on a Turing machien.
**** Belief transitions
- A (possibly randomised) function $f : B \times A \times X \to B$ 
\[
b_{t+1} = f(b_t, a_t, x_t)
\]
- $b_t \in S$: Belief at time $t$.
- $a_t \in A$: Action at time $t$
- $x_t \in X$: Observation at time $t$.
- $f$ is implemented by the agent's algorithm

** Exercises
*** Exercises and Assignments
**** Exercises (From AI3e, 2.7)
- 1. Representations
- 2. Top-level controller.
- 3. Obstacle avoidance.
- 4. Robot trap.
- 10. Autonomous cars: driver preferences
**** Assignments (From AI3e, 2.7)
- 5. Moving targets
- 7. Sensing
- 8. Batteries
- 9. Which functions?
- 11. Autonomous cars: state of the art.
  


* Designing agents
** Goals
** Elementary Decision Theory
*** Preferences
**** Types of rewards                                               :example:
- For e.g. a student: Tickets to concerts.
- For e.g. an investor: A basket of stocks, bonds and currency.
- For everybody: Money.

**** Preferences among rewards
For any rewards $x, y \in R$, we either
- (a) Prefer $x$ at least as much as $y$ and write $x \preceq^* y$.
- (b) Prefer $x$ not more than $y$ and write $x \succeq^* y$.
- (c) Prefer $x$ about the same as $y$ and write $x \eqsim^* y$.
- (d) Similarly define $\succ^*$ and $\prec^*$
  
*** Utility and Cost
**** Utility function
To make it easy, assign a utility $U(x)$ to every reward through a
utility function $U : R \to \Reals$.

**** Utility-derived preferences
We prefer items with higher utility, i.e.
- (a) $U(x) \geq U(y)$ $\Leftrightarrow$ $x \succeq^* y$
- (b) $U(x) \leq U(y)$ $\Leftrightarrow$ $y \succeq^* x$

**** Cost
It is sometimes more convenient to define a cost function $C: R \to \Reals$ so that we prefer items with lower cost, i.e.
- $C(x) \geq C(y)$ $\Leftrightarrow$ $y \succeq^* x$

**** Decision making as an optimisation problem
How can we find the decision maximising utility / minimising cost?

*** Choice of the utility function
**** Designer input
- The AI designer selects the utility (or goals)
- The choice is not always obvious!
**** The *value-alignment* problem
- The designer selects a utility they *think* is the best choice
- However, their choice results in unintended behaviour
- Example: Autonomous vehicles

**** The value-alignment in *populations*
- 
** Discussion
*** Goals versus preferences
** Exercises
