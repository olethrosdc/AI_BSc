#+TITLE: Multi-Agent Systems
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{algorithm,algorithmic}
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \bR {\matrixsym{R}}
#+LaTeX_HEADER: \newcommand \bp {\vectorsym{\pi}}
#+LaTeX_HEADER: \newcommand \ba {\vectorsym{a}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \cset[2] {\left\{#1 ~\middle|~ #2 \right\}}
#+LaTeX_HEADER: \newcommand \pol {\pi}
#+LaTeX_HEADER: \newcommand \Pols {\Pi}
#+LaTeX_HEADER: \newcommand \mdp {\mu}
#+LaTeX_HEADER: \newcommand \MDPs {\mathcal{M}}
#+LaTeX_HEADER: \newcommand \PC {\texttt{P}}
#+LaTeX_HEADER: \newcommand \NP {\texttt{NP}}
#+LaTeX_HEADER: \newcommand \PPAD {\texttt{PPAD}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Bels {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \Unif {\textrm{Unif}}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Mult}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Dir {\textrm{Dir}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \Simplex {\mathbb{\Delta}}
#+LaTeX_HEADER: \newcommand \pn {\param^{(n)}}
#+LaTeX_HEADER: \newcommand \pnn {\param^{(n+1)}}
#+LaTeX_HEADER: \newcommand \pnp {\param^{(n-1)}}
#+LaTeX_HEADER: \usetikzlibrary{shapes.geometric}
#+LaTeX_HEADER: \tikzstyle{utility}=[diamond,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=8mm]
#+LaTeX_HEADER: \tikzstyle{select}=[rectangle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{hidden}=[dashed,draw=black,fill=red!10]
#+LaTeX_HEADER: \tikzstyle{RV}=[circle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3

* Multi-Agent Systems
** Introduction
*** Multi-agent decision making
- *Two* versus n-player games
- *Co-operative* games
- *Zero-sum* games
- General-sum games
- *Stochastic* games
- Partial information games

*** Rock/Paper/Scissors
#+ATTR_LATEX: :width 0.5\textwidth
[[./figures/rock-paper-scissors.png]]
- Number of players: 2
- Zero-sum.
- Deterministic.
- Simultaneous move.

*** Chess/Go/Checkers/Othello
#+ATTR_LATEX: :width 0.5\textwidth
[[./figures/chess.jpg]]
- Number of players: 2
- Zero-sum
- Deterministic 
- Alternating, Full information

*** Backgammon
#+ATTR_LATEX: :width 0.5\textwidth
[[./figures/backgammon.png]]
- Number of players: 2
- Zero-sum
- Stochastic
- Alernating, Full information

*** Poker/Blackjack
#+ATTR_LATEX: :width 0.5\textwidth
[[./figures/poker.jpg]]
- Number of players: $n$
- Zero-sum
- Stochastic [Partially]
- Alternating, Partial information

*** Doom/Quake/CoD
#+ATTR_LATEX: :width 0.5\textwidth
[[./figures/doom.png]]

- Number of players: $n$
- General sum
- Stochastic
- Simultaneous, Sequential, Partial information

*** Auctions
#+ATTR_LATEX: :width 0.5\textwidth
[[./figures/auction.jpg]]

- Number of players: $n$
- General sum
- Deterministic
- Simultaneous move

*** Humans and AI

Any system involving interaction between multiple agent can be describe through game theory. One question is how to define the preferences of each agent.

**** Human preferences
- These are typically unknown.
- They might not be expressible in mathematical form.
- Nevertheless, we make the utility assumption.
**** AI preferences
- These are typically specified by humans as utilities.
- However, it is hard to fully specify them.


** Game representations
*** Normal form

In the table below, we see how much reward each player obtains for every combination of actions
| $\rho^1, \rho^2$ | b = 0 | b = 1 |
|------------------+-------+-------|
| /                | <     | <     |
| a = 0            | 2, 1  | 4, 0  |
|------------------+-------+-------|
| a = 1            | 1, 0  | 3, 1  |

**** Simultaneous moves
We assume that each player is playing without seeing the move of the other player. 
**** Commitment
However, we can also look at *commitment* or *Stackleberg* games, where one player either /commits/ to playing a move, or plays before the other player.
**** Information structure
For other types of move sequencing, we have to encode the information structure of a game as a graph.
*** Co-operative, adversarial and general games
More generally, we can say that every player $i$ in the game:
- Takes an action $a^i \in A_i$.
- Obtains a reward $\rho^i(x)$ for each possible outcome/choice $x$.

#+BEAMER: \pause
**** 2-player Zero-sum games
- $\rho^1 = -\rho^2$
- Can be solved effiicently.
#+BEAMER: \pause
**** n-player Collaborative games
- $\rho^i = \rho^j$ for all players $i,j$.
- If the players can co-ordinate, then it reduces to a single-agent problem with action-space $A = A_1 \times \cdots A_n$.
#+BEAMER: \pause
**** n-player General-sum games
- $\rho^i$ can be anything.
- Finding solutions for these games is harder.

*** Zero-Sum: Rock Paper Scissors
| $\rho^1, \rho^2$ | Rock  | Paper | Scissors |
|------------------+-------+-------+----------|
| /                | <     | <     | <        |
| Rock             | 0, 0  | -1, 1 | 1, -1    |
|------------------+-------+-------+----------|
| Paper            | 1, -1 | 0, 0  | -1, 1    |
|------------------+-------+-------+----------|
| Scissors         | -1, 1 | 1, -1 | 0, 0     |



*** Co-operative: Party

People want to bring something to the party.  Ideally, one brings
food, and the other drinks.  But if they do not co-ordinate, then
there is only food, or only drink.


| $\rho^1, \rho^2$ | food   | drink    |
|------------------+--------+----------|
| /                | <      | <        |
| food             | 2, 2   | 10, 10   |
|------------------+--------+----------|
| drink            | 10, 10 | 1, 1     |

Here, co-ordination makes the outcomes better for everybody.


*** General-Sum: Prisoner's dilemma
| $\rho^1, \rho^2$ | cooperate | defect   |
|------------------+-----------+----------|
| /                | <         | <        |
| cooperate        | -1, -1    | -5, 0    |
|------------------+-----------+----------|
| defect           | 0, -5     | -3, -3   |


*** Basic concepts in normal form games

| $\rho^1, \rho^2$ | b = 0 | b = 1 |
|------------------+-------+-------|
| /                | <     | <     |
| a = 0            | 2, 1  | 4, 0  |
|------------------+-------+-------|
| a = 1            | 1, 0  | 3, 1  |

**** Domination and best response
- $b = 1$ is a *best response* to $a = 1$, i.e. $\rho^2(1,1) > \rho^2(1,0)$
- $a = 0$ is a *strictly dominant* strategy. Given any $b$, it is strictly better to play $a = 0$, i.e. $\rho^1(0, b) > \rho^1(1, b)$.
- If a pair $(a,b)$ is /not dominated/, then it is *Pareto*-efficient.
  
**** Questions
- How much reward can $a$ obtain?
- Does $b$ have a dominant strategy?
- Does this take into account what $b$ likes?



*** Pareto-Optimality
[[./figures/Pareto.pdf]]

*** Commitment

Let us see what happens when one player *commits* to a move
| $\rho^1, \rho^2$ | b = 0 | b = 1 |
|------------------+-------+-------|
| /                | <     | <     |
| a = 0            | 2, 1  | 4, 0  |
|------------------+-------+-------|
| a = 1            | 1, 0  | 3, 1  |

**** Player $a$ is first
    :PROPERTIES:
    :BEAMER_col: 0.5
    :BEAMER_env: block
    :END:
- What should $b$ play?
- What is $a$'s best move?

\begin{tikzpicture}[level 1/.style={sibling distance=8em},
   level 2/.style={sibling distance=4em},
   level 3/.style={sibling distance=2em}]
   \node[select]{$a$}
   child {node[select] {$b$}
     child {node[RV] {2,1} edge from parent node[left] {0}}	
     child {node[RV] {4,0} edge from parent node[right] {1}}
     edge from parent
     node[left] {0}
   }    	    
   child {node[select] {$b$}
     child {node[RV] {1,0} edge from parent node[left] {0}}	
     child {node[RV] {3,1} edge from parent node[right] {1}}
     edge from parent
     node[right] {1}
   };     	    
\end{tikzpicture}



**** Player $b$ is first
    :PROPERTIES:
    :BEAMER_col: 0.5
    :BEAMER_env: block
    :END:
- What should $a$ play in each case?
\begin{tikzpicture}[level 1/.style={sibling distance=8em},
   level 2/.style={sibling distance=4em},
   level 3/.style={sibling distance=2em}]
   \node[select]{$b$}
   child {node[select] {$a$}
     child {node[RV] {2,1} edge from parent node[left] {0}}	
     child {node[RV] {1,0} edge from parent node[right] {1}}
     edge from parent
     node[left] {0}
   }     	    
   child {node[select] {$a$}
     child {node[RV] {1,0} edge from parent node[left] {0}}	
     child {node[RV] {3,1} edge from parent node[right] {1}}
     edge from parent
     node[right] {1}
   };    	    
\end{tikzpicture}




* Two-Player zero-sum Games
*** Extensive-form alternating-move game
**** Variables                                                        :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.4
     :END:
- The *state* $s_t \in S$.
- The *actions* $a^i_t \in A$.
- The *rewards* $r^i_t \in \Reals$, $r_t = (r^1_t, r^2_t)$.
- The transition probabilities
\[
\Pr(s_{t+1} \mid s_t , a_{t-1}^i)
\]
#+BEAMER: \pause
**** Bayesian network                                                 :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.6
     :END:
\begin{tikzpicture}
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \node[select] at (0,2) (atp) {$a^1_{t-1}$};
  \node[select] at (2,2) (at) {$a^2_t$};
  \node[utility] at (2,1) (rtp) {$r_{t}$};
  \node[utility] at (4,1) (rt) {$r_{t+1}$};
  \draw[->] (atp) -- (st);
  \draw[->] (atp) -- (rtp);
  \draw[->] (stp) -- (rtp);
  \draw[->] (at) -- (rt);
  \draw[->] (st) -- (rt);
  \draw[->] (at) -- (stn);
  \draw[->] (stp) -- (st);
  \draw[->] (st) -- (stn);
\end{tikzpicture}



*** Extensive-form alternating-move zero sum games
#+ATTR_BEAMER: :overlay <+->
- At time $t$:
- The state is $s_t$, players receive rewards $r^1_t = \rho(s_t), r^2_t = -\rho(s_t)$
- Player $i$ chooses action $a^i_t$, which is revealed.
- The state changes to $s_{t+1}$, and is revealed.
- Players receive reward $\rho(s_{t+1}), -\rho(s_{t+1})$
- Player $j = 1 - i$ chooses action $a^j_{t+1}$.
- The state changes to $s_{t+2}$.
- Player 1 receives $\rho(s_t)$ and 2 receives $-\rho(s_t)$.
#+BEAMER: \pause
The utility for player 1 is 
\[
U^{1} = \sum_t \rho(s_t),
\]
while for 2 it is
\[
U^{2} = -\sum_t \rho(s_t)
\]
*** Backwards induction for Alternating Zero Sum Games
Let *$\pi_1$* and *$\pi_2$* be the policies of *each* player and *$\pi$* the *joint* policy.
#+BEAMER: \pause
**** The value function of a policy $\pi = (\pi_1, \pi_2)$
For the utility of player 1, we get:
\begin{align}
V^{1,\pol}_t(s) 
&\defn \E_\pi [U^{1}_t \mid s_t = s]
= \rho(s) + \E[U^{1}_{t+1} \mid s_t = s]
\uncover<3->{
\\
&= \rho(s) + \sum_{a^1} \pol(a^1 \mid s) \sum_{j} V^{1,\pi}_{t+1}(j) P(j\mid s, a^1)
}
\uncover<4->{\\
V^{1,\pol}_{t+1}(j) 
&= \rho(j) + \sum_{a^2} \pol(a^2 \mid j)  \sum_{j} V^{1,\pi}_{t+2}(j) P(k \mid j, a^2)
}
\end{align}
*** The optimal value function
We can define the optimal value function analogously to MDPs, but 
player 2 is minimising. 

The value for player 1, together with the recursion is given below:
#+BEAMER: \pause
\begin{align}
V^{1,*}_t(s) 
&= \max_{\pi_1} \min_{\pi_2} \E_\pi [U^{1}_t \mid s_t = s]
\uncover<3->{\\
&= \rho(s) + \max_{a^1} \sum_{j} V^{1,*}_{t+1}(j) P(j\mid s, a^1)
}
\uncover<4->{\\
V^{1,*}_{t+1}(j) 
&= \rho(j) + \min_{a^2}  \sum_{j} V^{1,*}_{t+1}(j) P(k \mid j, a^2)
}
\end{align}


*** Normal-form simultaneous-move zero-sum games
    (Also called *minimax* games)
- Player $a$ chooses action $a$ in secret.
- Player $b$ chooses action $b$ in secret.
- Players observe both actions
- Player $a$ receives $\rho(a,b)$, and $b$ receives $-\rho(a,b)$. 

**** Mixed strategies
Each player chooses an action randomly, independently of one another:
\[
\pol(a, b) = \pol_1(a) \pol_2(b)
\]
$\pol_i$ is called a *mixed* strategy.
*** Optimal strategies for zero-sum games
**** The value of a strategy pair
The expected value of the game for the first player is
\[
V(\pol_1, \pol_2) \defn \sum_{a,b} \pol_1(a) \rho(a, b) \pol_2(b) = \bp_1^\top \bR \bp_2,
\]
where $\bp_i$ is the vector form representation of $i$'s strategy.
**** The value of the game
Any zero-sum game has at least one solution $\pol^*$ over mixed strategies so that
\[
V(\pol^*_1, \pol^*_2)
= \max_{\pol_1} \min_{\pol_2} V(\pol_1, \pol_2)
= \min_{\pol_2} \max_{\pol_1}  V(\pol_1, \pol_2)
\]
The problem can be solved through *linear programming*

The idea is to set find a policy corresponding to the greatest lower bound (or lowest upper bound) on the value.

*** Linear programming solution for ZSG
****  linear programming problem
This is an optimisation problem with linear objective and constraints. In *canonical form* it is written as:
\begin{align*}
\min_x~ & \theta^\top x,&
\textrm{s.t.~} & c^\top x \geq 0.
\end{align*}

**** Primal formulation
Find the higest lower bound for player 1
\begin{align*}
\max_v~&v,& 
\textrm{s.t.~} &(\bR \bp_2)_j \geq v ~ \forall j, ~\sum_j \pi_{2}(j) = 1, \pi_{2}(j) \geq 0
\end{align*}

**** Dual formulation
Find the lowest upper bound for player 2
\begin{align*}
\min_v~&v,& 
\textrm{s.t.~} &(\bp_1^\top \bR)_j \leq v ~ \forall j, ~\sum_j \pi_{1}(j) = 1, \pi_{1}(j) \geq 0
\end{align*}




* General sum games
** Normal-form games
*** Normal-form general sum games
**** Game structure
- Each player $i$ chooses action $a^i \in A_i$ in secret.
- The *joint action* is $\ba = (a^1, \ldots, a^n)$.
- The players then receive a reward $\rho^i(\ba)$
**** Mixed strategies
Players can independently draw actions $a^i$ from $\pol(a^i)$
The expected utility of the strateg

*** Example: penalty shot
| $\rho^1, \rho^2$ | kick left | kick right |
|------------------+-----------+------------|
| /                | <         | <          |
| dive left        | 1, -1     | -1, 1      |
|------------------+-----------+------------|
| dive right       | -1 1      | 1, -1      |


*** Example: Chicken
| $\rho^1, \rho^2$ | turn   | dare     |
|------------------+--------+----------|
| /                | <      | <        |
| turn             | 0, 0   | -1, +1   |
|------------------+--------+----------|
| dare             | +1, -1 | -10, -10 |

*** Example: Prisoner's dilemma
| $\rho^1, \rho^2$ | cooperate | defect   |
|------------------+-----------+----------|
| /                | <         | <        |
| cooperate        | -1, -1    | -5, 0    |
|------------------+-----------+----------|
| defect           | 0, -5     | -3, -3   |





*** Computing Nash equlibria


- A Nash equilibrium always exists (Nash, 1950)
- Nash is $\PPAD$, with $\PC \subseteq \PPAD \subseteq \NP$
**** The Brouwer problem (PPAD)
Input:
- a function $F : [0,1]^m \to [0,1]^m$
- $L \in (0,1)$ is a *Lipschitz constant* such that $\|F(x) - F(x')\| \leq L \|x - x'\|$
- An $\epsilon >0$
Output:
- $x^*$ such that $\|F(x^*) - x^*\| \leq \epsilon$.
**** The connection with Nash
- Given by Nash himself in his 1950 proof.
- The *fixed point* of $F$ is the *Nash equlibrium*
*** The Linear Complementarity Problem
- $\sum_b \rho^1(a,b) \pi_2(b) + s_1(a)= v_1$ for all $a$
- $\sum_a \rho^2(j,b) \pi_1(a) + s_2(b) = v_1$ for all $b$
- $\|\pi_i\|_1 = $, $\pi_i \geq 0$
- $s \geq 0$
- $\pi_i \cdot s_i = 0$: assigns zero to slack variables corresponding to actions with probability > 0
*** Optimistic hedge
**** Hedge
\[
w_{t+1} \propto w_t * exp(\eta r_t)
\]
**** Optimistic hedge
\[
x_{t+1} \propto x_t * \exp(\eta r_{t-1} - 2r_t)
\]


** Extensive-form games
*** Extensive-form general sum games
- At time $t$:
- The state is $s_t$, players receive rewards $\rho^i(s_t)$.
- Player $i = I(s_t)$ chooses an action.
- The state changes to $s_{t+1}$, and is revealed.
The utility for each player is
\[
U^{i} = \sum_t \rho^i(s_t)
\]

*** Backwards induction for Alternating General Sum Games
Let *$\pi_i$* be the policy of the \(i\)-th player and *$\pi$* the *joint* policy.
**** The value function of a policy $\pi = (\pi_i)_{i=1}^n$
For any player $i$, we can define their value at time $t$ as:
\begin{align}
V^{i,\pol}_t(s) 
&\defn \E_\pi [U^{i}_t \mid s_t = s]\\
&= \rho^i(s) + \sum_{a \in A} \pol_{I(s)}(a \mid s) \sum_{j} V^{1,\pi}_{t+1}(j) P(j\mid s, a)
\end{align}

**** Optimal policies
For *perfect information* games, we can use this recursion:
\begin{align}
a^*_t(s) &= 
 \argmax_{a \in A} 
 \sum_{j} V^{I(s),*}_{t+1}(j)P(j\mid s, a)\\
V^{i,*}_t &=  \rho^i(s) +  \sum_{j} V^{i,\pi}_{t+1}(j) P(j\mid s, a^*_t(s)) &&\forall i
\end{align}
This ensures that we update the values of *all players* at each step.



