#+TITLE: Decisions and randomness
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \input{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSubsection[]{\begin{frame}<beamer>\tableofcontents[currentsubsection]\end{frame}}

* Statistical Decision Theory
  #+TOC: headlines [currentsection]
** Elementary Decision Theory
*** Preferences
**** Types of rewards                                               :example:
- For e.g. a student: Tickets to concerts.
- For e.g. an investor: A basket of stocks, bonds and currency.
- For everybody: Money.

**** Preferences among rewards
For any rewards $x, y \in R$, we either
- (a) Prefer $x$ at least as much as $y$ and write $x \preceq^* y$.
- (b) Prefer $x$ not more than $y$ and write $x \succeq^* y$.
- (c) Prefer $x$ about the same as $y$ and write $x \eqsim^* y$.
- (d) Similarly define $\succ^*$ and $\prec^*$
  
*** Utility and Cost
**** Utility function
To make it easy, assign a utility $U(x)$ to every reward through a
utility function $U : R \to \Reals$.

**** Utility-derived preferences
We prefer items with higher utility, i.e.
- (a) $U(x) \geq U(y)$ $\Leftrightarrow$ $x \succeq^* y$
- (b) $U(x) \leq U(y)$ $\Leftrightarrow$ $y \succeq^* x$

**** Cost
     It is sometimes more convenient to define a cost function $C: R \to \Reals$ so that we prefer items with lower cost, i.e.
- $C(x) \geq C(y)$ $\Leftrightarrow$ $y \succeq^* x$

*** Random outcomes
**** Choosing among rewards
-[A] Bet 10 CHF on black
-[B] Bet 10 CHF on 0
-[C] Bet nothing
What is the reward here?

**** Choosing among trips
-[A] Taking the car to Zurich (50' without delays, 80' with delays)
-[B] Taking the train to Zurich (60' without delays)
What is the reward here? 

**** Random rewards
- Each gamble gives us different rewards with different probabilities.
- These rewards are then *random*
- For simplicity, we assign a real-valued *utility* to outcomes. This is a *random variable*
** Random variables, expectation and variance
*** Random variables
A random variable $f : \Omega \to \Reals$ is a real-valued *function*, with $\omega \sim P$.
**** The distribution of $f$
The probability that $f$ lies in some subset $A \subset \Reals$ is
\[
P_f(A) \defn P(\{\omega \in \Omega : f(\omega) \in A\}),
\]
and we write $f \sim P_f$. 
**** Shorthands for RV
- For RVs $f : \Omega \to \Reals$, we write $P(f \in A)$ to mean $P_f(A)$.
- For RVs $f : \Omega \to X$, where $X$ is a finite set e.g. $\{1, 2, \ldots, n\}$, we write $P(f = x) = P_f(\{x\})$ for any $x \in X$.
*** Independence of random variables

Two RVs $f,g$ are independent in the same way that events are independent. 
\[
P(f \in A \wedge g \in B) = P(f \in A) P(g \in B) = P_f(A) P_g(B).
\]
In that sense, $f \sim P_f$ and $g \sim P_g$. 
**** Formal definition
More specifically, we are measuring the set of \(\omega\) values for which $f(\omega) \in A$ and $g(\omega) \in B$:
\[
P(\{\omega : f(\omega) \in A, g(\omega) \in B\}) = P_f(A) P_g(B).
\]
**** Shorthand notation
Since the above is very cumbersome, we usually just write that
\[
P(f, g) = P(f) P(g)
\]
for any two independent random variables $f, g$.
*** Expectation
For any real-valued random variable $f: \Omega \to \Reals$, the expectation with respect to a probability measure $P$ is
\[
\E_P(f) = \sum_{\omega \in \Omega} f(\omega) P(\omega).
\]
When $\Omega$ is continuous, we can use a density $p$
\[
\E_P(f) = \int_{\Omega} f(\omega) p(\omega) d\omega.
\]
**** Linearity of expectations
For any RVs $x, y$:
\[
\E_P(x + y) = \E_P(x) + \E_P(y)
\]
*** Multiple variables
**** The joint distribution $P(x,y)$
For two (or more) RVs $x : \Omega \to \Reals$, and $y : \Omega \to
 \Reals$, this is a *shorthand* for the distribution of $(x(\omega),
 y(\omega))$ when $\omega \sim P$. We can also use $P(x = i, y = j)$ for the probability that the two variables assume the values $i, j$ respectively.
**** Independence
If $x,y$ are independent RVs then $P(x,y) = P_x(x) P_y(y)$.
**** Correlation
If $x,y$ are *not* correlated then $\E_P(xy) = \E(x)\E(y)$.
**** IID (Independent and Identically Distributed) random variables
A sequence $x_t$ of r.v.s is IID if $x_t \sim P$
so that
\[
(x_1, \ldots, x_t, \ldots, x_T) \sim P^T
\]
i.e. a \(T\)-length sample is drawn from the product distribution $P^T = P \times P \times \cdots \times P$.
*** Conditional expectation
The conditional expectation of a random variable $f: \Omega \to \Reals$, with respect to a probability measure $P$ conditioned on some event $B$ is simply
\[
\E_P(f | B) = \sum_{\omega \in \Omega} f(\omega) P(\omega | B).
\]
Conditional expectations are similar to conditional probabilities.
*** Conditional probabilities of RVs
Similarly to the notation over sets,
\[
P(A \cap B) = P(A \mid B) P(B),
\]
when dealing with RVs, it is common to use the notation
\[
P(x, y) = P(x | y) P(y)
\]
This equation works for all possible values of $x, y$ e.g.
\[
P(x = 1, y = 0) = P(x = 1 | y = 0) P(y = 0)
\]
which then denotes the probability msas of each

** Statistical Decision Theory

*** Expected utility
**** Actions, outcomes and utility
In this setting, we obtain random outcomes that depend on our actions.
- Actions $a \in A$
- Outcomes $\omega \in \Omega$.
- Probability of outcomes $P(\omega \mid a)$
- Utility $U : \Omega \to \Reals$
**** Expected utility
The expected utility of an action is:
\[
\E_P[U \mid a] = \sum_{\omega \in \Omega} U(\omega) P(\omega \mid a).
\]

**** The expected utility hypothesis
We prefer $a$ to $a'$ if and only if
\[
\E_P[U \mid a] \geq \E_P[U \mid a']
\]

*** The St-Petersburg Paradox
**** The game
If you give me $x$ CHF, then I promise to
(a) Throw a fair coin until it comes heads.
(b) If it does so after $T$ throws, then I will give you $2^T$ CHF.
**** The question
- How much $x$ are you willing to pay to play?
- Given that the expected amount of money is infinite, why are you only willing to pay a small $x$?

*** Example: Betting
 In this example, probabilities reflect actual randomness

|------------+---------------------+------------+---------------|
| Choice     | Win Probability $p$ | Payout $w$ | Expected gain |
|------------+---------------------+------------+---------------|
| Don't play | 0                   |          0 |             0 |
| Black      | 18/37               |          2 |               |
| Red        | 18/37               |          2 |               |
| 0          | 1/37                |         36 |               |
| 1          | 1/37                |         36 |               |
|------------+---------------------+------------+--------------- |

#+ATTR_LATEX: width=\textwidth
[[./figures/roulette.jpg]]
What are the expected gains for these bets?
*** Example: Route selection
- In this example, probabilities reflect subjective beliefs

|--------------+-----------+-----------------+--------------+---------------|
| Choice       | Best time | Chance of delay | Delay amount | Expected time |
|--------------+-----------+-----------------+--------------+---------------|
| Train        |        80 | 5%              |            5 |               |
| Car, route A |        60 | 50%             |           30 |               |
| Car, route B |        70 | 10%             |           10 |               |
|--------------+-----------+-----------------+--------------+---------------|

*** Example: Estimation
- In this example, probabilities are calculated starting from subjective beliefs
**** Mean-Square Estimation
If we want to guess $\hat{\param}$, and we knew that $\param \sim P$, then the guess
\[
\hat{\param} = \E_P(\param) = \argmin_{\hat{\param}} \E_P [(\param - \hat{\param})^2]
\]


*** Example: Noisy optimisation
We wish to find the maximum of a function
\begin{align}
f(x) &\defn \E[g | x],
&
\E[g | x] = \int_{- \infty}^\infty g(\omega, x) p(\omega) d\omega
\end{align}

