#+TITLE: Decisions and randomness
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \input{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSubsection[]{\begin{frame}<beamer>\tableofcontents[currentsubsection]\end{frame}}

* Statistical Decision Theory
** Elementary Decision Theory
*** Preferences
**** Types of rewards                                               :example:
- For e.g. a student: Tickets to concerts.
- For e.g. an investor: A basket of stocks, bonds and currency.
- For everybody: Money.

**** Preferences among rewards
For any rewards $x, y \in R$, we either
- (a) Prefer $x$ at least as much as $y$ and write $x \preceq^* y$.
- (b) Prefer $x$ not more than $y$ and write $x \succeq^* y$.
- (c) Prefer $x$ about the same as $y$ and write $x \eqsim^* y$.
- (d) Similarly define $\succ^*$ and $\prec^*$
  
*** Utility and Cost
**** Utility function
To make it easy, assign a utility $U(x)$ to every reward through a
utility function $U : R \to \Reals$.

**** Utility-derived preferences
We prefer items with higher utility, i.e.
- (a) $U(x) \geq U(y)$ $\Leftrightarrow$ $x \succeq^* y$
- (b) $U(x) \leq U(y)$ $\Leftrightarrow$ $y \succeq^* x$

**** Cost
     It is sometimes more convenient to define a cost function $C: R \to \Reals$ so that we prefer items with lower cost, i.e.
- $C(x) \geq C(y)$ $\Leftrightarrow$ $y \succeq^* x$

*** Random outcomes
**** Choosing among rewards
-[A] Bet 10 CHF on black
-[B] Bet 10 CHF on 0
-[C] Bet nothing
What is the reward here?

**** Choosing among trips
-[A] Taking the car to Zurich (50' without delays, 80' with delays)
-[B] Taking the train to Zurich (60' without delays)
What is the reward here? 

**** Random rewards
- Each gamble gives us different rewards with different probabilities.
- These rewards are then *random*
- For simplicity, we assign a real-valued *utility* to outcomes. This is a *random variable*
** Statistical Decision Theory

*** Expected utility
**** Actions, outcomes and utility
In this setting, we obtain random outcomes that depend on our actions.
- Actions $a \in A$
- Outcomes $\omega \in \Omega$.
- Probability of outcomes $P(\omega \mid a)$
- Utility $U : \Omega \to \Reals$
**** Expected utility
The expected utility of an action is:
\[
\E_P[U \mid a] = \sum_{\omega \in \Omega} U(\omega) P(\omega \mid a).
\]

**** The expected utility hypothesis
We prefer $a$ to $a'$ if and only if
\[
\E_P[U \mid a] \geq \E_P[U \mid a']
\]

*** The St-Petersburg Paradox
**** The game
If you give me $x$ CHF, then I promise to
(a) Throw a fair coin until it comes heads.
(b) If it does so after $T$ throws, then I will give you $2^T$ CHF.
**** The question
- How much $x$ are you willing to pay to play?
- Given that the expected amount of money is infinite, why are you only willing to pay a small $x$?

*** Example: Betting
 In this example, probabilities reflect actual randomness

|------------+---------------------+------------+---------------|
| Choice     | Win Probability $p$ | Payout $w$ | Expected gain |
|------------+---------------------+------------+---------------|
| Don't play | 0                   |          0 |             0 |
| Black      | 18/37               |          2 |               |
| Red        | 18/37               |          2 |               |
| 0          | 1/37                |         36 |               |
| 1          | 1/37                |         36 |               |
|------------+---------------------+------------+--------------- |

#+ATTR_LATEX: width=\textwidth
[[./figures/roulette.jpg]]
What are the expected gains for these bets?
*** Example: Route selection
- In this example, probabilities reflect subjective beliefs

|--------------+-----------+-----------------+--------------+---------------|
| Choice       | Best time | Chance of delay | Delay amount | Expected time |
|--------------+-----------+-----------------+--------------+---------------|
| Train        |        80 | 5%              |            5 |               |
| Car, route A |        60 | 50%             |           30 |               |
| Car, route B |        70 | 10%             |           10 |               |
|--------------+-----------+-----------------+--------------+---------------|

*** Example: Estimation
- In this example, probabilities are calculated starting from subjective beliefs
**** Mean-Square Estimation
If we want to guess $\hat{\param}$, and we knew that $\param \sim P$, then the guess
\[
\hat{\param} = \E_P(\param) = \argmin_{\hat{\param}} \E_P [(\param - \hat{\param})^2]
\]
minimises the squared error. This is because
\begin{align}
\frac{d}{d \hat{\theta}}
 \E_P [(\param - \hat{\param})^2]
&=
\frac{d}{d \hat{\theta}}
 \sum_\omega [\theta(\omega) -  \hat{\param}]^2 P(\omega)\\
&=
 \sum_\omega \frac{d}{d \hat{\theta}}
 [\theta(\omega) -  \hat{\param}]^2 P(\omega)\\
&=
 \sum_\omega 2 [\theta(\omega) -  \hat{\param}] (-1) P(\omega)
&=
	 2 (\hat{\param} - \E_P [\theta]).
\end{align}
Setting this to $0$ gives $\hat{\param} - \E_P [\theta]$

*** Example: Noisy optimisation
We wish to find the maximum of a function
\begin{align}
f(x) &\defn \E[g | x],
&
\E[g | x] = \int_{- \infty}^\infty g(\omega, x) p(\omega) d\omega
\end{align}

For this problem we need to use some more complex optimisation method, such as gradient methods
* Gradient methods
** Gradients for optimisation
*** The gradient descent method: one dimension
- Function to minimise $f : \Reals \to \Reals$.
- Derivative $\frac{d}{d \param} f(\beta)$
**** Gradient descent algorithm
- Input: initial value $\param^0$, *learning rate* schedule $\alpha_t$
- For $t=1, \ldots, T$
  - $\param^{t+1} = \param^t - \alpha_t \frac{d}{d \param} f(\param^t)$
- Return $\param^T$

**** Properties
- If $\sum_t \alpha_t = \infty$ and $\sum_t \alpha_t^2 < \infty$, it finds a local minimum $\param^T$, i.e. there is $\epsilon > 0$ so that
\[
f(\param^T) < f(\param), \forall \param: \|\param^T - \param\| < \epsilon.
\]
*** Gradient methods for expected value :example:
**** Estimate the expected value
$x_t \sim P$ with $\E_P[x_t] = \mu$.
#+BEAMER: \pause
**** Objective: mean squared error
Here $\ell(x, \param) = (x - \param)^2$.
\[
\min_\param \E_P[(x_t - \param)^2].
\]
#+BEAMER: \pause
**** Exact gradient update
If we know $P$, then we can calculate
\begin{align}
\param^{t+1} &= \param^t - \alpha_t \frac{d}{d\param} \E_P[(x - \param^t)^2]\\
\frac{d}{d\param} \E_P[(x - \param^t)^2] &= 2 \E_P[x] - \param^t
\end{align}

*** Gradient for mean estimation :example:
- Let us show this in detail
\begin{align*}
 \frac{d}{d\param} \E_P [(x - \param)^2] 
&= \int_{-\infty}^\infty dP(x) \frac{d}{d\param} (x - \param)^2
\\
&=  \int_{-\infty}^\infty dP(x) 2(x - \param)
\\
&=  2 \E_P[x] - 2\param.
\end{align*}
- If we set the derivative to zero, then we find the optimal solution:
\[
\param^* = \E_P[x]
\]
- How can we do this if we only have data $x_t \sim P$?
*** Mean-squared error cost function
\begin{tikzpicture}[domain=-1:2, range=-1:2]
   \draw[dotted, color=gray] (-1.1,-2.1) grid (3.1,4.1);
   \draw[->] (0,0) -- (2,0) node[right] {$\beta$};
   \draw[->] (0,0) -- (0,4) node[above] {$\ell$};
   \draw[color=red] plot (\x, {(\x-1)^2})  node[right] {$\mu = 1$};
   \draw[color=blue] plot (\x, {(\x)^2})  node[right] {$\mu = 0$};
\end{tikzpicture}
Here we see a plot of $\ell(\mu, \beta) = (\beta - \mu)^2$.
*** Stochastic gradient for mean estimation
**** Sampling :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
For any bounded random variable $f$, 
\[
\E_P[f] = \int_{X} dP(x) f(x)
 = 
\lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T f(x_t)
 = 
\E_P \left[\frac{1}{T} \sum_{t=1}^T f(x_t)\right]
, \qquad x_t \sim P
\]
**** Sampling :B_example:
     :PROPERTIES:
     :BEAMER_env: example
     :END:
- If we sample $x$ we approximate the gradient:
\begin{align*}
 \frac{d}{d\param} \E_P [(x - \param)^2] 
= \int_{-\infty}^\infty \!\!\!\! dP(x) \frac{d}{d\param} (x - \param)^2
\approx \frac{1}{T} \sum_{t=1}^T \frac{d}{d\param} (x_t - \param)^2
= \frac{1}{T} \sum_{t=1}^T 2(x_t - \param)
\end{align*}
#+BEAMER: \pause
- If we update $\param$ after each new sample $x_t$, we obtain:
\[
\param^{t+1} = \param^t + 2 \alpha_t (x_t - \param^t)
\]

*** The gradient method
- Function to minimise $f : \Reals^n \to \Reals$.
- Derivative $\nabla_\param f(\param)  = \left(\frac{\partial f(\param)}{\partial \param_1}, \ldots, \frac{\partial f(\param)}{\partial \param_n}\right)$,
 where $\frac{\partial f}{\partial \beta_n}$ denotes the *partial* derivative, i.e. varying one argument and keeping the others fixed.
**** Gradient descent algorithm
- Input: initial value $\param^0$, learning rate schedule $\alpha_t$
- For $t=1, \ldots, T$
  - $\param^{t+1} = \param^t - \alpha_t \nabla_\param f(\param^t)$
- Return $\param^T$

**** Properties
- If $\sum_t \alpha_t = \infty$ and $\sum_t \alpha_t^2 < \infty$, it finds a local minimum $\param^T$, i.e. there is $\epsilon > 0$ so that
\[
f(\param^T) < f(\param), \forall \param: \|\param^T - \param\| < \epsilon.
\]
*** Stochastic gradient method
This is the same as the gradient method, but with added noise:
- $\param^{t+1} = \param^t - \alpha_t [\nabla_\param f(\param^t) + \omega_t]$
- $\E[\omega_t] = 0$ is sufficient for convergence.
#+BEAMER: \pause
**** When the cost is an expectation                     :B_example:
	 :PROPERTIES:
	 :BEAMER_env: example
	 :END:
In machine learning, the cost is frequently an expectation of some function $\ell$, 
\[
f(\param) = \int_X dP(x) \ell(x, \param)
\]
This can be approximated with a sample
\[
f(\param) \approx \frac{1}{T} \sum_t \ell(x_t, \param)
\]
The same holds for the gradient:
\[
\nabla_\param f(\param) = \int_X dP(x) \nabla_\param \ell(x, \param)
\approx \frac{1}{T} \sum_t \nabla_\param \ell(x_t, \param)
\]

** The perceptron as a gradient algorithm
*** Perceptron algorithm as gradient descent
**** Target error function
\[
\E_{\alert{P}}^\param[\ell] = \int_{\CX} d\alert{P}(x) \sum_y \alert{P}(y|x) \ell(x, y, \param)
\]
Minimises the error on the true distribution.
#+BEAMER: \pause
**** Empirical error function
\[
\E_{\alert{D}}^\param[\ell]= \frac{1}{T} \sum_{t=1}^T \ell(x_t, y_t, \param),
\qquad\alert{D} = (x_t, y_t)_{t=1}^T, \quad x_t, y_t \sim P.
\]
Minimises the error on the empirical distribution.
*** Cost functions and the chain rule
**** Perceptron cost function
The cost of each example
\begin{align}
\ell(x,y, \param) 
&= \overbrace{\ind{y(x^\top \param) < 0}}^{\textrm{misclassified?}} \overbrace{[ - y (x^\top \param)]}^{\textrm{margin of error}}
\end{align}
where the *indicator function $\ind{A}$* is  1 when $A$ is true and $0$ otherwise.
\begin{center}
\begin{tikzpicture}[domain=-2:2, samples=200,range=-1:2]
   \draw[dotted, color=gray] (-2.1,-2.1) grid (3.1,3.1);
   \draw[->] (0,0) -- (2,0) node[right] {$f(x)$};
   \draw[->] (0,0) -- (0,4) node[above] {$\ell$};
   \draw[thick, color=red] plot (\x, {max(0, \x)}) node [right] {perceptron cost};
   \draw[dashed, thick, color=blue] plot (\x, {\x >= 0)}) node [right] {classification cost};
\end{tikzpicture}
\end{center}

Here we see a plot of $\ell(\mu, \beta) = (\beta - \mu)^2$.
*** Derivative of the perceptron cost function
The total cost over the data is defined as
\[
L(D, \param) = \sum_{(x, y) \in D} \ell(x, y, \param)
\]
Taking the derivative, we have
\[
\nabla_\param L(D, \param) = \nabla_\param \sum_{(x, y) \in D} \ell(x, y, \beta)
  = \sum_{(x, y) \in D} \nabla_\param \ell(x, y, \param)
\]


**** Reminder: The chain rule
Let $z = g(y)$, $y = f(x)$ so that $z= g(f(x))$. Then $\frac{dz}{dx} = \frac{dz}{d\alert{y}}\frac{d\alert{y}}{dx}$

#+BEAMER: \pause
**** Applying the chain rule to calculate the gradient
#+ATTR_BEAMER: :overlay <+->
- $\nabla_\param \ell(x,y, \param) = - \ind{y(x^\top \param) < 0} \nabla_\param [y(x^\top \param)]$.
- $\frac{\partial \param}{\partial{\param_i}} [y(x_t^\top \param)] = y x_{t,i}$ (gradient of Perceptron's output)
- Gradient update: $\param^{t+1} = \param^t - \nabla_\param \ell(x,y, \param) = \param^t + y x_{t}$
#+BEAMER: \pause
The classification error cost function is *not* differentiable :(
*** Margins and confidences
#+ATTR_BEAMER: :overlay <+->
We can think of the output of the network as a measure of confidence
#+attr_html: :width 100px
#+attr_latex: :width 100px
[[./fig/margin.pdf]]
#+BEAMER: \pause
By applying the *logit* function, we can bound a real number $x$ to $[0,1]$:
\[
f(x) = \frac{e^x}{1 + e^x} = \frac{1}{1 + e^{-x}}
\]
*** Logistic regression
**** Output as a measure of confidence, given the parameter $\param$
\[
P_\param(y = 1| x) = \frac{1}{1 + \exp(- x_t^\top \param)}
\]
The original output $x_t^\top \param$ is now passed through the logit function.
#+BEAMER: \pause
**** Negative Log likelihood
#+ATTR_BEAMER: :overlay <+->
$\ell(x_t, y_t, \param) = - \ln P_\param( y_t | x_t) = \ln(1 + \exp(- y_t x_t^\top \param))$
\begin{align*}
\nabla_\param \ell(x_t, y_t, \param) 
&= \frac{1}{1 + \exp(- y x_t^\top \param)} \nabla_\param[1 + \exp(-y x_t^\top \param)]
\\
&= \frac{1}{1 + \exp(- y x_t^\top \param)} \exp(-y x_t^\top \param) [\nabla_\param (-y_t x_t^\top \param)]
\\
&= - \frac{1}{1 + \exp(x_t^\top \param)} (x_{t,i})_{i=1}^ne
\end{align*}
- $\E_P(\ell) = \int_X dP(x) \sum_{y \in Y} P(y|x) P_\param(y_t + x_t)$
