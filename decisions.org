#+TITLE: Decisions and randomness
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \input{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSubsection[]{\begin{frame}<beamer>\tableofcontents[currentsubsection]\end{frame}}

* Statistical Decision Theory
** Elementary Decision Theory
*** Preferences
**** Types of rewards                                               :example:
- For e.g. a student: Tickets to concerts.
- For e.g. an investor: A basket of stocks, bonds and currency.
- For everybody: Money.

**** Preferences among rewards
For any rewards $x, y \in R$, we either
- (a) Prefer $x$ at least as much as $y$ and write $x \preceq^* y$.
- (b) Prefer $x$ not more than $y$ and write $x \succeq^* y$.
- (c) Prefer $x$ about the same as $y$ and write $x \eqsim^* y$.
- (d) Similarly define $\succ^*$ and $\prec^*$
  
*** Utility and Cost
#+BEAMER: \pause
**** Utility function
To make it easy, assign a utility $U(x)$ to every reward through a
utility function $U : R \to \Reals$.

**** Utility-derived preferences
We prefer items with higher utility, i.e.
- (a) $U(x) \geq U(y)$ $\Leftrightarrow$ $x \succeq^* y$
- (b) $U(x) \leq U(y)$ $\Leftrightarrow$ $y \succeq^* x$
#+BEAMER: \pause
**** Cost
     It is sometimes more convenient to define a cost function $C: R \to \Reals$ so that we prefer items with lower cost, i.e.
- $C(x) \geq C(y)$ $\Leftrightarrow$ $y \succeq^* x$

*** Random outcomes
#+ATTR_LATEX: width=\textwidth
[[./figures/roulette.jpg]]


**** Choosing among rewards: Roulette
- [A] Bet 10 CHF on black
- [B] Bet 10 CHF on 0
- [C] Bet nothing

- What is the reward here?
- What is the outcome?
*** Uncertain outcomes
- [A] Taking the car to Zurich (50'-80' with delays)
- [B] Taking the train to Zurich (60' without delays)
What is the reward here? 

**** Car                                                              :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
#+ATTR_LATEX: width=0.25\textwidth
[[./figures/car.jpg]]
**** Train                                                            :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.5
     :END:
#+ATTR_LATEX: width=0.25\textwidth
[[./figures/train.jpeg]]




*** Independent outcomes

**** Graphical model
\begin{center}
      \begin{tikzpicture}
        \node[select] at (0,0) (a) {$a$};
	\node[RV] at (4,0) (w) {$\omega$};
        \node[utility] at (2,0) (U) {$U$};
	\draw[->] (a) -- (U);
	\draw[->] (w) -- (U);
      \end{tikzpicture}
\end{center}

**** Random rewards
- We *select* our action.
- Outcomes are *random*, with $\omega \sim P$, but *independent* of our action
- We then obtain a random *utility* with distribution depending on $a$.
\[
\Pr(U = u \mid a) = P(\{\omega : U(a, \omega) = u\})
\]


*** General case
**** Graphical model
\begin{center}
      \begin{tikzpicture}
        \node[select] at (0,0) (a) {$a$};
	\node[RV] at (2,0) (w) {$\omega$};
        \node[utility] at (4,0) (U) {$U$};
	\draw[->] (a) -- (w);
	\draw[->] (w) -- (U);
	\draw[->, dashed] (a) to [bend right=45] (U);
      \end{tikzpicture}
\end{center}
**** Random rewards
- We *select* our action.
- The action determines the *outcome* distribution.
- The utility may depend on *both* the outcome and reward.
*** Route selection
**** Utility                                                      :B_example:
     :PROPERTIES:
     :BEAMER_env: example
     :END:
|----------------+-----+-----+-----+-----+-----+-----+-----|
| /              |   < |     |     |     |     |     |     |
| $U(a, \omega)$ | 30' | 40' | 50' | 60' | 70' | 80' | 90' |
|----------------+-----+-----+-----+-----+-----+-----+-----|
| Train          |  -1 |  -2 |  -5 | -10 | -15 | -20 | -30 |
| Car            | -10 | -20 | -30 | -40 | -50 | -60 | -70 |
|----------------+-----+-----+-----+-----+-----+-----+-----|

**** Probability                                                  :B_example:
     :PROPERTIES:
     :BEAMER_env: example
     :END:
|--------------------+-----+-----+-----+-----+-----+-----+-----|
| /                  |   < |     |     |     |     |     |     |
| $P(\omega \mid a)$ | 30' | 40' | 50' | 60' | 70' | 80' | 90' |
|--------------------+-----+-----+-----+-----+-----+-----+-----|
| Train              |  0% |  0% | 50% | 45% |  4% |  1% |  0% |
| Car                |   0 | 40% | 30% | 15% | 10% |  3% |  2% |
|--------------------+-----+-----+-----+-----+-----+-----+-----|




** Statistical Decision Theory

*** Expected utility
**** Actions, outcomes and utility
In this setting, we obtain random outcomes that depend on our actions.
- Actions $a \in A$
- Outcomes $\omega \in \Omega$.
- Probability of outcomes $P(\omega \mid a)$
- Utility $U : \Omega \alert{\times A} \to \Reals$
**** Expected utility
The expected utility of an action is:
\[
\E_P[U \mid a] = \sum_{\omega \in \Omega} U(\omega \alert{, a}) P(\omega \alert{\mid a}).
\]

**** The expected utility hypothesis
We prefer $a$ to $a'$ if and only if
\[
\E_P[U \mid a] \geq \E_P[U \mid a']
\]

*** Example: Betting
 In this example, probabilities reflect actual randomness

|------------+---------------------+------------+---------------|
| Choice     | Win Probability $p$ | Payout $w$ | Expected gain |
|------------+---------------------+------------+---------------|
| Don't play | 0                   |          0 |             0 |
| Black      | 18/37               |          2 |               |
| Red        | 18/37               |          2 |               |
| 0          | 1/37                |         36 |               |
| 1          | 1/37                |         36 |               |
|------------+---------------------+------------+--------------- |

#+ATTR_LATEX: width=\textwidth
[[./figures/roulette.jpg]]
What are the expected gains for these bets?
*** The St-Petersburg Paradox
**** The game
If you give me $x$ CHF, then I promise to:
- (a) Throw a fair coin until it comes heads.
- (b) If it does so after $T$ throws, then I will give you $2^T$ CHF.
**** The question
- How much $x$ are you willing to pay to play?
- Given that the expected amount of money is infinite, why are you only willing to pay a small $x$?

*** Example: Route selection
- In this example, probabilities reflect subjective beliefs

|--------------+-----------+-----------------+--------------+---------------|
| Choice       | Best time | Chance of delay | Delay amount | Expected time |
|--------------+-----------+-----------------+--------------+---------------|
| Train        |        80 | 5%              |            5 |               |
| Car, route A |        60 | 50%             |           30 |               |
| Car, route B |        70 | 10%             |           10 |               |
|--------------+-----------+-----------------+--------------+---------------|


*** Example: Noisy optimisation
**** Simple maximisation
For a function $f : \Reals \to \Reals$, find a maximum $x^*$ i.e. $f(x^*) \geq f(x) \forall x$.

#+BEAMER: \pause
**** Necessary conditions                                         :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
If $f: \Reals \to \Reals$ is a continuous function, a maximum point $x^*$ satisfies:
\[
\frac{d}{dx} f(x^*) = 0,
\qquad
\frac{d}{dx^2} f(x^*) < 0.
\]

#+BEAMER: \pause
**** Noisy optimisation
- We select $x$ but *do not* observe $f(x)$.
- We observe a *random* $g$ with $\E[g | x] = f(x)$.
\begin{align}
f(x) &\defn \E[g | x],
&
\E[g | x] = \int_{- \infty}^\infty g(\omega, x) p(\omega) d\omega
\end{align}





*** Mean-squared error cost function
\begin{tikzpicture}[domain=-1:2, range=-1:2]
   \draw[dotted, color=gray] (-1.1,-2.1) grid (3.1,4.1);
   \draw[->] (0,0) -- (2,0) node[right] {$x$};
   \draw[->] (0,0) -- (0,4) node[above] {$g(\omega, x)$};
   \draw[color=red] plot (\x, {(\x-1)^2})  node[right] {$\omega = 1$};
   \draw[color=blue] plot (\x, {(\x)^2})  node[right] {$\omega = 0$};
\end{tikzpicture}
This example is for a quadratic loss: $g(\omega, x) = (\omega - x)^2$.

*** Example: Estimation
- $\param$: *parameter* (random)
- $\hat{\param}$: *estimate* (our action)
- $(\param - \hat{\param})^2$: *cost* function
#+BEAMER: \pause
**** Mean-squared error minimiser
If we want to guess $\hat{\param}$, and we knew that $\param \sim P$, then the guess
\[
\hat{\param} = \E_P(\param) = \argmin_{\hat{\param}} \E_P [(\param - \hat{\param})^2]
\]
minimises the squared error. 
#+BEAMER: \pause
This is because
\begin{align}
\frac{d}{d \hat{\theta}}
 \E_P [(\param - \hat{\param})^2]
&=
\frac{d}{d \hat{\theta}}
 \sum_\omega [\theta(\omega) -  \hat{\param}]^2 P(\omega)\\
&=
 \sum_\omega \frac{d}{d \hat{\theta}}
 [\theta(\omega) -  \hat{\param}]^2 P(\omega)\\
&=
 \sum_\omega 2 [\theta(\omega) -  \hat{\param}] (-1) P(\omega)
&=
	 2 (\hat{\param} - \E_P [\theta]).
\end{align}
Setting this to $0$ gives $\hat{\param} =\E_P [\theta]$

* Gradient methods
** Gradients for optimisation
*** The gradient descent method: one dimension
- Function to minimise $f : \Reals \to \Reals$.
- Derivative $\frac{d}{d \param} f(\param)$
#+BEAMER: \pause
**** Gradient descent algorithm
- Input: initial value $\param^0$, *learning rate* schedule $\alpha_t$
- For $t=1, \ldots, T$
  - $\param^{t+1} = \param^t - \alpha_t \frac{d}{d \param} f(\param^t)$
- Return $\param^T$
#+BEAMER: \pause
**** Properties
- If $\sum_t \alpha_t = \infty$ and $\sum_t \alpha_t^2 < \infty$, it finds a local minimum $\param^T$, i.e. there is $\epsilon > 0$ so that
\[
f(\param^T) < f(\param), \forall \param: \|\param^T - \param\| < \epsilon.
\]
*** One-dimensional minimisation example

*** Gradient methods for expected value :example:
**** Estimate the expected value
$x_t \sim P$ with $\E_P[x_t] = \mu$.
#+BEAMER: \pause
**** Objective: mean squared error
Here $\ell(x, \param) = (x - \param)^2$.
\[
\min_\param \E_P[(x_t - \param)^2].
\]
#+BEAMER: \pause
**** Exact derivative update
If we know $P$, then we can calculate
\begin{align}
\param^{t+1} &= \param^t - \alpha_t \frac{d}{d\param} \E_P[(x - \param^t)^2]\\
\frac{d}{d\param} \E_P[(x - \param^t)^2] &= 2 (\E_P[x] - \param^t)
\end{align}

*** Stochastic derivative
- Function to minimise $f : \Reals \to \Reals$.
- Derivative $\frac{d}{d \param} f(\param)$
- $f(\param) = \E[g | \param]$
- $\frac{d}{d \param} f = \E[ \frac{d}{d \param} g | \param]$
#+BEAMER: \pause
**** Stochastic derivative algorithm
- Input: initial value $\param^0$, *learning rate* schedule $\alpha_t$
- For $t=1, \ldots, T$
  - Observe $g(\omega_t, \param^t$), where $\omega_t \sim P$.
  - $\param^{t+1} = \param^t - \alpha_t \frac{d}{d \param} g(\omega_t, \param^t)$
- Return $\param^T$


*** Stochastic gradient for mean estimation
**** Sampling :B_theorem:
     :PROPERTIES:
     :BEAMER_env: theorem
     :END:
For any bounded random variable $f$, 
\[
\E_P[f] = \int_{X} dP(x) f(x)
 = 
\lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T f(x_t)
 = 
\E_P \left[\frac{1}{T} \sum_{t=1}^T f(x_t)\right]
, \qquad x_t \sim P
\]
**** Derivative ampling                                           :B_example:
     :PROPERTIES:
     :BEAMER_env: example
     :END:
We can also approximate the gradient through sampling:
\begin{align*}
 \frac{d}{d\param} \E_P [(x - \param)^2] 
&= \int_{-\infty}^\infty \!\!\!\! dP(x) \frac{d}{d\param} (x - \param)^2
\\
&\approx \frac{1}{T} \sum_{t=1}^T \frac{d}{d\param} (x_t - \param)^2
= \frac{1}{T} \sum_{t=1}^T 2(x_t - \param)
\end{align*}
#+BEAMER: \pause
- Wen can even update $\param$ after *each sample* $x_t$:
\[
\param^{t+1} = \param^t + 2 \alpha_t (x_t - \param^t)
\]
*** The gradient method
- Function to minimise $f : \Reals^{\alert{n}} \to \Reals$.
- *Gradient* $\nabla_\param f(\param)  = \left(\frac{\partial f(\param)}{\partial \param_1}, \ldots, \frac{\partial f(\param)}{\partial \param_n}\right)$,
- *Partial* derivative $\frac{\partial f}{\partial \param_n}$
#+BEAMER: \pause
**** Gradient descent algorithm
- Input: initial value $\param^0$, learning rate schedule $\alpha_t$
- For $t=1, \ldots, T$
  - $\param^{t+1} = \param^t - \alpha_t \nabla_\param f(\param^t)$
- Return $\param^T$
#+BEAMER: \pause
**** Properties
- If $\sum_t \alpha_t = \infty$ and $\sum_t \alpha_t^2 < \infty$, it finds a local minimum $\param^T$, i.e. there is $\epsilon > 0$ so that
\[
f(\param^T) < f(\param), \forall \param: \|\param^T - \param\| < \epsilon.
\]

*** When the cost is an expectation                               :B_example:
	 :PROPERTIES:
	 :BEAMER_env: example
	 :END:
In machine learning, we sometimes want to minimise the *expectation* of a *cost* $\ell$, 
\[
f(\param) \defn \E[\ell | \param] = \int_\Omega dP(\omega) \ell(\omega, \param)
\]
This can be appro\omegaimated with a sample
\[
f(\param) \approx \frac{1}{T} \sum_t \ell(\omega_t, \param)
\]
The same holds for the gradient:
\[
\nabla_\param f(\param) = \int_\OMEGA dP(\omega) \nabla_\param \ell(\omega, \param)
\approx \frac{1}{T} \sum_t \nabla_\param \ell(\omega_t, \param)
\]




*** Stochastic gradient method
- Function to *minimise* $f : \Reals^n \to \Reals$.
- *Gradient* $\nabla f(\param)$
- $f(\param) = \E[\ell | \param]$
- $\nabla_\param f = \E[ \nabla_\param \ell | \param]$

#+BEAMER: \pause
**** Algorithm
- Input: initial value $\param^0$, *learning rate* schedule $\alpha_t$
- For $t=1, \ldots, T$
  - Observe $\ell(\omega_t, \param^t$), where $\omega_t \sim P$.
  - $\param^{t+1} = \param^t - \alpha_t \nabla_\param g(\omega_t, \param^t)$
- Return $\param^T$

**** Alternative view: Noisy gradients
- $\param^{t+1} = \param^t - \alpha_t [\nabla_\param f(\param^t) + \epsilon_t]$
- $\E[\epsilon_t] = 0$ is sufficient for convergence.
#+BEAMER: \pause

