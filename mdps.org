#+TITLE: Markov Decision Processes
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+startup: beamer
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [smaller,ignorenonframetext,presentation]
#+LaTeX_HEADER: \input{preamble}
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSubsection[]{\begin{frame}<beamer>\tableofcontents[currentsubsection]\end{frame}}

* Markov processes
#+TOC: headlines [currentsection]
** The Markov process
Markov processes, or Markov chains, are very simple models. However,
they form the basis of many modern, state-of-the art models algorithms
in machine learning and computer science more generally. Their most
noteworthy applications include compression, control, financial
forecasting and large language models.

*** Snakes and Ladders
#+ATTR_HTML: :width 300px
#+ATTR_LATEX: :height 0.9\textheight
[[./figures/Snakes_and_Ladders.jpeg]]
*** Markov Process
A sequence of random variables 
$s_1, s_2, \ldots, s_t$ is a *Markov process* if the next variable $s_{t+1}$ only depends on the current value $s_t$
\[
P(s_{t+1} \mid s_{t}, \ldots, s_1) = P(s_{t+1} \mid s_{t}) \qquad \forall t
\]
#+BEAMER: \pause
- The variable $s_t \in S$ is the current *state* of the process.
- For finite $S$, the matrix $p_{i,j} \defn P(s_t = j \mid s_{t-1} = i)$ is called the *transition matrix*.
#+BEAMER: \pause
**** Bayesian network

\begin{center}
\begin{tikzpicture}
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \draw[->] (stp) -- (st);
  \draw[->] (st) -- (stn);
\end{tikzpicture}
\end{center}


#+BEAMER: \pause
**** Snakes and ladders :B_example:
     :PROPERTIES:
     :BEAMER_env: example
     :END:
- What is the state?
- What is the transition matrix?

*** Snakes and ladders workout
**** State variable
- $s_t = (m_t, x_{0,t}, x_{1,t})$
- $m_t$: whose turn it is
- $x_{0,t}$: location of player 0
- $x_{1,t}$: location of player 1
**** Transition matrix (with no snakes or ladders)
- $x_{1 - m_t, t+1} = x_{1 - m_t, t}$ w.p. 1
- $m_{t+1} = 1 - m_t$ w.p. 1.
- $P(x_{m_t, t+1} = x_{m_t, t} + k) = 1/6$ for all $k < 100 - x_{m_t, t}$ with $k \leq 6$
- $P(x_{m_t, t+1} = 100) = [(100 -x_{m_t, t+1})/6]_+$ 
**** Taking snakes/ladders into account
- $f: \{1, 100\} \to \{1, 100\}$ tells us where snakes/ladders take us.
- $P(x_{m_t, t+1} = \alert{f}(x_{m_t, t} + k)) = 1/6$ for all $k < 100 - x_{m_t, t}$. 
*** Simple betting game
- You have $s_t$ CHF.
- With probability $p$ you gain 1 CHF, and otherwise you lose 1 CHF.
#+BEAMER: \pause
**** State space
- $s_t \in \Naturals$ (i.e. we cannot owe money)
#+BEAMER: \pause
**** Transition matrix :no_beamer:
For all $k > 1$.
- $P(s_{t+1} = k+1 \mid s_t = k) = p$
- $P(s_{t+1} = k-1 \mid s_t = k) = 1 - p$
Otherwise:
- $P(s_{t+1} = 0 \mid s_t = 0) = 1$ (i.e. the game ends)


#+BEAMER: \pause
**** Martingale property (*)
If $p = 1/2$ then the process is a *martingale*, i.e.
\[
\E[s_{t+1} \mid s_t] = s_t
\]
This is because $\E[s_{t+1} \mid s_t = k] = 
(k + 1) p + (k -1)(1 - p)  = k$.

*** Python example:
#+BEGIN_SRC python
class BettingGame:
  def __init__(p, s):
    self.p = p
    self.s = s
  def generate_state():
    if (np.random.uniform() < p and self.s >= 1):
      self.s += 1
    else:
      self.s -= 1
    if self.s < 0:
      self.s = 0
    return self.s
#+END_SRC


*** Language models

**** Text prediction
Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna 
#+BEAMER: \pause 
\alert{aliqua}

#+BEAMER: \pause 
**** First-order Markov model
Lorem \to ipsum\\
ipsum \to dolor\\
\cdots\\
magna \to aliqua

#+BEAMER: \pause
**** Second-order Markov model
Lorem ipsum \to dolor\\
ipsum dolor \to sit\\
dolor sit \to amet


*** k-order Markov models

**** First-order
\begin{center}
\begin{tikzpicture}
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \draw[->] (stp) -- (st);
  \draw[->] (st) -- (stn);
\end{tikzpicture}
\end{center}

**** Second-order
\begin{center}
\begin{tikzpicture}
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \draw[->] (stp) -- (st);
  \draw[->] (stp) to [bend right=45] (stn);
  \draw[->] (st) -- (stn);
\end{tikzpicture}
\end{center}

**** Third-order
\begin{center}
\begin{tikzpicture}
  \node[RV] at (-2,0) (stp2) {$s_{t-2}$};
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \draw[->] (stp2) to [bend right = 45] (st);
  \draw[->] (stp2) -- (stp);
  \draw[->] (stp) -- (st);
  \draw[->] (stp2) to [bend right = 45] (stn);
  \draw[->] (stp) to [bend right=45] (stn);
  \draw[->] (st) -- (stn);
\end{tikzpicture}
\end{center}



* Markov decision processes
#+TOC: headlines [currentsection]
** Markov decision process
*** Simple betting game (I)
- You have $s_t$ CHF.
- At time $t$, you take an action $a_t$, whether to *stop* or *play*
- If you *play*, with probability $p$ you gain 1 CHF, and otherwise you lose 1 CHF.
- If you *stop*, the game ends.
*** Markov Decision Process
**** Variables                                                        :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.4
     :END:
- The *state* $s_t \in S$.
- The *action* $a_t \in A$.
- The *reward* $r_t \in \Reals$.
#+BEAMER: \pause
**** Bayesian network                                                 :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.6
     :END:
\begin{tikzpicture}
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \node[select] at (0,2) (atp) {$a_{t-1}$};
  \node[select] at (2,2) (at) {$a_t$};
  \node[utility] at (2,1) (rtp) {$r_{t}$};
  \node[utility] at (4,1) (rt) {$r_{t+1}$};
  \draw[->] (atp) -- (st);
  \draw[->] (atp) -- (rtp);
  \draw[->] (stp) -- (rtp);
  \draw[->] (at) -- (rt);
  \draw[->] (st) -- (rt);
  \draw[->] (at) -- (stn);
  \draw[->] (stp) -- (st);
  \draw[->] (st) -- (stn);
\end{tikzpicture}


#+BEAMER: \pause
**** Markov Decision Process :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
A Markov decision process $\mdp$ on $(S, A)$ has the property that for any sequence of actions $a_1, \ldots$
\begin{align*}
P_\mdp(s_{t+1} \mid s_t, a_t, r_{t}, s_{t-1}, a_{t-1}, \ldots) = P_\mdp(s_{t+1} \mid s_t, a_t)
\tag{transition}
\\
P_\mdp(r_{t+1} \mid s_t, a_t, r_{t}, s_{t-1}, a_{t-1}, \ldots) = P_\mdp(r_{t+1} \mid s_t, a_t)
\tag{reward}
\end{align*}
#+BEAMER: \pause
The goal in a *finite-horizon* MDP is to maximise the \(T\)-horizon utility:
\[
U_t = \sum_{k=t}^T r_k
\]

*** Simple betting game (II)
**** State space
- $s_t \in \Naturals$ (i.e. we cannot owe money)
#+BEAMER: \pause
**** Game end conditions
- $t = T$ (we reach the horizon)
- $s_t = 0$ (we lose all our money)
- $a_t  = \textrm{stop}$ (we stop)
#+BEAMER: \pause
**** Transition matrix
- Win: $P(s_{t+1} = k+1 \mid s_t = k, a_t = \textrm{play}) = p$, $k > 0$
- Lose: $P(s_{t+1} = k-1 \mid s_t = k, a_t = \textrm{play}) = 1 - p$, $k > 0$.
- Stop: $P(s_{t+1} = 0 \mid s_t = k, a_t = \textrm{stop}) = 1$
- Absorbing/terminal state: $P(s_{t+1} = 0 \mid s_t = 0, a_t = a) = 1$
#+BEAMER: \pause
**** Rewards
- $r_t = 0$ when you *play*
- $r_t = s_t$ when you *stop*
*** Simple betting game (II)
**** Always play
\[
\pol(a_t = \textrm{play} | s_t ) = 1
\]
**** Never play
\[
\pol(a_t = \textrm{play} | s_t ) = 0
\]
**** Play until you have more than 100 CHF
\[
\pol(a_t = \textrm{play} | s_t < 100 ) = 1,
\qquad
\pol(a_t = \textrm{stop} | s_t \geq 100 ) = 1.
\]

**** Play until you double your money
\[
\pol(a_t = \textrm{play} | s_t < 2 s_1 ) = 1,
\qquad
\pol(a_t = \textrm{stop} | s_t \geq 2 s_1 ) = 1.
\]

*** Policies in Markov decision processes

**** Variables                                                        :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.4
     :END:
- The *policy* $\pol$
- The *state* $s_t \in S$.
- The *action* $a_t \in A$.
- The *reward* $r_t \in \Reals$.
#+BEAMER: \pause
**** Bayesian network                                                 :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.6
     :END:
\begin{tikzpicture}
  \node[select] at (2,3) (pol) {$\pol$};
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \node[RV] at (0,2) (atp) {$a_{t-1}$};
  \node[RV] at (2,2) (at) {$a_t$};
  \node[utility] at (2,1) (rtp) {$r_{t}$};
  \node[utility] at (4,1) (rt) {$r_{t+1}$};
  \draw[->] (pol) -- (atp);
  \draw[->] (pol) -- (at);
  \draw[->] (atp) -- (st);
  \draw[->] (st) to [bend right = 45] (at);
  \draw[->] (stp) to [bend right = 45] (atp);
  \draw[->] (atp) -- (rtp);
  \draw[->] (stp) -- (rtp);
  \draw[->] (at) -- (rt);
  \draw[->] (st) -- (rt);
  \draw[->] (at) -- (stn);
  \draw[->] (stp) -- (st);
  \draw[->] (st) -- (stn);
\end{tikzpicture}

**** Markov Policy                                             :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
A Markov policy takes an action $a$ at time $t$ with probability 
\[
\pol(a_t = a \mid s_t = s)
\]
**** The expected utility of a policy
\[
\E^\pol_\mdp[U_t] = \sum_{k=t}^T \E^\pol_\mdp[r_k]
\]
 
** Utility and value functions

*** Value function
- The utility from step $t$ is $U_t \defn \sum_{k=t}^T r_k$
#+BEAMER: \pause
**** The state value function
This is the expected utility obtained by following a *policy $\pol$* starting from some *state $s$*.
\[
V^\pol_t(s) \defn \E_\pol(U_t \mid s_t = s)
\]
#+BEAMER: \pause
**** The state-action value function
This is the expected utility obtained by following a policy $\pol$ starting from some state $s$ and *playing action $a$*
\[
Q^\pol_t(s, a) \defn \E_\pol(U_t \mid s_t = s, a_t = a)
\]
#+BEAMER: \pause
**** The optimal value function
There is some policy $\pol^*$ satisfying
\begin{align*}
V^*(s) &\defn V^{\pol^*}(s) \geq V^\pol(s) \qquad \forall \pol, s
\\
Q^*(s, a) &\defn Q^{\pol^*}(s, a) \geq Q^\pol(s, a) \qquad \forall \pol, s, a
\end{align*}


*** The expected utility recursion
Value functions satisfy the following recursion
\begin{align*}
V^\pol_t(s_t) 
\uncover<2->{
&= \E_\pol(U_t \mid s_t)\\
}
\uncover<3->{
&= \E_\pol\left[\sum_{k=t}^T r_k \middle| s_t\right]\\
}
\uncover<4->{
&= \E_\pol[r_t \mid s_t] + \E_\pol\left[\sum_{k=t+1}^T r_k \middle| s_t  \right]\\
}
\uncover<5->{
&= \E_\pol[r_t \mid s_t] + \E_\pol\left[U_{t+1}| s_t \right]\\
}
\uncover<6->{
&= \E_\pol[r_t \mid s_t] + \sum_{s_{t+1} \in S} \Pr_\pol(s_{t+1} \mid s_t) \E_\pol\left[U_{t+1}| s_{t+1} \right]\\
}
\uncover<7->{
&= \E_\pol[r_t \mid s_t] +  \sum_a \pol(a \mid s_t) \sum_{s_{t+1} \in S} P_\mdp(s_{t+1} \mid s_t, a) V^\pol_{t+1}(s_{t+1}).
}
\end{align*}
#+BEAMER: \pause
#+BEAMER: \pause
#+BEAMER: \pause
#+BEAMER: \pause
#+BEAMER: \pause
#+BEAMER: \pause
#+BEAMER: \pause
**** Exercise
Prove that 
\[
Q^\pol_t(s, a) = r(s, a) + \sum_{s' \in S} P_\mdp(s' \mid s, a) \sum_{a' \in A} Q^\pol_{t+1}(s', a') \pol(a_{t+1} = a' \mid s_{t+1} = s')
\]

*** Backwards induction
**** State value function 
The value function of the *optimal* policy can be found by setting $V^*_T(s) = \max_a r(s,a)$ for all $s$ and recursing:
#+BEAMER: \pause
\[
V^*_t(s) 
=
\max_a r(s,a)  + \sum_{s' \in S} P_\mdp(s' \mid s, a) V^*_{t+1}(s'),
\]
#+BEAMER: \pause
Optimal action at $s, t$: $\argmax_a r(s,a)  + \sum_{s' \in S} P_\mdp(s' \mid s, a) V^*_{t+1}(s')$.
#+BEAMER: \pause
**** On the state-action value function
Similarly, set $Q^*_T(s,a) = r(s,a)$ and  recurse:
#+BEAMER: \pause
\[
Q^*_t(s, a) 
=
 r(s,a)  + \sum_{s' \in S} P_\mdp(s' \mid s, a) \max_{a'} Q^*_{t+1} (s', a').
\]
#+BEAMER: \pause
Optimal action at step $t$: $\argmax_a Q^*_t(s,a)$.

* Examples
#+TOC: headlines [currentsection]

** Toy examples
*** Chain
\begin{center}
\begin{tikzpicture}
  \node[RV] at (0,0) (1) {$\epsilon$};
  \node[RV] at (1,0) (2) {$0$};
  \node[RV] at (2,0) (3) {$0$};
  \node[RV] at (3,0) (4) {$0$};
  \node[RV] at (4,0) (5) {$1$};
  \draw[blue, ->] (1) -- (2);
  \draw[blue, ->] (2) -- (3);	 
  \draw[blue, ->] (3) -- (4);
  \draw[blue, ->] (4) -- (5);
  \draw[red,->] (2) to [bend right=45] (1);	
  \draw[red,->] (3) to [bend right=45] (1);	
  \draw[red,->] (4) to [bend right=45] (1);	
  \draw[red,->] (5) to [bend right=45] (1);	
  \draw[red,->] (1) to [loop above] (1);	
  \draw[blue,->] (5) to [loop above] (5);	
\end{tikzpicture}
\end{center}

**** Reward
At state $s = 1$ is $\epsilon < 1$ and the reward at state $s = 5$ is $1$.

**** Transition probabilities for $a = 1$
\[
P(s_{t+1} = \min\{5, i + 1\} \mid s_t = i, a = 1) = 1 - \delta,
\]
and
\[
 P(s_{t+1} = 1 \mid s_t = i, a = 1) =  \delta.
\]
**** Transition probabilities for $a = 2$:
\[
P(s_{t+1} = \min\{5, i + 1\} \mid s_t = i, a = 2) = \delta,
\]
and
\[
\qquad P(s_{t+1} = 1 \mid s_t = i, a = 2) = 1-  \delta.
\]


*** Wumpus world
- State: $s_t = (x_t, y_t, d_t, w_t)$, the x-y location of the agent, the direction, and the amount of arrows left.
- Actions: $a \in \{L, R, M, S\}$ for left, right, move and shoot.
- Rewards are given for killing the Wumpus, dying, or finding the treasure.
**** Deterministic/Stochastic Wumpus
- An action/observation is always the same/is random
**** Observable/Unobservable Wumpus
- We know where the holes, the treasure and the Wumpus is/they are unknown
**** Static/Dynamic/Strategic/ Wumpus
- The Wumpus is stationary/moves according to a fixed policy/has goals to achieve
*** Deterministic, Observable Wumpus

This is the simplest setting. It is a deterministic planning problem.
For this, you can
1. Define a way to describe the Wumpus world
2. Find a policy for solving the Wumpus world as given. This policy is going to be deterministic and Markov. 

Of course, the optimal policy for each *instance* of the Wumpus problem is going to be different.

I recommend summarising the Wumpus problem in two parts:
(a) A matrix $G$ where
$G[x,y]$ is a number indicating what is contained in this location,
(b) $x_t, y_t, d_t, w_t$ being the agent-relevant variables.

You can either use any logical planning algorithm, or an MDP algorithm with deterministic transitions for this problem.

*** Stochastic, Observable Wumpus

To make the environment stochastic, we can add the following extensions

(a) The Wumpus moves according to some stochastic policy. For example, the Wumpus could randomly move in a direction, so that on average it moves away from us.
(b) Our actions do not always work (e.g. we may turn in the wrong direction)
(c) We do not always die when we encounter a hole or the Wumpus.

For this, you can
1. Define a way to describe the Wumpus world
2. Find a policy for solving the Wumpus world as given. This policy is going to be deterministic and Markov. 

Of course, the optimal policy for each *instance* of the Wumpus problem is going to be different.

I recommend summarising the Wumpus problem in two parts:
(a) A matrix $G$ where
$G[x,y]$ is a number indicating what is contained in this location,
(b) $x_t, y_t, d_t, w_t$ being the agent-relevant variables.

You can either use any logical planning algorithm, or an MDP algorithm with deterministic transitions for this problem.


*** Deterministic, Unobservable Wumpus

This setting is significantly harder to work with. Now we have observations whenever we are near a hole or the Wumpus.

You can either:
(a) Use a logical description of the world, and a SAT algorithm.
(b) Use a probabilistic description with all probabilities being 0 or 1, and an MDP algorithm.

In either case, a simple idea is to summarise the knowledge of the Wumpus problem as a matrix $G$ where $G[x,y]$ indicates one of:
- Empty.
- Hole.
- Wumpus.
- Treasure.
- Breeze Observed.
- Stink Smelled.
- Unknown.

For simplicity, you can always start with the setting where you know you are dealing with one of a *small number* of possible worlds.

*** Static, Stochastic-Observation, Unobservable Wumpus

Here we assume the Wumpus does not move, and observations are stochastic: sometimes we feel a breeze, sometimes not. We assume we know the probability of a breeze.

The first problem is to summarise what we know about the Wumpus problem.
Now we can have an entry $G[x,y]$ in the matrix which is a *vector of probabilities* for the possible contents of the co-ordinate:
(Empty, Hole, Wumpus, Treasure)

For simplicity, you can always start with the setting where you know you are dealing with one of a *small number* of possible worlds. Then you only need to deal with the probability of each world being the right one.





