#+TITLE: Artificial Intelligence
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{algorithm,algorithmic}
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \cset[2] {\left\{#1 ~\middle|~ #2 \right\}}
#+LaTeX_HEADER: \newcommand \pol {\pi}
#+LaTeX_HEADER: \newcommand \Pols {\Pi}
#+LaTeX_HEADER: \newcommand \mdp {\mu}
#+LaTeX_HEADER: \newcommand \MDPs {\mathcal{M}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Bels {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \Unif {\textrm{Unif}}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Mult}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Dir {\textrm{Dir}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \Simplex {\mathbb{\Delta}}
#+LaTeX_HEADER: \newcommand \pn {\param^{(n)}}
#+LaTeX_HEADER: \newcommand \pnn {\param^{(n+1)}}
#+LaTeX_HEADER: \newcommand \pnp {\param^{(n-1)}}
#+LaTeX_HEADER: \usetikzlibrary{shapes.geometric}
#+LaTeX_HEADER: \tikzstyle{utility}=[diamond,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=8mm]
#+LaTeX_HEADER: \tikzstyle{select}=[rectangle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{hidden}=[dashed,draw=black,fill=red!10]
#+LaTeX_HEADER: \tikzstyle{RV}=[circle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* Introduction

** About the course  
*** Aims
This course will focus on algorithms and models for Artificial
Intelligence.  We will concentrate mainly on the decision making,
rather than the learning, side of artificial intelligence. Learning is
already addressed in statistics courses, as well as the machine
learning course in the final year.

*** Philosophy
The philosophy of this course is as follows: 
- We give example problems.
- We use theory to explain and generalise from those examples to general problems.
- We describe algorithms to /solve/ general problems.
- We implement algorithms to solve the specific examples.

In general, the course will start from the simplest problems and
slowly progress to the more complex ones.

*** How to use this notebook
- This notebook contains a summary of all the foundational material in the course. It is not sufficient for studying for the exam, or as a reference material.
- For details, use the links to external resources.
- There are some more detailed slides available for each lecture.
*** Books and schedule
Artificial Intelligence: Foundations of Computational Agents, 3rd Edition
Artificial Intelligence: a Modern Approach, 4th Edition

|--------+-----------------------------+---------------------------------+------------------------------------|
| Module | Topics                      | AI:FoCA                         | AI: aMA                            |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      1 | - Preferences               | 1. AI and Agents                | 2.1. Agents and environments       |
|        | - Utility                   | 1.2. Complexity                 | 2.2 Rationality                    |
|        | - States                    | 1.3. Application domains        | 2.3. Environments                  |
|        | - Actions                   | 1.4. Knowledge representation   | 2.4. Agents                        |
|        | - Beliefs                   | 2. Architecture                 | 2.4.1 Programs                     |
|        | - Fairness                  | 2.1. Control                    | 2.4.2-3 Reflex agents              |
|        |                             | 2.2. Hierarchical control       | 2.4.4. Goals                       |
|        |                             | 2.3. Moral machines             | 2.4.5. Utility                     |
|        |                             |                                 | 2.4.6. Learning                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      2 | Depth-First Search          | 3. Search                       | 3.1 Problem-solving                |
|        | Breadth-First Search        | 3.1. Search in graphs           | 3.2. Examples.                     |
|        |                             |                                 | 3.3. Best-First search             |
|        |                             |                                 | 3.4.1. Breadth-first               |
|        |                             |                                 | 3.4.3. Depth-first search          |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      3 | Heuristic Search            | 3.2. Uninformed search          | 3.5.2. A*                          |
|        | A* Search                   | 3.3. Heuristic search           | 3.6. Heuristic Functions           |
|        |                             |                                 |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      4 | Dynamic Programming         |                                 | 3.4.2 Dijkstra                     |
|        |                             | 3.4. Dynamic programming        |                                    |
|        | Branch and bound            | 3.5. Branch and bound           |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      5 | Constraint programming      | 4. Reasoning with constraints   | 6. CSP                             |
|        | Logical reasoning           | 4.1. Variables and Constraints  | 7. Logical Agents                  |
|        |                             | 4.2. CSPs and Search            |                                    |
|        |                             | 4.6. Local Search               |                                    |
|        | Deterministic planning      | 4.8. Optimization               |                                    |
|        |                             |                                 |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      6 | Uncertainty                 | 9. Reasoning with Uncertainty   | 12.1. Acting under uncertaint      |
|        | Aleatory/Epistemic          | 9.1. Probability                | 12.2. Basic probability notation   |
|        | Probability Theory          | 9.2. Independence               | 12.3. Inference                    |
|        | Bayes Theorem               |                                 | 12.4. Independence                 |
|        | Probabilistic inference     |                                 | 12.5. Bayes's theorem              |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      7 | Expected Utility Theory     | 12.1 Preferences and Utility    | 16.1. Beliefs and Desires          |
|        |                             | 12.2 One-off decisions          | 16.2. utility theory               |
|        |                             |                                 | 16.3. Utility functions            |
|        |                             |                                 | 16.5. Decision networks            |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      8 | Markov Decision Processes   | 12.3 Sequential Decisions       | 17.1. Sequential decision problems |
|        | Dynamic Programming         | 12.4 The value of information   | 17.2. Algorithms for MDPs          |
|        |                             | 12.5 Decision processes         |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      9 | Alternating Zero-Sum Games  | 14.1. Multi-agent framework     | 5.1. Game Theory                   |
|        | Stochastic Zero-Sum Games   | 14.2. Representations of games  | 5.2. Zero-Sum Games                |
|        | Linear programming          | 14.3. Perfect information games | 5.3. Alpha-Beta Search             |
|        |                             |                                 | 5.5. Stochastic Games              |
|--------+-----------------------------+---------------------------------+------------------------------------|
|     10 | Belief networks             | 9.3. Belief Networks            | 13.1. Representing knowledge       |
|        |                             | 9.4. Probabilistic Inference    | 13.2. Bayesian Networks            |
|        |                             |                                 | 13.3. Exact inference in BNs       |
|--------+-----------------------------+---------------------------------+------------------------------------|
|     11 | Supervised learning         |                                 |                                    |
|        | Learning as inference       |                                 |                                    |
|        | Learning as optimisation    |                                 |                                    |
|        | Stochastic gradient descent |                                 |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|     12 | Reinforcement learning      |                                 |                                    |
|        | Bandit problems             |                                 |                                    |
|        | Q-learning                  |                                 |                                    |
|        | Stochastic approximation    |                                 |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|



*** Notation
- $\Reals, \Reals^d$: the real line and $d$-dimension Euclidean space
- $\Simplex^d$ the $d$-dimensional simplex
- $\Simplex(A)$ the set of distributions over $A$.
- $\ind{x}$: indicator function (1 if $x$ is true, 0 otherwise)
- $\Pr$: probability
- $\E$: expectation
- $\pol \in \Pols$: policies, or algorithms.
- $\mdp \in \MDPs$: models
- $\param \in \Params$: parameters (i.e. models parameterised by vectors in $\Reals^d$)
- $u$: utility
- $c$: cost / constraints
- $s  \in S$: state
- $a  \in A$: action
- $r  \in \Reals$: reward

** Project
*** Application project.
Application projects proposals need to contain the following:
-  Domain description and goals: What is the problem, in general terms, and which aspect would you try and solve in an AI framework? Make sure to cite relevant literature and course material. 
-  Methodology: How would you formalise the problem mathematically? Which algorithms and/or models do you intend to apply at different stages of the project? Feel free to read widely about both the problem and algorithms and do cite relevant literature. Make sure to employ techniques taught in the course, e.g. logic + search or probabilities and MDPs etc.
-  Experiment design: How would you know that the method is working? How would you compare with existing solutions? In what context would you expect an improvement? How would you measure it? How will you test the robustness of your solution over variations in the problem instance?
- Expected results: What results do you expect to obtain, and what do you think might go wrong? In what way do you expect an improvement?

*** Algorithmic project.
-  Algorithmic/theory problem and goals: What is the deficiency, in general terms, of current theory and algorithms that your method would try to improve? As an example, the goal could be reducing computational complexity, increasing data efficiency, improving robustness or applicability of a specific family of algorithms; or introducing a slightly different setting to existing ones. In other words, which is the open problem you will be addressing? Make sure to cite relevant literature to better identify the problem.
-  Methodology: What kind of existing algorithms, theory or technical result would you rely on? Would you be combining various existing results? What would be the most significant novelty of your methodology? Do cite relevant literature.
- Experiment design (if applicable): How would you know that the method is working? How would you compare with existing solutions? In what context would you expect an improvement? How would you measure it? 
- Expected results: What results do you expect to obtain, and what do you think might go wrong? In what way do you expect an improvement?

*** Grading for projets:

Grades will be adjusted based on group size with on letter grade up/down for double/half the mean group size.

- Environments: A. Complex, well described environment that captures all of the elements of the application or algorithmic problem. B. The environment is simple or lacks description. C. An adequate environment that captures the basic setting. D. Insufficient environment or description. E. Insuffcient environment and description.
- Algorithms: A. Significantly novel algorithms that are well described. B. Some novelty in the algorithms, with good descriptions. C. Some novelty in the algorithms, but descriptions are lacking. D. Insufficient novelty or descriptions. E. Insufficient novelty and descriptions. 
- Experiments: A. Thorough experiments with ablation tests and comparisons over algorithms and environments, that are well-described. B. Somewhat incomplete experiments or descriptions. C. Sufficient experiments and descriptions. D. Insufficient experiments or descriptions. E. Insufficient experiments and descriptions.

Criteria for full marks in each part of the project are the following. 

1. Documenting of the work in a way that enables reproduction.
2. Technical correctness of their analysis.
3. Demonstrating that they have understood the assumptions underlying their analysis.
4. Addressing issues of reproducibility in research.
5. Addressing scientific and ethical questions where applicable, and if not, clearly explain why they are not.
6. Consulting additional resources beyond the source material with proper citations.

The follow marking guidelines are what one would expect from students attaining each grade. 

Detailed grading

*** A (6)


1. Submission of a detailed report from which one can definitely reconstruct their work without referring to their code. There should be no ambiguities in the described methodology. Well-documented code where design decisions are explained. 
2. Extensive analysis and discussion. Technical correctness of their analysis. Nearly error-free implementation.
3. The report should detail what models are used and what the assumptions are behind them. The conclusions of the should include appropriate caveats.  When the problem includes simple decision making, the optimality metric should be well-defined and justified. Simiarly, when well-defined optimality criteria should given for the experiment design, when necessary. The design should be (to some degree of approximation, depending on problem complexity) optimal according to this criteria.
4. Appropriate methods to measure reproducibility. Use of cross-validation or hold-out sets to measure performance. Use of an unbiased methodology for algorithm, model or parameter selection. Appropriate reporting of a confidence level (e.g. using bootstrapping) in their analytical results. Relevant assumptions are mentioned when required.
5. A clear definition of a scientific question. When dealing with data relating to humans, ethical concerns, such as privacy and/or fairness should be addressed.
6. The report contains some independent thinking, or includes additional resources beyond the source material with proper citations. The students go beyond their way to research material and implement methods not discussed in the course.

*** B (5.5)

1. Submission of a report from which one can plausibly reconstruct their work without referring to their code. There should be no major ambiguities in the described methodology. 
2. Technical correctness of their analysis, with a good discussion. Possibly minor errors in the implementation.
3. The report should detail what models are used, as well as the optimality criteria, including for the experiment design. The conclusions of the report must contain appropriate caveats. 
4. Use of cross-validation or hold-out sets to measure performance. Use of an unbiased methodology for algorithm, model or parameter selection. 
5. When dealing with data relating to humans, ethical concerns such as privacy and/or fairness should be addressed. While an analysis of this issue may not be performed, there is a substantial discussion of the issue that clearly shows understanding by the student.
6. The report contains some independent thinking, or the students mention other methods beyond the source material, with proper citations, but do not further investigate them.
   
*** C (5)

1. Submission of a report from which one can partially reconstruct most of their work without referring to their code. There might be some ambiguities in parts of the described methodology. 
2. Technical correctness of their analysis, with an adequate discussion. Some errors in a part of the implementation.
3. The report should detail what models are used, as well as the optimality criteria and the choice of experiment design. Analysis caveats are not included.
4. Either use of cross-validation or hold-out sets to measure performance, or use of an unbiased methodology for algorithm, model or parameter selection - but in a possibly inconsistent manner.
5. When dealing with data relating to humans, ethical issues are addressed superficially.
6. There is little mention of methods beyond the source material or independent thinking.

*** D (4.5)

1. Submission of a report from which one can partially reconstruct most of their work without referring to their code. There might be serious ambiguities in parts of the described methodology. 
2. Technical correctness of their analysis with limited discussion. Possibly major errors in a part of the implementation.
3. The report should detail what models are used, as well as the optimality criteria. Analysis caveats are not included.
4. Either use of cross-validation or hold-out sets to measure performance, or use of an unbiased methodology for algorithm, model or parameter selection - but in a possibly inconsistent manner.
5. When dealing with data relating to humans, ethical issues are addressed superficially or not at all.
6. There is little mention of methods beyond the source material or independent thinking.

*** E (4)
1. Submission of a report from which one can obtain a high-level idea of their work without referring to their code. There might be serious ambiguities in all of the described methodology. 
2. Technical correctness of their analysis with very little discussion. Possibly major errors in only a part of the implementation.
3. The report might mention what models are used or the optimality criteria, but not in sufficient detail and caveats are not mentioned.
4. Use of cross-validation or hold-out sets to simultaneously measure performance and optimise hyperparameters, but possibly in a way that introduces some bias.
5. When dealing with data relating to humans, ethical issues are not discussed.
6. There is no mention of methods beyond the source material or independent thinking.

*** F (<3)

1. The report does not adequately explain their work.
2. There is very little discussion and major parts of the analysis are technically incorrect, or there are errors in the implementation.
3. The models used might be mentioned, but not any other details.
4. There is no effort to ensure reproducibility or robustness.
5. When applicable: Ethical issues are not mentioned.
6. There is no mention of methods beyond the source material or independent thinking.


** Schedule

| 2.22 | 1-2 Introduction                                             |
| 2.29 | 3.1-3.5 Search, State Spaces, Graphs, Uniformed Search.      |
| 3.07 | 3.6. Informed Search, Heuristics, A*                         |
| 3.14 | 4.1-4.2, 4.8 Constrained Search                              |
| 3.21 | Exercises and Project Day                                    |
| 3.28 | 9.1 Probability, Independence, Belief Networks               |
| 4.11 | 12.1. Preferences and Utility. 12.2 Probability. Decision making. |
| 4.18 | Statistical estimation and supervised learning               |
| 4.25 | Markov decision processes                                    |
| 5.02 | Markov games                                                 |
| 5.16 | Simultaneous Games                                           |
| 5.23 | Reinforcement Learning                                       |
| 5.30 | Project presentations                                        |



* single agent problems with certainty
** Uninformed search
| Graph definitions         | 10 |
| Tree example              |  5 |
| Shortcut example          |  5 |
| Depth-first search        | 10 |
| The shortest path problem | 10 |
| Goals and DFS             | 10 |
| Shortest-path DFS         | 10 |
| Breadth-first search      | 10 |
| Iterative deepening       | 10 |
| Uniform cost search       | 10 |
|---------------------------+----|
|                           | 95 |
#+TBLFM: @11$2=vsum(@1..@10)

** Informed search
*** Heuristics
*** $A^*$-search

*** Dynamic programming

*** Branch and Bound

** Constraints
|-------------------------+-----|
| Local search            |  20 |
|-------------------------+-----|
| Constraint Satisfaction |  20 |
| Graph Colouring         |  20 |
| Meeting Scheduling      |  20 |
|-------------------------+-----|
| onstraint Optimisation  |  20 |
| Travelling Salesman     |  20 |
| Maximum Flow            |  20 |
|-------------------------+-----|
| Logical constraints     |  20 |
| Towers of Hanoi         |  20 |
|-------------------------+-----|
|                         | 160 |
#+TBLFM: @9$2=vsum(@1..@8)

** Infinite choices
*** Lipschitz search
If we know the function $f$ is Lipschitz-smooth, i.e.
\[
\exists L > 0 : |f(x) - f(y)| \leq L |x  - y|,
\]
then we also know that for any point $z$:
\[
f(z) < f(x) + L |x - z|,
\qquad
f(z) < f(y) + L |y - z|
\]
**** Schubert's Algorithm [[https://www.jstor.org/stable/2156138][(Schubert, 1972)]]
\begin{algorithmic}
\STATE \textbf{Input:} $L > 0$, $X$, $x_0 \in X$.
\FOR {$t=1, \ldots, T$}
\STATE $x_{t} = \argmax_{x \in X} \min \cset{f(x_k) + L|x_k - x|}{k=0, \ldots, t-1}$
\ENDFOR
\end{algorithmic}
**** Discussion
- This is guaranteed to *converge* to the optimal solution.
- If $L$ is *unknown*, DIRECT [[http://www.planchet.net/EXT/ISFA/1226.nsf/769998e0a65ea348c1257052003eb94f/f9ca730ca27def69c12576d8002ed895/$FILE/Jones93.pdf][(Jones et al. 1993)]] can be used.
- If $f$ is noisy, the problem becomes a *continuum bandit* problem.
*** First-order gradient methods
- Gradient descent
- Stochastic gradient descent
**** Properties  
- Incremental algorithms
- Can converge to a *local* optimum

*** Single-variable gradient descent
**** Setting
- Input: $f : \Reals \to \Reals$
- Problem: $\max_x f(x)$
- Derivative: $\frac{d}{dx} f(x) \defn \lim_{\Delta \to 0} \frac{f(x + \Delta)  - f(x)}{\Delta}$.
**** Algorithm
1) Input: $x^{(0)}$, f
2) For $t = 1, \ldots$:
3) Calculate direction $g_t = \frac{d}{dx} f(x_{t-1})$
4) Select step-size $\alpha_t$
5) Update $x^{(t)} = x^{(t-1)} + \alpha_t g_t$.
   
*** Multiple-variable gradient descent
**** Setting
- Input: $f : \Reals^d \to \Reals$, $x = (x_1, \ldots, x_d)$
- Problem: $\max_x f(x)$
- Partial Derivative: $\frac{\partial}{\partial x_i} f(x) \defn \lim_{\Delta \to 0} \frac{f(x_1, \ldots, x_i + \Delta, \ldots, x_d)  - f(x)}{\Delta}$.
- Gradient $\nabla_x f(x) = \left[\frac{\partial}{\partial x_1} f(x), \ldots, \frac{\partial}{\partial x_i} f(x), \ldots, \frac{\partial}{\partial x_d} f(x)\right]^\top$.
**** Algorithm
1) Input: $x_0$, f
2) For $t = 1, \ldots$:
3) Calculate direction $g_t = \nabla_x f(x_{t-1})$
4) Select step-size $\alpha_t$
5) Update $x_{t} = x_{t-1} + \alpha_t g_t$.
   
*** Stochastic gradient descent
**** As gradient descent with errors
- Calculate direction $g_t = \nabla_x f(x_{t-1}) + \epsilon_t$
- $\epsilon_t$ is typically zero-mean noise.
**** In learning from data
The gradient can be broken up into a sum of gradients:
\[
f(x) = \sum_t v(x, z_t),
\qquad
\nabla_x f(x) = \sum_t \nabla_x v(x, z_t),
\]
$x_t = x_{t-1} + \alpha_t \nabla_x v(x, z_t)$.
**** In Bayesian quadrature
The function is an expectation:
\[
f(x) = \int_Z v(x, z) p(z) dz.
\qquad
\nabla_x f(x) \approx \sum_t \nabla_x v(x, z_t), 
\]
where $z_t \sim p(z)$ are samples from $p$.

* single agent problems with uncertainty
** Probability
*** Probability fundamentals
**** Probability measure $P$
- Defined on a universe $\Omega$
- $P : \Sigma \to [0,1]$ is a function of subsets of $\Omega$.
- A subset $A \subset \Omega$ is an *event* and $P$ measures its likelihood.
**** Axioms of probability
- $P(\Omega) = 1$
- For $A, B \subset \Omega$, if $A \cap B = \emptyset$ then $P(A \cup B) = P(A) + P(B)$.
**** Marginalisation
If $A_1, \ldots, A_n \subset \Omega$ are a partition of $\Omega$
\[
P(B) = \sum_{i = 1}^n P(B \cap A_i).
\]
** Conditional probability and independence
*** Conditional probability
**** Conditional probability
    :PROPERTIES:
    :BEAMER_env: definition
    :END:
The conditional probability of an event $A$ given an event $B$ is defined as 
\[
P(A | B) \defn \frac{P(A \cap B)}{P(B)}
\]
The above definition requires $P(B)$ to exist and be positive.

**** Conditional probabilities as a collection of probabilities
More generally, we can define conditional probabilities as simply a
collection of probability distributions:
\[
\cset{P_\param(A)}{\theta \in \Param},
\]
where $\Param$ is an arbitrary set. 

*** The theorem of Bayes
**** Bayes's theorem
    :PROPERTIES:
    :BEAMER_env: theorem
    :END:
\[
P(A | B) = \frac{P(B | A)}{P(B)} 
\]
#+BEAMER: \pause

**** The general case
If $A_1, \ldots, A_n$ are a partition of $\Omega$, meaning that they
are mutually exclusive events (i.e. $A_i \cap A_j = \emptyset$ for $i
\neq j$) such that one of them must be true (i.e. $\bigcup_{i=1}^n A_i =
\Omega$), then
\[
P(B) = \sum_{i=1}^n P(B | A_i) P(A_i)
\]
and 
\[
P(A_j | B) = \frac{P(B | A_j)}{\sum_{i=1}^n P(B | A_i) P(A_i)}
\]

*** Independence
**** Independent events
$A, B$ are independent iff $P(A \cap B) = P(A) P(B)$.
**** Conditional independence
 $A, B$ are conditionally independent given $C$ iff $P(A \cap B | C) = P(A | C) P(B | C)$.
** Random variables and expectation 
*** Random variables
A random variable $f : \Omega \to \Reals$ is a real-value function measurable with respect to the underlying probability measure $P$, and we write $f \sim P$.
**** The distribution of $f$
The probability that $f$ lies in some subset $A \subset \Reals$ is
\[
P_f(A) \defn P(\{\omega \in \Omega : f(\omega) \in A\}).
\]
**** Independence
Two RVs $f,g$ are independent in the same way that events are independent:
\[
P(f \in A \wedge g \in B) = P(f \in A) P(g \in B) = P_f(A) P_g(B).
\]
In that sense, $f \sim P_f$ and $g \sim P_g$.

*** Expectation
For any real-valued random variable $f: \Omega \to \Reals$, the expectation with respect to a probability measure $P$ is
\[
\E_P(f) = \sum_{\omega \in \Omega} f(\omega) P(\omega).
\]
**** Linearity of expectations
For any RVs $x, y$:
\[
\E_P(x + y) = \E_P(x) + \E_P(y)
\]
**** Independence
If $x,y$ are independent RVs then $\E_P(xy) = \E(x)\E(y)$.
**** Correlation
If $x,y$ are *not* correlated then $\E_P(xy) = \E(x)\E(y)$.
**** IID (Independent and Identically Distributed) random variables
A sequence $x_t$ of r.v.s is IID if $x_t \sim P$
$(x_1, \ldots, x_t, \ldots, x_T) \sim P^T$.

*** Conditional expectation
The conditional expectation of a random variable $f: \Omega \to \Reals$, with respect to a probability measure $P$ conditioned on some event $B$ is simply
\[
\E_P(f | B) = \sum_{\omega \in \Omega} f(\omega) P(\omega | B).
\]

** Statistical Decision Theory
*** Expected utility
**** Actions, outcomes and utility
In this setting, we obtain random outcomes that depend on our actions.
- Actions $a \in A$
- Outcomes $\omega \in \Omega$.
- Probability of outcomes $P(\omega \mid a)$
- Utility $U : \Omega \to \Reals$
**** Expected utility
The expected utility of an action is:
\[
\E_P[U \mid a] = \sum_{\omega \in \Omega} U(\omega) P(\omega \mid a).
\]

**** The expected utility hypothesis
We prefer $a$ to $a'$ if and only if
\[
\E_P[U \mid a] \geq \E_P[U \mid a']
\]

** Supervised learning
*** Supervised learning

** Markov decision processes
*** Markov decision process
- Action space $A$.
- State space $S$.
- Transition kernel $s_{t+1} = j \mid s_t = s, a_t = a \sim P_\mdp(j \mid s, a)$.
- Reward $r_t = \rho(s_t, a_t)$ (can also be random).
- Utility
\[
U_t = \sum_{k=t}^T r_t.
\]
*** Value functions
**** The state value function
For any given MDP $\mdp$ and policy $\pol$ we define
\[
V^\pol_{\mdp, t}(s) \defn \E^\pol_{\mdp, t} \left[ U_t ~\middle|~ s_t = s \right]
\]
**** The state-action value function
\[
Q^\pol_{\mdp, t}(s, a) \defn \E^\pol_{\mdp, t} \left[ U_t ~\middle|~ s_t = s, a_t = a \right]
\]
**** The optimal value functions
For an optimal policy $\pol^*$
\[
V^*_{\mdp, t}(s) \defn V^{\pol^*}_{\mdp, t}(s) \geq V^\pol_{\mdp, t}(s),
\qquad
Q^*_{\mdp, t}(s,a) \defn Q^{\pol^*}_{\mdp, t}(s,a) \geq V^\pol_{\mdp, t}(s,a) 
\]
*** The Bellman equations
**** State value function
\begin{align*}
V^\pol_{\mdp, t}(s)
& \defn \E^\pol_{\mdp}[U_{t}\mid s_t = s] \\
& = \E^\pol_{\mdp}[r_t + U_{t+1}\mid s_t = s] \\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \E^\pol_{\mdp}[U_{t+1} \mid s_t = s]\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} \E^\pol_{\mdp}[U_{t+1} \mid s_{t+1} = j] \Pr^\pol_\mdp(s_{t+1} = j \mid s_t = s)\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} V^\pol_{\mdp, t+1}(j)  \Pr^\pol_\mdp(s_{t+1} = j \mid s_t = s)\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} V^\pol_{\mdp, t+1}(j) \sum_{a \in A} P_\mdp(j \mid s, a) \pol(a_t \mid s_t).
\end{align*}
**** State-action value function
\begin{align*}
Q^\pol_{\mdp, t}(s)
&= \rho(s,a) +  \sum_{j \in S} V^\pol_{\mdp, t+1}(j) P_\mdp(j \mid s, a)
\end{align*}

*** Optimal policies

**** Bellman optimality condition
The value function of the optimal policy satisfies this:
\begin{align*}
V^*_{\mdp, t}(s)
& = 
\max_{a}  [\rho(s,a) +  \sum_{j \in S} V^*_{\mdp, t+1}(j) P_\mdp(j \mid s, a)]
\end{align*}
**** Dynamic programming 
To find $V^*, Q^*$, first initialise $V^*_{\mdp, T}(s) = \max_a \rho(s,a)$. 
Then for $t = T-1, T-2, \ldots, 1$:
\begin{align*}
Q^*_{\mdp, t}(s,a) &= \rho(s,a) +  \sum_{j \in S} V^*_{\mdp, t+1}(j) P_\mdp(j \mid s, a).\\
V^*_{\mdp, t}(s) &= \max_a Q^*_{\mdp, t}(s,a).
\end{align*}
**** The optimal policy
The optimal policy is deterministic with:
\[
a_t = \argmax_a Q^*(s_t, a)
\]

* Multi-player Games
** Introduction
*** Multi-agent decision making
- *Two* versus $n$-player games
- *Co-operative* games
- *Zero-sum* games
- General-sum games
- *Stochastic* games
- Partial information games
*** Prisoner's Dilemma
*** Prisoner's Choice

** Two-Player zero-sum Games
*** Extensive-form alternating-move games
- At time $t$:
- Player chooses action $a_t$, which is revealed.
- Player chooses action $b_t$.
- Player $a$ receives $\rho(a_t, b_t)$ and $b$ receives $-\rho(a_t, b_t)$.
The utility for each player is 
$U = \sum_t \rho(a_t, b_t)$.
*** Backwards induction for ZSG 
\begin{algorithmic}
\FOR {$t=T, T-1, \ldots, 1$} 
\STATE x
\ENDFOR
\end{algorithmic}
*** Normal-form simultaneous-move games
- Player $a$ chooses action $a$ in secret.
- Player $b$ chooses action $b$ in secret.
- Players observe both actions
- Player $a$ receives $U(a,b)$, and $b$ receives $-U(a,b)$. 

*** Linear Programming
*** The linear programming problem
Linear programming is a constrained minimisation problem where the objective and the constraints are both linear.
\begin{align*}
\min_x~ & \theta^\top x\\
\textrm{s.t.~} & c^\top x \geq 0.
\end{align*}
We can have

