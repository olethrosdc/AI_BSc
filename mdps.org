#+TITLE: Markov Decision Processes
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+startup: beamer
#+LaTeX_CLASS: beamer
#+LaTeX_HEADER: \usepackage{algorithm,algorithmic}
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \usepackage{tikzsymbols}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \callcset[2] {\left\{#1 ~\middle|~ #2 \right\}}
#+LaTeX_HEADER: \newcommand \pol {\pi}
#+LaTeX_HEADER: \newcommand \Pols {\Pi}
#+LaTeX_HEADER: \newcommand \mdp {\mu}
#+LaTeX_HEADER: \newcommand \MDPs {\mathcal{M}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Bels {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \Unif {\textrm{Unif}}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Mult}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Dir {\textrm{Dir}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \Simplex {\mathbb{\Delta}}
#+LaTeX_HEADER: \newcommand \pn {\param^{(n)}}
#+LaTeX_HEADER: \newcommand \pnn {\param^{(n+1)}}
#+LaTeX_HEADER: \newcommand \pnp {\param^{(n-1)}}
#+LaTeX_HEADER: \newcommand{\indep}{\mathrel{\text{\scalebox{1.07}{$\perp\mkern-10mu\perp$}}}}
#+LaTeX_HEADER: \usetikzlibrary{shapes.geometric}
#+LaTeX_HEADER: \usetikzlibrary{arrows.meta, positioning, quotes}
#+LaTeX_HEADER: \tikzstyle{utility}=[diamond,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=8mm]
#+LaTeX_HEADER: \tikzstyle{select}=[rectangle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{hidden}=[dashed,draw=black,fill=red!10]
#+LaTeX_HEADER: \tikzstyle{RV}=[circle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{con}=[rectangle,draw=white,fill=gray,inner sep=0mm, minimum size=6mm]
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* Markov decision processes
#+TOC: headlines [currentsection]
** The Markov process
*** Markov processes
[[./figures/Snakes_and_Ladders.jpeg]]
*** Markov Process
A sequence of random variables 
$s_1, s_2, \ldots, s_t$ is a *Markov process* if the nest variable $s_{t+1}$ only depends on the current value $s_t$
\[
P(s_{t+1} \mid s_{t}, \ldots, s_1) = P(s_{t+1} \mid s_{t}) \qquad \forall t
\]
#+BEAMER: \pause
- The variable $s_t \in S$ is the current *state* of the process.
- For finite $S$, the matrix $p_{i,j} \defn P(s_t = j \mid s_{t-1} = i)$ is called the *transition matrix*.
#+BEAMER: \pause
**** Bayesian network
It is sometimes called  *Markov chain* because of the structure formed by the RVs:
\begin{tikzpicture}
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \draw[->] (stp) -- (st);
  \draw[->] (st) -- (stn);
\end{tikzpicture}
#+BEAMER: \pause
**** Snakes and ladders
- What is the state?
- What is the transition matrix?

*** Markov Decision Process
**** Variables                                                        :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.4
     :END:
- The *state* $s_t \in S$.
- The *action* $a_t \in A$.
- The *reward* $r_t \in \Reals$.
#+BEAMER: \pause
**** Bayesian network                                                 :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.6
     :END:
\begin{tikzpicture}
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \node[select] at (0,2) (atp) {$a_{t-1}$};
  \node[select] at (2,2) (at) {$a_t$};
  \node[utility] at (2,1) (rtp) {$r_{t}$};
  \node[utility] at (4,1) (rt) {$r_{t+1}$};
  \draw[->] (atp) -- (st);
  \draw[->] (atp) -- (rtp);
  \draw[->] (stp) -- (rtp);
  \draw[->] (at) -- (rt);
  \draw[->] (st) -- (rt);
  \draw[->] (at) -- (stn);
  \draw[->] (stp) -- (st);
  \draw[->] (st) -- (stn);
\end{tikzpicture}

**** Markov Decision Process :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
A Markov decision process $\mdp$ on $(S, A)$ has the property that for any sequence of actions $a_1, \ldots$
\begin{align*}
P_\mdp(s_{t+1} \mid s_t, a_t, r_{t}, s_{t-1}, a_{t-1}, \ldots) = P_\mdp(s_{t+1} \mid s_t, a_t)
\\
P_\mdp(r_{t+1} \mid s_t, a_t, r_{t}, s_{t-1}, a_{t-1}, \ldots) = P_\mdp(r_{t+1} \mid s_t, a_t)
\end{align*}
The goal in a *finite-horizon* MDP is to maximise the \(T\)-horizon utility:
\[
U = \sum_{t=1}^T r_t
\]
*** Policies in Markov decision processes

**** Variables                                                        :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.4
     :END:
- The *policy* $\pol$
- The *state* $s_t \in S$.
- The *action* $a_t \in A$.
- The *reward* $r_t \in \Reals$.
#+BEAMER: \pause
**** Bayesian network                                                 :BMCOL:
     :PROPERTIES:
     :BEAMER_col: 0.6
     :END:
\begin{tikzpicture}
  \node[select] at (2,3) (pol) {$\pol$};
  \node[RV] at (0,0) (stp) {$s_{t-1}$};
  \node[RV] at (2,0) (st) {$s_t$};
  \node[RV] at (4,0) (stn) {$s_{t+1}$};
  \node[RV] at (0,2) (atp) {$a_{t-1}$};
  \node[RV] at (2,2) (at) {$a_t$};
  \node[utility] at (2,1) (rtp) {$r_{t}$};
  \node[utility] at (4,1) (rt) {$r_{t+1}$};
  \draw[->] (pol) -- (atp);
  \draw[->] (pol) -- (at);
  \draw[->] (atp) -- (st);
  \draw[->] (st) to [bend right = 45] (at);
  \draw[->] (stp) to [bend right = 45] (atp);
  \draw[->] (atp) -- (rtp);
  \draw[->] (stp) -- (rtp);
  \draw[->] (at) -- (rt);
  \draw[->] (st) -- (rt);
  \draw[->] (at) -- (stn);
  \draw[->] (stp) -- (st);
  \draw[->] (st) -- (stn);
\end{tikzpicture}

**** Markov Policy                                             :B_definition:
     :PROPERTIES:
     :BEAMER_env: definition
     :END:
A Markov policy takes an action $a$ at time $t$ with probability 
\[
\pol(a_t = a \mid s_t = s)
\]
**** The expected utility of a policy
\[
\E_\pol[U] = \sum_{t=1}^T \E_\pol[r_t]
\]
 
* Backwards induction
#+TOC: headlines [currentsection]

** Utility and value functions

*** Value function
- The utility from step $t$ is $U_t \defn \sum_{k=t}^T r_k$
**** The state value function
This is the expected utility obtained by following a *policy $\pol$* starting from some *state $s$*.
\[
V^\pol_t(s) \defn \E_\pol(U_t \mid s_t = s)
\]
**** The state-action value function
This is the expected utility obtained by following a policy $\pol$ starting from some state $s$ and *playing action $a$*
\[
Q^\pol_t(s, a) \defn \E_\pol(U_t \mid s_t = s, a_t = a)
\]
**** The optimal value function
There is some policy $\pol^*$ satisfying
\begin{align*}
V^*(s) &\defn V^{\pol^*}(s) \geq V^\pol(s) \qquad \forall \pol, s
\\
Q^*(s, a) &\defn Q^{\pol^*}(s, a) \geq Q^\pol(s, a) \qquad \forall \pol, s, a
\end{align*}


*** The expected utility recursion
Value functions satisfy the following recursion
\begin{align*}
V^\pol_t(s_t) 
&= \E_\pol(U_t \mid s_t)\\
&= \E_\pol\left[\sum_{t=1}^T r_t \middle| s_t\right]\\
&= \E_\pol[r_t \mid s_t] + \E_\pol\left[\sum_{k=t+2}^T r_k \middle| s_t  \right]\\
&= \E_\pol[r_t \mid s_t] + \E_\pol\left[U_{t+1}| s_t \right]\\
&= \E_\pol[r_t \mid s_t] + \sum_{s_{t+1} \in S} \Pr_\pol(s_{t+1} \mid s_t) \E_\pol\left[U_{t+1}| s_{t+1} \right]\\
&= \E_\pol[r_t \mid s_t] +  \sum_a \pol(a \mid s_t) \sum_{s_{t+1} \in S} P_\mdp(s_{t+1} \mid s_t, a) V^\pol_{t+1}(s_{t+1}).
\end{align*}
**** Exercise
Prove that 
\[
Q^\pol_t(s, a) = r(s, a) + \sum_{s' \in S} P_\mdp(s' \mid s, a) \sum_{a' \in A} Q^\pol_{t+1}(s', a') \pol(a_{t+1} = a' \mid s_{t+1} = s')
\]

*** Backwards induction
**** On the state value function
To find the value function of the optimal policy, we can perform the following recursion, after setting $V^*_T(s) = \max_a r(s,a)$ for all $s$.
\[
V^*_t(s) 
=
\max_a r(s,a)  + \sum_{s' \in S} P_\mdp(s' \mid s, a) V^*_{t+1}(s'),
\]
where the optimal action at $s, t$ is $\argmax_a r(s,a)  + \sum_{s' \in S} P_\mdp(s' \mid s, a) V^*_{t+1}(s')$.
**** On the state-action value function
Alternatively, we can write this in terms of the Q-value function, where we set
$Q^*_T(s,a) = r(s,a)$ and then recurse:
\[
Q^*_t(s, a) 
=
 r(s,a)  + \sum_{s' \in S} P_\mdp(s' \mid s, a) \max_{a'} Q^*_{t+1} Q(s', a').
\]
Here the optimal action at step $t$ is just $\argmax_a Q^*_t(s,a)$.

* Examples
#+TOC: headlines [currentsection]

** Toy examples
*** Chain
\begin{tikzpicture}
  \node[RV] at (0,0) (1) {$\epsilon$};
  \node[RV] at (1,0) (2) {$0$};
  \node[RV] at (2,0) (3) {$0$};
  \node[RV] at (3,0) (4) {$0$};
  \node[RV] at (4,0) (5) {$1$};
  \draw[blue, ->] (1) -- (2);
  \draw[blue, ->] (2) -- (3);	 
  \draw[blue, ->] (3) -- (4);
  \draw[blue, ->] (4) -- (5);
  \draw[red,->] (2) to [bend right=45] (1);	
  \draw[red,->] (3) to [bend right=45] (1);	
  \draw[red,->] (4) to [bend right=45] (1);	
  \draw[red,->] (5) to [bend right=45] (1);	
  \draw[red,->] (1) to [loop above] (1);	
  \draw[blue,->] (5) to [loop above] (5);	
\end{tikzpicture}

In this MDP, there are 5 states, and the transition probabilities are:
\[
P(s_{t+1} = \min\{5, i + 1\} \mid s_t = i, a = 1) = 1 - \delta,
\qquad P(s_{t+1} = 1 \mid s_t = i, a = 1) =  \delta.
\]
For the alternative action $a = 0$, the probabilities are reversed
\[
P(s_{t+1} = \min\{5, i + 1\} \mid s_t = i, a = 1) = \delta,
\qquad P(s_{t+1} = 1 \mid s_t = i, a = 1) = 1-  \delta.
\]
Further, the reward at state $s = 1$ is $\epsilon < 1$ and the reward at state $s = 5$ is $1$.


*** Wumpus world
- State: $s_t = (x_t, y_t, d_t, w_t)$, the x-y location of the agent, the direction, and the amount of arrows left.
- Actions: $a \in \{L, R, M, S\}$ for left, right, move and shoot.
- Rewards are given for killing the Wumpus, dying, or finding the treasure.
**** Deterministic/Stochastic Wumpus
- An action/observation is always the same/is random
**** Observable/Unobservable Wumpus
- We know where the holes, the treasure and the Wumpus is/they are unknown
**** Static/Dynamic/Strategic/ Wumpus
- The Wumpus is stationary/moves according to a fixed policy/has goals to achieve
*** Deterministic, Observable Wumpus

This is the simplest setting. It is a deterministic planning problem.
For this, you can
1. Define a way to describe the Wumpus world
2. Find a policy for solving the Wumpus world as given. This policy is going to be deterministic and Markov. 

Of course, the optimal policy for each *instance* of the Wumpus problem is going to be different.

I recommend summarising the Wumpus problem in two parts:
(a) A matrix $G$ where
$G[x,y]$ is a number indicating what is contained in this location,
(b) $x_t, y_t, d_t, w_t$ being the agent-relevant variables.

You can either use any logical planning algorithm, or an MDP algorithm with deterministic transitions for this problem.

*** Stochastic, Observable Wumpus

To make the environment stochastic, we can add the following extensions

(a) The Wumpus moves according to some stochastic policy. For example, the Wumpus could randomly move in a direction, so that on average it moves away from us.
(b) Our actions do not always work (e.g. we may turn in the wrong direction)
(c) We do not always die when we encounter a hole or the Wumpus.

For this, you can
1. Define a way to describe the Wumpus world
2. Find a policy for solving the Wumpus world as given. This policy is going to be deterministic and Markov. 

Of course, the optimal policy for each *instance* of the Wumpus problem is going to be different.

I recommend summarising the Wumpus problem in two parts:
(a) A matrix $G$ where
$G[x,y]$ is a number indicating what is contained in this location,
(b) $x_t, y_t, d_t, w_t$ being the agent-relevant variables.

You can either use any logical planning algorithm, or an MDP algorithm with deterministic transitions for this problem.


*** Deterministic, Unobservable Wumpus

This setting is significantly harder to work with. Now we have observations whenever we are near a hole or the Wumpus.

You can either:
(a) Use a logical description of the world, and a SAT algorithm.
(b) Use a probabilistic description with all probabilities being 0 or 1, and an MDP algorithm.

In either case, a simple idea is to summarise the knowledge of the Wumpus problem as a matrix $G$ where $G[x,y]$ indicates one of:
- Empty.
- Hole.
- Wumpus.
- Treasure.
- Breeze Observed.
- Stink Smelled.
- Unknown.

For simplicity, you can always start with the setting where you know you are dealing with one of a *small number* of possible worlds.

*** Static, Stochastic-Observation, Unobservable Wumpus

Here we assume the Wumpus does not move, and observations are stochastic: sometimes we feel a breeze, sometimes not. We assume we know the probability of a breeze.

The first problem is to summarise what we know about the Wumpus problem.
Now we can have an entry $G[x,y]$ in the matrix which is a *vector of probabilities* for the possible contents of the co-ordinate:
(Empty, Hole, Wumpus, Treasure)

For simplicity, you can always start with the setting where you know you are dealing with one of a *small number* of possible worlds. Then you only need to deal with the probability of each world being the right one.





