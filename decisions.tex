% Created 2025-04-03 Do 16:21
% Intended LaTeX compiler: pdflatex
\documentclass[smaller]{beamer}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\input{preamble}
\AtBeginSubsection[]{\begin{frame}<beamer>\tableofcontents[currentsubsection]\end{frame}}
\usetheme{default}
\author{Christos Dimitrakakis}
\date{\today}
\title{Decisions and randomness}
\hypersetup{
 pdfauthor={Christos Dimitrakakis},
 pdftitle={Decisions and randomness},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 26.3 (Org mode 9.1.9)}, 
 pdflang={English}}
\begin{document}

\maketitle
\begin{frame}{Outline}
\tableofcontents
\end{frame}


\section{Statistical Decision Theory}
\label{sec:orgcfa13ed}
\subsection{Elementary Decision Theory}
\label{sec:org12e8694}
\begin{frame}[label={sec:org680ca91}]{Preferences}
\begin{block}{Types of rewards}
\begin{itemize}
\item For e.g. a student: Tickets to concerts.
\item For e.g. an investor: A basket of stocks, bonds and currency.
\item For everybody: Money.
\end{itemize}
\end{block}

\begin{block}{Preferences among rewards}
For any rewards \(x, y \in R\), we either
\begin{itemize}
\item (a) Prefer \(x\) at least as much as \(y\) and write \(x \preceq^* y\).
\item (b) Prefer \(x\) not more than \(y\) and write \(x \succeq^* y\).
\item (c) Prefer \(x\) about the same as \(y\) and write \(x \eqsim^* y\).
\item (d) Similarly define \(\succ^*\) and \(\prec^*\)
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:org67bce33}]{Utility and Cost}
\pause
\begin{block}{Utility function}
To make it easy, assign a utility \(U(x)\) to every reward through a
utility function \(U : R \to \Reals\).
\end{block}

\begin{block}{Utility-derived preferences}
We prefer items with higher utility, i.e.
\begin{itemize}
\item (a) \(U(x) \geq U(y)\) \(\Leftrightarrow\) \(x \succeq^* y\)
\item (b) \(U(x) \leq U(y)\) \(\Leftrightarrow\) \(y \succeq^* x\)
\end{itemize}
\pause
\end{block}
\begin{block}{Cost}
It is sometimes more convenient to define a cost function \(C: R \to \Reals\) so that we prefer items with lower cost, i.e.
\begin{itemize}
\item \(C(x) \geq C(y)\) \(\Leftrightarrow\) \(y \succeq^* x\)
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:orgd93decf}]{Random outcomes}
\begin{center}
\includegraphics[width=.9\linewidth]{./figures/roulette.jpg}
\end{center}


\begin{block}{Choosing among rewards: Roulette}
\begin{itemize}
\item\relax [A] Bet 10 CHF on black
\item\relax [B] Bet 10 CHF on 0
\item\relax [C] Bet nothing

\item What is the reward here?
\item What is the outcome?
\end{itemize}
\end{block}
\end{frame}
\begin{frame}[label={sec:orgd0373cb}]{Uncertain outcomes}
\begin{itemize}
\item\relax [A] Taking the car to Zurich (50'-80' with delays)
\item\relax [B] Taking the train to Zurich (60' without delays)
\end{itemize}
What is the reward here? 

\begin{columns}
\begin{column}{0.5\columnwidth}
\begin{center}
\includegraphics[width=.9\linewidth]{./figures/car.jpg}
\end{center}
\end{column}
\begin{column}{0.5\columnwidth}
\begin{center}
\includegraphics[width=.9\linewidth]{./figures/train.jpeg}
\end{center}
\end{column}
\end{columns}
\end{frame}




\begin{frame}[label={sec:orga39deca}]{Independent outcomes}
\begin{block}{Graphical model}
\begin{center}
      \begin{tikzpicture}
        \node[select] at (0,0) (a) {$a$};
	\node[RV] at (4,0) (w) {$\omega$};
        \node[utility] at (2,0) (U) {$U$};
	\draw[->] (a) -- (U);
	\draw[->] (w) -- (U);
      \end{tikzpicture}
\end{center}
\end{block}

\begin{block}{Random rewards}
\begin{itemize}
\item We \alert{select} our action.
\item Outcomes are \alert{random}, with \(\omega \sim P\), but \alert{independent} of our action
\item We then obtain a random \alert{utility} with distribution depending on \(a\).
\end{itemize}
\[
\Pr(U = u \mid a) = P(\{\omega : U(a, \omega) = u\})
\]
\end{block}
\end{frame}


\begin{frame}[label={sec:org3c650f4}]{General case}
\begin{block}{Graphical model}
\begin{center}
      \begin{tikzpicture}
        \node[select] at (0,0) (a) {$a$};
	\node[RV] at (2,0) (w) {$\omega$};
        \node[utility] at (4,0) (U) {$U$};
	\draw[->] (a) -- (w);
	\draw[->] (w) -- (U);
	\draw[->, dashed] (a) to [bend right=45] (U);
      \end{tikzpicture}
\end{center}
\end{block}
\begin{block}{Random rewards}
\begin{itemize}
\item We \alert{select} our action.
\item The action determines the \alert{outcome} distribution.
\item The utility may depend on \alert{both} the outcome and reward.
\end{itemize}
\end{block}
\end{frame}
\begin{frame}[label={sec:org8112ade}]{Route selection}
\begin{example}[Utility]
\begin{center}
\begin{tabular}{l|rrrrrrr}
\hline
\(U(a, \omega)\) & 30' & 40' & 50' & 60' & 70' & 80' & 90'\\
\hline
Train & -1 & -2 & -5 & -10 & -15 & -20 & -30\\
Car & -10 & -20 & -30 & -40 & -50 & -60 & -70\\
\hline
\end{tabular}
\end{center}
\end{example}

\begin{example}[Probability]
\begin{center}
\begin{tabular}{l|lllllll}
\hline
\(P(\omega \mid a)\) & 30' & 40' & 50' & 60' & 70' & 80' & 90'\\
\hline
Train & 0\% & 0\% & 50\% & 45\% & 4\% & 1\% & 0\%\\
Car & 0 & 40\% & 30\% & 15\% & 10\% & 3\% & 2\%\\
\hline
\end{tabular}
\end{center}
\end{example}
\end{frame}




\subsection{Statistical Decision Theory}
\label{sec:org40063a2}

\begin{frame}[label={sec:org9ff6121}]{Expected utility}
\begin{block}{Actions, outcomes and utility}
In this setting, we obtain random outcomes that depend on our actions.
\begin{itemize}
\item Actions \(a \in A\)
\item Outcomes \(\omega \in \Omega\).
\item Probability of outcomes \(P(\omega \mid a)\)
\item Utility \(U : \Omega \alert{\times A} \to \Reals\)
\end{itemize}
\end{block}
\begin{block}{Expected utility}
The expected utility of an action is:
\[
\E_P[U \mid a] = \sum_{\omega \in \Omega} U(\omega \alert{, a}) P(\omega \alert{\mid a}).
\]
\end{block}

\begin{block}{The expected utility hypothesis}
We prefer \(a\) to \(a'\) if and only if
\[
\E_P[U \mid a] \geq \E_P[U \mid a']
\]
\end{block}
\end{frame}

\begin{frame}[label={sec:org97206bf}]{Example: Betting}
In this example, probabilities reflect actual randomness

\begin{center}
\begin{tabular}{llrr}
\hline
Choice & Win Probability \(p\) & Payout \(w\) & Expected gain\\
\hline
Don't play & 0 & 0 & 0\\
Black & 18/37 & 2 & \\
Red & 18/37 & 2 & \\
0 & 1/37 & 36 & \\
1 & 1/37 & 36 & \\
\hline
\end{tabular}
\end{center}

\begin{center}
\includegraphics[width=.9\linewidth]{./figures/roulette.jpg}
\end{center}
What are the expected gains for these bets?
\end{frame}
\begin{frame}[label={sec:org768fc45}]{The St-Petersburg Paradox}
\begin{block}{The game}
If you give me \(x\) CHF, then I promise to:
\begin{itemize}
\item (a) Throw a fair coin until it comes heads.
\item (b) If it does so after \(T\) throws, then I will give you \(2^T\) CHF.
\end{itemize}
\end{block}
\begin{block}{The question}
\begin{itemize}
\item How much \(x\) are you willing to pay to play?
\item Given that the expected amount of money is infinite, why are you only willing to pay a small \(x\)?
\end{itemize}
\end{block}
\end{frame}

\begin{frame}[label={sec:org6c7e7d3}]{Example: Route selection}
\begin{itemize}
\item In this example, probabilities reflect subjective beliefs
\end{itemize}

\begin{center}
\begin{tabular}{lrlrl}
\hline
Choice & Best time & Chance of delay & Delay amount & Expected time\\
\hline
Train & 80 & 5\% & 5 & \\
Car, route A & 60 & 50\% & 30 & \\
Car, route B & 70 & 10\% & 10 & \\
\hline
\end{tabular}
\end{center}
\end{frame}


\begin{frame}[label={sec:orgb3350f5}]{Example: Noisy optimisation}
\begin{block}{Simple maximisation}
For a function \(f : \Reals \to \Reals\), find a maximum \(x^*\) i.e. \(f(x^*) \geq f(x) \forall x\).

\pause
\end{block}
\begin{theorem}[Necessary conditions]
If \(f: \Reals \to \Reals\) is a continuous function, a maximum point \(x^*\) satisfies:
\[
\frac{d}{dx} f(x^*) = 0,
\qquad
\frac{d}{dx^2} f(x^*) < 0.
\]

\pause
\end{theorem}
\begin{block}{Noisy optimisation}
\begin{itemize}
\item We select \(x\) but \alert{do not} observe \(f(x)\).
\item We observe a \alert{random} \(g\) with \(\E[g | x] = f(x)\).
\end{itemize}
\begin{align}
f(x) &\defn \E[g | x],
&
\E[g | x] = \int_{- \infty}^\infty g(\omega, x) p(\omega) d\omega
\end{align}
\end{block}
\end{frame}





\begin{frame}[label={sec:org07fe01b}]{Mean-squared error cost function}
\begin{tikzpicture}[domain=-1:2, range=-1:2]
   \draw[dotted, color=gray] (-1.1,-2.1) grid (3.1,4.1);
   \draw[->] (0,0) -- (2,0) node[right] {$x$};
   \draw[->] (0,0) -- (0,4) node[above] {$g(\omega, x)$};
   \draw[color=red] plot (\x, {(\x-1)^2})  node[right] {$\omega = 1$};
   \draw[color=blue] plot (\x, {(\x)^2})  node[right] {$\omega = 0$};
\end{tikzpicture}
This example is for a quadratic loss: \(g(\omega, x) = (\omega - x)^2\).
\end{frame}

\begin{frame}[label={sec:org7b46877}]{Example: Estimation}
\begin{itemize}
\item \(\param\): \alert{parameter} (random)
\item \(\hat{\param}\): \alert{estimate} (our action)
\item \((\param - \hat{\param})^2\): \alert{cost} function
\end{itemize}
\pause
\begin{block}{Mean-squared error minimiser}
If we want to guess \(\hat{\param}\), and we knew that \(\param \sim P\), then the guess
\[
\hat{\param} = \E_P(\param) = \argmin_{\hat{\param}} \E_P [(\param - \hat{\param})^2]
\]
minimises the squared error. 
\pause
This is because
\begin{align}
\frac{d}{d \hat{\theta}}
 \E_P [(\param - \hat{\param})^2]
&=
\frac{d}{d \hat{\theta}}
 \sum_\omega [\theta(\omega) -  \hat{\param}]^2 P(\omega)\\
&=
 \sum_\omega \frac{d}{d \hat{\theta}}
 [\theta(\omega) -  \hat{\param}]^2 P(\omega)\\
&=
 \sum_\omega 2 [\theta(\omega) -  \hat{\param}] (-1) P(\omega)
&=
	 2 (\hat{\param} - \E_P [\theta]).
\end{align}
Setting this to \(0\) gives \(\hat{\param} =\E_P [\theta]\)
\end{block}
\end{frame}

\section{Gradient methods}
\label{sec:orgdae3fa2}
\subsection{Gradients for optimisation}
\label{sec:orgeb1341d}
\begin{frame}[label={sec:org5f36401}]{The gradient descent method: one dimension}
\begin{itemize}
\item Function to minimise \(f : \Reals \to \Reals\).
\item Derivative \(\frac{d}{d \param} f(\param)\)
\end{itemize}
\pause
\begin{block}{Gradient descent algorithm}
\begin{itemize}
\item Input: initial value \(\param^0\), \alert{learning rate} schedule \(\alpha_t\)
\item For \(t=1, \ldots, T\)
\begin{itemize}
\item \(\param^{t+1} = \param^t - \alpha_t \frac{d}{d \param} f(\param^t)\)
\end{itemize}
\item Return \(\param^T\)
\end{itemize}
\pause
\end{block}
\begin{block}{Properties}
\begin{itemize}
\item If \(\sum_t \alpha_t = \infty\) and \(\sum_t \alpha_t^2 < \infty\), it finds a local minimum \(\param^T\), i.e. there is \(\epsilon > 0\) so that
\end{itemize}
\[
f(\param^T) < f(\param), \forall \param: \|\param^T - \param\| < \epsilon.
\]
\end{block}
\end{frame}
\begin{frame}[label={sec:org4dfab48}]{Gradient methods for expected value}
\begin{block}{Estimate the expected value}
\(x_t \sim P\) with \(\E_P[x_t] = \mu\).
\pause
\end{block}
\begin{block}{Objective: mean squared error}
Here \(\ell(x, \param) = (x - \param)^2\).
\[
\min_\param \E_P[(x_t - \param)^2].
\]
\pause
\end{block}
\begin{block}{Exact derivative update}
If we know \(P\), then we can calculate
\begin{align}
\param^{t+1} &= \param^t - \alpha_t \frac{d}{d\param} \E_P[(x - \param^t)^2]\\
\frac{d}{d\param} \E_P[(x - \param^t)^2] &= 2 (\E_P[x] - \param^t)
\end{align}
\end{block}
\end{frame}

\begin{frame}[label={sec:orge45363a}]{Stochastic derivative}
\begin{itemize}
\item Function to minimise \(f : \Reals \to \Reals\).
\item Derivative \(\frac{d}{d \param} f(\param)\)
\item \(f(\param) = \E[g | \param]\)
\item \(\frac{d}{d \param} f = \E[ \frac{d}{d \param} g | \param]\)
\end{itemize}
\pause
\begin{block}{Stochastic derivative algorithm}
\begin{itemize}
\item Input: initial value \(\param^0\), \alert{learning rate} schedule \(\alpha_t\)
\item For \(t=1, \ldots, T\)
\begin{itemize}
\item Observe \(g(\omega_t, \param^t\)), where \(\omega_t \sim P\).
\item \(\param^{t+1} = \param^t - \alpha_t \frac{d}{d \param} g(\omega_t, \param^t)\)
\end{itemize}
\item Return \(\param^T\)
\end{itemize}
\end{block}
\end{frame}


\begin{frame}[label={sec:org2efaf6c}]{Stochastic gradient for mean estimation}
\begin{theorem}[Sampling]
For any bounded random variable \(f\), 
\[
\E_P[f] = \int_{X} dP(x) f(x)
 = 
\lim_{T \to \infty} \frac{1}{T} \sum_{t=1}^T f(x_t)
 = 
\E_P \left[\frac{1}{T} \sum_{t=1}^T f(x_t)\right]
, \qquad x_t \sim P
\]
\end{theorem}
\begin{example}[Derivative ampling]
We can also approximate the gradient through sampling:
\begin{align*}
 \frac{d}{d\param} \E_P [(x - \param)^2] 
&= \int_{-\infty}^\infty \!\!\!\! dP(x) \frac{d}{d\param} (x - \param)^2
\\
&\approx \frac{1}{T} \sum_{t=1}^T \frac{d}{d\param} (x_t - \param)^2
= \frac{1}{T} \sum_{t=1}^T 2(x_t - \param)
\end{align*}
\pause
\begin{itemize}
\item Wen can even update \(\param\) after \alert{each sample} \(x_t\):
\end{itemize}
\[
\param^{t+1} = \param^t + 2 \alpha_t (x_t - \param^t)
\]
\end{example}
\end{frame}
\begin{frame}[label={sec:org9a24200}]{The gradient method}
\begin{itemize}
\item Function to minimise \(f : \Reals^{\alert{n}} \to \Reals\).
\item \alert{Gradient} \(\nabla_\param f(\param)  = \left(\frac{\partial f(\param)}{\partial \param_1}, \ldots, \frac{\partial f(\param)}{\partial \param_n}\right)\),
\item \alert{Partial} derivative \(\frac{\partial f}{\partial \param_n}\)
\end{itemize}
\pause
\begin{block}{Gradient descent algorithm}
\begin{itemize}
\item Input: initial value \(\param^0\), learning rate schedule \(\alpha_t\)
\item For \(t=1, \ldots, T\)
\begin{itemize}
\item \(\param^{t+1} = \param^t - \alpha_t \nabla_\param f(\param^t)\)
\end{itemize}
\item Return \(\param^T\)
\end{itemize}
\pause
\end{block}
\begin{block}{Properties}
\begin{itemize}
\item If \(\sum_t \alpha_t = \infty\) and \(\sum_t \alpha_t^2 < \infty\), it finds a local minimum \(\param^T\), i.e. there is \(\epsilon > 0\) so that
\end{itemize}
\[
f(\param^T) < f(\param), \forall \param: \|\param^T - \param\| < \epsilon.
\]
\end{block}
\end{frame}

\begin{frame}[label={sec:org5b8b4db}]{When the cost is an expectation}
In machine learning, we sometimes want to minimise the \alert{expectation} of a \alert{cost} \(\ell\), 
\[
f(\param) \defn \E[\ell | \param] = \int_\Omega dP(\omega) \ell(\omega, \param)
\]
This can be appro\omegaimated with a sample
\[
f(\param) \approx \frac{1}{T} \sum_t \ell(\omega_t, \param)
\]
The same holds for the gradient:
\[
\nabla_\param f(\param) = \int_\OMEGA dP(\omega) \nabla_\param \ell(\omega, \param)
\approx \frac{1}{T} \sum_t \nabla_\param \ell(\omega_t, \param)
\]
\end{frame}




\begin{frame}[label={sec:org3aa7cfc}]{Stochastic gradient method}
\begin{itemize}
\item Function to \alert{minimise} \(f : \Reals^n \to \Reals\).
\item \alert{Gradient} \(\nabla f(\param)\)
\item \(f(\param) = \E[\ell | \param]\)
\item \(\nabla_\param f = \E[ \nabla_\param \ell | \param]\)
\end{itemize}

\pause
\begin{block}{Algorithm}
\begin{itemize}
\item Input: initial value \(\param^0\), \alert{learning rate} schedule \(\alpha_t\)
\item For \(t=1, \ldots, T\)
\begin{itemize}
\item Observe \(\ell(\omega_t, \param^t\)), where \(\omega_t \sim P\).
\item \(\param^{t+1} = \param^t - \alpha_t \nabla_\param g(\omega_t, \param^t)\)
\end{itemize}
\item Return \(\param^T\)
\end{itemize}
\end{block}

\begin{block}{Alternative view: Noisy gradients}
\begin{itemize}
\item \(\param^{t+1} = \param^t - \alpha_t [\nabla_\param f(\param^t) + \epsilon_t]\)
\item \(\E[\epsilon_t] = 0\) is sufficient for convergence.
\end{itemize}
\pause
\end{block}
\end{frame}
\end{document}