#+TITLE: Artificial Intelligence
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \include{preamble}
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
#+latex_header: \AtBeginSection[]{\begin{frame}<beamer>\tableofcontents[hideothersubsections]\end{frame}}

* Introduction


** What is Artificial Intelligence?
*** A go grandmaster
[[./figures/FloorGoban.JPG]]
*** A farmer
[[./figures/farming.jpg]]
*** A computer
[[./figures/computer.jpg]]
*** A primate
[[./figures/ape-tool.jpg]]
*** A scientist
[[./figures/Curie.jpg]]

*** What makes something intelligent?
    #+BEAMER: \pause
**** Thoughts
#+ATTR_BEAMER: :overlay <+->
- We cannot know if somebody is making intelligent thoughts.
- Thoughts can only be *inferred* through *interaction*.
**** Behaviour
#+ATTR_BEAMER: :overlay <+->
- Fixed: Behaves the same in the same situtation.
- Adaptive: Learns from previous experiences.
#+BEAMER: \pause
**** Why do we need artificial intelligence?
     #+ATTR_BEAMER: :overlay <+->
- To do something a human can do (better? worse? cheaper?)
- To do something a human cannot do?
- To help humans do something more efficiently?

*** Decisions and learning
**** Algorithms as thoughts
- Incorporating new information
- Deciding what to do.

**** Learning: Using information to update knowledge.
- Situational awareness: what is going on right now?
- Epistemic knowledge: How does the world work.
**** Decision making and interaction
- Recommend a route.
- Buy and sell stocks.
- Make a move in chess.
*** To the moon!
Goal: See if humans can live on the moon.
#+BEAMER: \pause
**** Learning
- Orbital mechanics: What are the equations of motion?
- Trajectory following: Where is the spaceship right now?
#+BEAMER: \pause
**** Decisions
- How much should we spend?
- How much payload/fuel/time?
- What is a good trajectory for a given payload/fuel?
- How can the spaceship follow the trajectory?
- How can the spaceship land on the moon?
*** The view from statistics
#+BEAMER: \pause
**** Optimal decisions
- Problem-dependent
- Require defining a cost- or utility function
- The optimal solution has the lowest cost or maximal utility
#+BEAMER: \pause
**** Optimisation: algorithms for optimal solutions
- Discrete optimisation.
- Linear optimisation.
- Non-linear optimisation.
#+BEAMER: \pause
**** Statistics and Machine Learning
- How to learn from data and interactions.
- Summarising knowledge into a model.
- Using the knowledge to make decisions.

** About the course  
*** Schedule
1. Agents and the environment (1 week)
2. Search problems (2 weeks)
3. Dynamic programming (1 week)
4. Logic and constraints (1 week)
5. Uncertainty and probability (2 weeks)
6. Expected utility (1 week)
7. Markov decision processes (2 weeks)
8. Introduction to game theory (1 week)
9. Belief networks (1 week)
10. Social issues in AI (1 week)


*** Philosophy
The philosophy of this course is as follows: 
- We give example problems.
- We use theory to explain and generalise from those examples to general problems.
- We describe algorithms to /solve/ general problems.
- We implement algorithms to solve the specific examples.

In general, the course will start from the simplest problems and
slowly progress to the more complex ones.

*** How-to

**** Lectures
- All the relevant course content is given in-class.
- The slides only contain a summary.
- More details in the reference books.
**** Assignments: (40% of grade)
- Assignments are obligatory
- In English or French.
- Can be individual or group.
- You *must cite* sources, whether it be books, web pages, other students, or LLMs, e.g.
**** Written exam (60% of grade)
- No references allowed in the exam.
- Important concepts will be repeated/explained in the exam questions.

* Intelligent agents

*** The roots of AI
**** Mathematics: Idealise rational thought
- Logic
- Statistics
**** Economics and Game Theory: Formalise human behaviour
- Utility and Risk
- Rationality and Social Choice
**** Cognitive Sciences: Understand human behaviour
- Neuroscience
- Behavioural psychology
**** Computer Science: Building autonomous agents
- Artificial Intelligence
- Machine Learning

** The agent-environnment interaction
*** The agent
#+ATTR_LATEX: :width 0.7\textwidth
[[./figures/rat_maze.pdf]]
- Agents act in an environment
- Agents' beliefs and desires guide their behaviour.
*** Agent environment interface
#+ATTR_LATEX: :width \textwidth
[[./figures/environment_interaction.pdf]]

*** Variables
We interact with the environment at times $t = 1, 2, \ldots$.
**** Agent
- $\bel_t$: belief, owned by the agent.
- $a_t$: actions *generated* by the agent.
- $\pol$: policy (algorithm) for selecting the next action $a_{t+1}$ somehow.
**** Environment
- $s_t$: state, owned by the environment. 
- $x_t$: observations *generated* by the environment.
- The next state $s_{t+1}$ depends only *only on $s_t, a_t$*.

**** Interaction
- The agent reacts to the observations $x_t$ by choosing an action $a_t$.
- The environment reacts to the action $a_t$ with a new state $s_{t+1}$ and an observation $x_{t+1}$

** Examples
*** Mazes
[[./figures/labyrinth.jpg]]
*** Algebraic manipulation
[[./figures/rubik.png]]
*** Chess game
[[./figures/chess.jpg]]
*** Poker game
[[./figures/poker.jpg]]
*** Motor Racing
[[./figures/racing.jpg]]
*** Navigation assistant
[[./figures/planner.png]]
*** Space exploration
[[./figures/mars_rover.jpg]]
** Environments
*** Environment components
We generally consider dynamic environments, so at time $t$:
- $s_t$: state of the environment
- $x_t$: observation of the environment by the agent
- $a_t$: actions taken by the agent
**** Mazes :B_exampleblock:
     :PROPERTIES:
     :BEAMER_env: exampleblock
     :END:
- $s_t$: the location of the agent in the maze
- $x_t$: What the agent observes (exact location, or just surroundings?)
- $a_t$: Direction in which the agent moves

** Policies
*** Policies
- Policies determine the behaviour of the agent.
- They define what the agent does at any given time.
**** Reactive policies
We allow agents to randomise. The simplest agent choose actions only depending on the current observation:
\[
\pi(a_t | x_t) \tag{the probability with which the agent takes action $a_t$}
\]
**** Deterministic policies
A special cases when, for each $x_t$ the same action $f(x_t)$ is always taken, so that $\pi(a_t = f(x_t) | x_t) = 1$. Then we might prefer to write
\[
a_t = \pi(x_t)
\]
**** Adaptive policies
The action taken may depend on what the agent observed in the past:
\[
\pi(a_t \mid x_t, a_{t-1}, x_{t-1}, \ldots, a_1, x_1)
\]
*** Example policies
**** Reactive maze policy
- Ordered actions $A = \{\textrm{Up}, \textrm{Right}, \textrm{Down}, \textrm{Left}\}$
- Take action $a_{t+1} = a_t$ unless there is a wall in front.
- If there is a wall, take the next action, $a_{t+1} = a_t + 1$. (where + cycles over the 4 actions)

**** Problems with this policy
- Can it solve any maze?
- Why yes/no?
- What can we do to make sure that the agent visits every point of the maze?

** Agent structures
*** Example: Planning a trip
There are three train routes from Neuchatel to Luzerne 
- Neuchatel 6:58-IC-7:57 Olten 8:07-RE-8:55 Luzern (18 CHF)
- Neuchatel 7:01-S-7:52 Bern  8:00-IR-9:01 Luzern (22 CHF)
- Neuchatel 7:26-IC-8:18 Olten 8:30-IR-9:05 Luzern (26 CHF)
**** Criteria for choosing
- Price
- Crowdedness
- Length of time
**** Planning the trip
- Is "go through Bern" enough of a plan?
- What about delays or cancellations?
**** Multiple levels of actions 
- Which route to use, and fallbacks.
- What to pack
- How to get to the station
- Putting one foot in front of the other.
*** Learning and memory

**** Belief state
- Memory
- A summary of the agent's knowledge
- The state in a state machine
- The contents of the tape and read/write heads on a Turing machien.
**** Belief transitions
- A (possibly randomised) function $f : B \times A \times X \to B$ 
\[
\bel_{t+1} = f(\bel_t, a_t, x_t)
\]
- $\bel_t \in S$: Belief at time $t$.
- $a_t \in A$: Action at time $t$
- $x_t \in X$: Observation at time $t$.
- $f$ is implemented by the agent's algorithm
*** Memory in the maze example
**** Known maze, known location
- Agent observes everything.
- No memory required.
**** Known maze, unknown co-ordinates
- Agent only observes immediate surroundings.
- Memory keeps track of location.
**** Unknown maze, known co-ordinates
- Agent only observes immediate surroundings.
- Memory keeps track of maze layout
- Agent always knows its exact co-ordinates.
**** Unknown maze and co-ordinates
- Agent only observes immediate surroundings.
- Memory keeps track of maze layout and co-ordinates





*** Hierarchical control and the mind-body interface
**** What is the "agent" in a Mars Rover?
- The Rover itself?
- The CPU module controlling the rovers motors?
- The software that is running on the CPU?

**** High-level planner
Selecting plans for the low-level controller
**** Low-level controller
Selects actions for each plan selected by the high-level planner

* Designing agents
** Goals
*** Goals as a design principle
- Easy to define
- Can be too vague.
**** Example: mazes
- Assign "goal" to a maze location
- The agent should find the way to the goal.
**** Example: exams
- Goal: pass the exam
- The agent should find a strategy so that it passes the exam!

** Utilities
*** Utility as a design principle
- Hard to define.
- Can be too specific.
**** Example: mazes
- Prefer shortest path to longer ones to the goal.
- More complicated if we have intermediate goals.
**** Example: exams
- Prefer higher grades than lower grades.
- Prefer to study less than more

** Elementary Decision Theory
*** Preferences
**** Types of rewards                                               :example:
- For e.g. a student: Tickets to concerts.
- For e.g. an investor: A basket of stocks, bonds and currency.
- For everybody: Money.

**** Preferences among rewards
For any rewards $x, y \in R$, we either
- (a) Prefer $x$ at least as much as $y$ and write $x \preceq^* y$.
- (b) Prefer $x$ not more than $y$ and write $x \succeq^* y$.
- (c) Prefer $x$ about the same as $y$ and write $x \eqsim^* y$.
- (d) Similarly define $\succ^*$ and $\prec^*$
  
*** Utility and Cost
**** Utility function
To make it easy, assign a utility $U(x)$ to every reward through a
utility function $U : R \to \Reals$.

**** Utility-derived preferences
We prefer items with higher utility, i.e.
- (a) $U(x) \geq U(y)$ $\Leftrightarrow$ $x \succeq^* y$
- (b) $U(x) \leq U(y)$ $\Leftrightarrow$ $y \succeq^* x$

**** Cost
     It is sometimes more convenient to define a cost function $C: R \to \Reals$ so that we prefer items with lower cost, i.e.
- $C(x) \geq C(y)$ $\Leftrightarrow$ $y \succeq^* x$

**** Decision making as an optimisation problem
How can we find the decision maximising utility / minimising cost?
*** Additive utility functions
**** Shortest path
If $s_t$ describes our location in a maze, each path is a sequence $s_1, \ldots, s_T$. Then we can have
\[
U(s_1, \ldots, s_T) = \sum_t \rho(s_t).
\]
with $\rho(s_t)$ being a *reward* function.

If our goal is to reach the goal state $s^*$ as quickly as possible, we can use
\[
\rho(s) = \begin{cases}-1 & s = s^*\\ 0 & s = s^*\end{cases}
\]

*** Choice of the utility function
**** Designer input
- The AI designer selects the utility (or goals)
- The choice is not always obvious!
**** The *value-alignment* problem
- The designer selects a utility they *think* is the best choice
- However, their choice results in unintended behaviour
- Example: Autonomous vehicles
  
**** The value-alignment in *populations*
- Not everybody wants the same thing.
- We need to design *fair* policies.

*** Multi-agent problems

**** External agents
- Have their own utility/goals
- Are partly rational

**** Designed agents
- We can choose their utility/goals
- Computation/Optimality trade-off


** Discussion
*** Goals versus preferences
**** Maze-solving
- How should we define the utility/cost of every path?
- Is an additive utility sufficient?

**** Exam taking
- What if you say you want to perform super-well in the exam?
- How can set up the decision problem of how to study for the exam?



** Exercises and assignments
*** Exercises (From AI3e, 2.7)
- 1. Representations
- 2. Top-level controller.
- 3. Obstacle avoidance.
- 4. Robot trap.
- 10. Autonomous cars: driver preferences
*** Assignments (From AI3e, 2.7)
- 5. Moving targets
- 7. Sensing
- 8. Batteries
- 9. Which functions?
- 11. Autonomous cars: state of the art.
  
  


