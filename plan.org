#+TITLE: Artificial Intelligence
#+AUTHOR: Christos Dimitrakakis
#+EMAIL:christos.dimitrakakis@unine.ch
#+LaTeX_HEADER: \usepackage{algorithm,algorithmic}
#+LaTeX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage{amsmath}
#+LaTeX_HEADER: \usepackage{amssymb}
#+LaTeX_HEADER: \usepackage{isomath}
#+LaTeX_HEADER: \newcommand \E {\mathop{\mbox{\ensuremath{\mathbb{E}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Var {\mathop{\mbox{\ensuremath{\mathbb{V}}}}\nolimits}
#+LaTeX_HEADER: \newcommand \Bias {\mathop{\mbox{\ensuremath{\mathbb{B}}}}\nolimits}
#+LaTeX_HEADER: \newcommand\ind[1]{\mathop{\mbox{\ensuremath{\mathbb{I}}}}\left\{#1\right\}}
#+LaTeX_HEADER: \renewcommand \Pr {\mathop{\mbox{\ensuremath{\mathbb{P}}}}\nolimits}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LaTeX_HEADER: \DeclareMathOperator*{\sgn}{sgn}
#+LaTeX_HEADER: \newcommand \defn {\mathrel{\triangleq}}
#+LaTeX_HEADER: \newcommand \Reals {\mathbb{R}}
#+LaTeX_HEADER: \newcommand \Param {\Theta}
#+LaTeX_HEADER: \newcommand \param {\theta}
#+LaTeX_HEADER: \newcommand \vparam {\vectorsym{\theta}}
#+LaTeX_HEADER: \newcommand \mparam {\matrixsym{\Theta}}
#+LaTeX_HEADER: \newcommand \bW {\matrixsym{W}}
#+LaTeX_HEADER: \newcommand \bw {\vectorsym{w}}
#+LaTeX_HEADER: \newcommand \wi {\vectorsym{w}_i}
#+LaTeX_HEADER: \newcommand \wij {w_{i,j}}
#+LaTeX_HEADER: \newcommand \bA {\matrixsym{A}}
#+LaTeX_HEADER: \newcommand \ai {\vectorsym{a}_i}
#+LaTeX_HEADER: \newcommand \aij {a_{i,j}}
#+LaTeX_HEADER: \newcommand \bx {\vectorsym{x}}
#+LaTeX_HEADER: \newcommand \cset[2] {\left\{#1 ~\middle|~ #2 \right\}}
#+LaTeX_HEADER: \newcommand \pol {\pi}
#+LaTeX_HEADER: \newcommand \Pols {\Pi}
#+LaTeX_HEADER: \newcommand \mdp {\mu}
#+LaTeX_HEADER: \newcommand \MDPs {\mathcal{M}}
#+LaTeX_HEADER: \newcommand \bel {\beta}
#+LaTeX_HEADER: \newcommand \Bels {\mathcal{B}}
#+LaTeX_HEADER: \newcommand \Unif {\textrm{Unif}}
#+LaTeX_HEADER: \newcommand \Ber {\textrm{Bernoulli}}
#+LaTeX_HEADER: \newcommand \Mult {\textrm{Mult}}
#+LaTeX_HEADER: \newcommand \Beta {\textrm{Beta}}
#+LaTeX_HEADER: \newcommand \Dir {\textrm{Dir}}
#+LaTeX_HEADER: \newcommand \Normal {\textrm{Normal}}
#+LaTeX_HEADER: \newcommand \Simplex {\mathbb{\Delta}}
#+LaTeX_HEADER: \newcommand \pn {\param^{(n)}}
#+LaTeX_HEADER: \newcommand \pnn {\param^{(n+1)}}
#+LaTeX_HEADER: \newcommand \pnp {\param^{(n-1)}}
#+LaTeX_HEADER: \usetikzlibrary{shapes.geometric}
#+LaTeX_HEADER: \tikzstyle{utility}=[diamond,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=8mm]
#+LaTeX_HEADER: \tikzstyle{select}=[rectangle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_HEADER: \tikzstyle{hidden}=[dashed,draw=black,fill=red!10]
#+LaTeX_HEADER: \tikzstyle{RV}=[circle,draw=black,draw=blue!50,fill=blue!10,inner sep=0mm, minimum size=6mm]
#+LaTeX_CLASS_OPTIONS: [smaller]
#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+TAGS: activity advanced definition exercise homework project example theory code
#+OPTIONS:   H:3
* Introduction

** About the course  
*** Aims
This course will focus on algorithms and models for Artificial
Intelligence.  We will concentrate mainly on the decision making,
rather than the learning, side of artificial intelligence. Learning is
already addressed in statistics courses, as well as the machine
learning course in the final year.

*** Philosophy
The philosophy of this course is as follows: 
- We give example problems.
- We use theory to explain and generalise from those examples to general problems.
- We describe algorithms to /solve/ general problems.
- We implement algorithms to solve the specific examples.

In general, the course will start from the simplest problems and
slowly progress to the more complex ones.

*** How to use this notebook
- This notebook contains a summary of all the foundational material in the course. It is not sufficient for studying for the exam, or as a reference material.
- For details, use the links to external resources.
- There are some more detailed slides available for each lecture.
*** Books and schedule
Artificial Intelligence: Foundations of Computational Agents, 3rd Edition
Artificial Intelligence: a Modern Approach, 4th Edition

|--------+-----------------------------+---------------------------------+------------------------------------|
| Module | Topics                      | AI:FoCA                         | AI: aMA                            |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      1 | - Preferences               | 1. AI and Agents                | 2.1. Agents and environments       |
|        | - Utility                   | 1.2. Complexity                 | 2.2 Rationality                    |
|        | - States                    | 1.3. Application domains        | 2.3. Environments                  |
|        | - Actions                   | 1.4. Knowledge representation   | 2.4. Agents                        |
|        | - Beliefs                   | 2. Architecture                 | 2.4.1 Programs                     |
|        | - Fairness                  | 2.1. Control                    | 2.4.2-3 Reflex agents              |
|        |                             | 2.2. Hierarchical control       | 2.4.4. Goals                       |
|        |                             | 2.3. Moral machines             | 2.4.5. Utility                     |
|        |                             |                                 | 2.4.6. Learning                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      2 | Depth-First Search          | 3. Search                       | 3.1 Problem-solving                |
|        | Breadth-First Search        | 3.1. Search in graphs           | 3.2. Examples.                     |
|        |                             |                                 | 3.3. Best-First search             |
|        |                             |                                 | 3.4.1. Breadth-first               |
|        |                             |                                 | 3.4.3. Depth-first search          |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      3 | Heuristic Search            | 3.2. Uninformed search          | 3.5.2. A*                          |
|        | A* Search                   | 3.3. Heuristic search           | 3.6. Heuristic Functions           |
|        |                             |                                 |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      4 | Dynamic Programming         |                                 | 3.4.2 Dijkstra                     |
|        |                             | 3.4. Dynamic programming        |                                    |
|        | Branch and bound            | 3.5. Branch and bound           |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      5 | Constraint programming      | 4. Reasoning with constraints   | 6. CSP                             |
|        | Logical reasoning           | 4.1. Variables and Constraints  | 7. Logical Agents                  |
|        |                             | 4.2. CSPs and Search            |                                    |
|        |                             | 4.6. Local Search               |                                    |
|        | Deterministic planning      | 4.8. Optimization               |                                    |
|        |                             |                                 |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      6 | Uncertainty                 | 9. Reasoning with Uncertainty   | 12.1. Acting under uncertaint      |
|        | Aleatory/Epistemic          | 9.1. Probability                | 12.2. Basic probability notation   |
|        | Probability Theory          | 9.2. Independence               | 12.3. Inference                    |
|        | Bayes Theorem               |                                 | 12.4. Independence                 |
|        | Probabilistic inference     |                                 | 12.5. Bayes's theorem              |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      7 | Belief networks             | 9.3. Belief Networks            | 13.1. Representing knowledge       |
|        |                             | 9.4. Probabilistic Inference    | 13.2. Bayesian Networks            |
|        |                             |                                 | 13.3. Exact inference in BNs       |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      8 | Expected Utility Theory     | 12.1 Preferences and Utility    | 16.1. Beliefs and Desires          |
|        |                             | 12.2 One-off decisions          | 16.2. utility theory               |
|        |                             |                                 | 16.3. Utility functions            |
|        |                             |                                 | 16.5. Decision networks            |
|--------+-----------------------------+---------------------------------+------------------------------------|
|      9 | Markov Decision Processes   | 12.3 Sequential Decisions       | 17.1. Sequential decision problems |
|        | Dynamic Programming         | 12.4 The value of information   | 17.2. Algorithms for MDPs          |
|        |                             | 12.5 Decision processes         |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|     10 | Alternating Zero-Sum Games  | 14.1. Multi-agent framework     | 5.1. Game Theory                   |
|        | Stochastic Zero-Sum Games   | 14.2. Representations of games  | 5.2. Zero-Sum Games                |
|        | Linear programming          | 14.3. Perfect information games | 5.3. Alpha-Beta Search             |
|        |                             |                                 | 5.5. Stochastic Games              |
|--------+-----------------------------+---------------------------------+------------------------------------|
|     11 | Supervised learning         |                                 |                                    |
|        | Learning as inference       |                                 |                                    |
|        | Learning as optimisation    |                                 |                                    |
|        | Stochastic gradient descent |                                 |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|
|     12 | Reinforcement learning      |                                 |                                    |
|        | Bandit problems             |                                 |                                    |
|        | Q-learning                  |                                 |                                    |
|        | Stochastic approximation    |                                 |                                    |
|--------+-----------------------------+---------------------------------+------------------------------------|




** Schedule

| 2.22 | 1-2 Introduction                                        |
| 2.29 | 3.1-3.5 Search, State Spaces, Graphs, Uniformed Search. |
| 3.07 | 3.6. Informed Search, Heuristics, A*                    |
| 3.14 | 4.1-4.2, 4.8 Constrained Search                         |
| 3.21 | Exercises and Project Day                               |
| 3.28 | 9.1 Probability, Independence, Belief Networks          |
| 4.11 | 12.1. Preferences and Utility. Probability              |
| 4.18 | Statistical estimation and supervised learning          |
| 4.25 | Markov decision processes                               |
| 5.02 | Markov games                                            |
| 5.16 | Simultaneous Games                                      |
| 5.23 | Reinforcement Learning                                  |
| 5.30 | Project presentations                                   |


* single agent problems with certainty
** Uninformed search
| Graph definitions         | 10 |
| Tree example              |  5 |
| Shortcut example          |  5 |
| Depth-first search        | 10 |
| The shortest path problem | 10 |
| Goals and DFS             | 10 |
| Shortest-path DFS         | 10 |
| Breadth-first search      | 10 |
| Iterative deepening       | 10 |
| Uniform cost search       | 10 |
|---------------------------+----|
|                           | 95 |
#+TBLFM: @11$2=vsum(@1..@10)

** Informed search
*** Heuristics
*** $A^*$-search

*** Dynamic programming

*** Branch and Bound


** Logic 
*** Logic
**** Statements
- A statement $A$ may be true or false

**** Unary operators
- negation: $\neg A$ is true if $A$ is false (and vice-versa).

**** Binary operators
- or: $A \vee B$ ($A$ or $B$) is true if either $A$ or $B$ are true.
- and: $A \wedge B$ is true if both $A$ and $B$ are true.
- implies: $A \Rightarrow B$: is false if $A$ is true and $B$ is false.
- iff: $A \Leftrightarrow B$: is true if $A,B$ have equal truth values.

**** Operator precedence
$\neg, \wedge, \vee, \Rightarrow, \Leftrightarrow$

*** Set theory
- First, consider some universal set $\Omega$.
- A set $A$ is a collection of points $x$ in $\Omega$.
- $\{x \in \Omega : f(x)\}$: the set of points in $\Omega$ with the property that $f(x)$ is true.

**** Unary operators
- $\neg A =  \{x \in \Omega : x \notin A\}$.
**** Binary operators
- $A \cup B$ if $\{x \in \Omega : x \in A \vee x \in B\}$ - (c.f. $A \vee B$)
- $A \cap B$ if $\{x \in \Omega : x \in A \wedge x \in B\}$ - (c.f. $A \wedge B$)
**** Binary relations
- $A \subset B$ if $x \in A \Rightarrow x \in B$ - (c.f. $A \implies B$)
- $A = B$ if $x \in A \Leftrightarrow x \in B$ - (c.f. $A \Leftrightarrow B$)

*** Knowledge base
**** Syntax and Semtantics
- Syntax: How to construct sentences
- Semantix: What sentences mean
**** Truth
- A statement $A$ is either true or false in any model $m$.
**** Model
- $M(A)$ the set of all models where $A$ is true.
**** Entailment
- $A \models B$ means that $B$ is true whenever $A$ is true.
- $A \models B$ if and only if $M(A) \subseteq M(B)$.
**** Knowledge-Base
- A set of sentences that are true.
**** Inference
- $KB \vdash_i A$: Algorithm $i$ can derive $A$ from KB.
*** Propositional logic syntax
-Sentence $\to$ Atomic | Complex
-Atomic \to True | False | A | B | C | \ldots
-Complex \to (Sentence) | [Sentence]
- | $\neg$  Sentence (not)
- | Sentence $\wedge$ Sentence (and)
- | Sentence $\vee$ Sentence (or)
- | Sentence $\Rightarrow$ Sentence (implies)
- | Sentence $\Leftrightarrow$ Sentence (if and only if)

Precedence: $\neg, \wedge, \vee, \Rightarrow, \Leftrightarrow$

*** Set theory semantics of propositional logic
**** Atoms as sets 
- Let $\Omega$ be the universal set.
- Any atom $A$ is a subset of $\Omega$.
- Any model $\omega$ is an element of $\Omega$.
**** Definitions
- $A \Rightarrow B$ is equivalent to $A \supset B$.
- $\neg (\neg A) \equiv A$
- $(A \Rightarrow B) \equiv (\neg B \Rightarrow \neg A)$
- $(A \Rightarrow B) \equiv (\neg A \vee B)$

**** For any model $m$:
- $\neg P$ is true iff $P$ is false in $m$.
- $P \wedge Q$ is true iff $P, Q$ are true in $m$.
- $P \vee Q$ is true iff either $P$ or $Q$ is true in $m$.
- $P \Rightarrow Q$ is true unless $P$ is true and $Q$ is false in $m$.
- $P \Leftrightarrow Q$ if $P,Q$ are both true or both false in $m$.


- If $A \subset B$ then, for every $\omega \in A$,  $\omega \in B$.
- If $\omega \in A \cap B$ then $\omega \in A$.

** Deterministic planning
*** [[https://artint.info/3e/html/ArtInt3e.Ch6.S1.html][States, actions and goals]]
- States $s \in S$
- Actions $a \in A$
- Transition function $\tau : S \times A \to S$
**** STRIPS represnetation
- State: $S \subset \{0,1\}^n$ of $n$ atoms.
- Precondition: $c(s, a) = 1$ if $a$ can be performed in $s$.
- Effect: Assigns values to *some* atoms.
**** Feature-based
- Effect: Transition function $\tau_i: S \times A \to \{0,1\}$ for each $i \in [n]$.

** Infinite choices
*** Lipschitz search
If we know the function $f$ is Lipschitz-smooth, i.e.
\[
\exists L > 0 : |f(x) - f(y)| \leq L |x  - y|,
\]
then we also know that for any point $z$:
\[
f(z) < f(x) + L |x - z|,
\qquad
f(z) < f(y) + L |y - z|
\]
**** Schubert's Algorithm [[https://www.jstor.org/stable/2156138][(Schubert, 1972)]]
\begin{algorithmic}
\STATE \textbf{Input:} $L > 0$, $X$, $x_0 \in X$.
\FOR {$t=1, \ldots, T$}
\STATE $x_{t} = \argmax_{x \in X} \min \cset{f(x_k) + L|x_k - x|}{k=0, \ldots, t-1}$
\ENDFOR
\end{algorithmic}
**** Discussion
- This is guaranteed to *converge* to the optimal solution.
- If $L$ is *unknown*, DIRECT [[http://www.planchet.net/EXT/ISFA/1226.nsf/769998e0a65ea348c1257052003eb94f/f9ca730ca27def69c12576d8002ed895/$FILE/Jones93.pdf][(Jones et al. 1993)]] can be used.
- If $f$ is noisy, the problem becomes a *continuum bandit* problem.
*** First-order gradient methods
- Gradient descent
- Stochastic gradient descent
**** Properties  
- Incremental algorithms
- Can converge to a *local* optimum

*** Single-variable gradient descent
**** Setting
- Input: $f : \Reals \to \Reals$
- Problem: $\max_x f(x)$
- Derivative: $\frac{d}{dx} f(x) \defn \lim_{\Delta \to 0} \frac{f(x + \Delta)  - f(x)}{\Delta}$.
**** Algorithm
1) Input: $x^{(0)}$, f
2) For $t = 1, \ldots$:
3) Calculate direction $g_t = \frac{d}{dx} f(x_{t-1})$
4) Select step-size $\alpha_t$
5) Update $x^{(t)} = x^{(t-1)} + \alpha_t g_t$.
   
*** Multiple-variable gradient descent
**** Setting
- Input: $f : \Reals^d \to \Reals$, $x = (x_1, \ldots, x_d)$
- Problem: $\max_x f(x)$
- Partial Derivative: $\frac{\partial}{\partial x_i} f(x) \defn \lim_{\Delta \to 0} \frac{f(x_1, \ldots, x_i + \Delta, \ldots, x_d)  - f(x)}{\Delta}$.
- Gradient $\nabla_x f(x) = \left[\frac{\partial}{\partial x_1} f(x), \ldots, \frac{\partial}{\partial x_i} f(x), \ldots, \frac{\partial}{\partial x_d} f(x)\right]^\top$.
**** Algorithm
1) Input: $x_0$, f
2) For $t = 1, \ldots$:
3) Calculate direction $g_t = \nabla_x f(x_{t-1})$
4) Select step-size $\alpha_t$
5) Update $x_{t} = x_{t-1} + \alpha_t g_t$.
   
*** Stochastic gradient descent
**** As gradient descent with errors
- Calculate direction $g_t = \nabla_x f(x_{t-1}) + \epsilon_t$
- $\epsilon_t$ is typically zero-mean noise.
**** In learning from data
The gradient can be broken up into a sum of gradients:
\[
f(x) = \sum_t v(x, z_t),
\qquad
\nabla_x f(x) = \sum_t \nabla_x v(x, z_t),
\]
$x_t = x_{t-1} + \alpha_t \nabla_x v(x, z_t)$.
**** In Bayesian quadrature
The function is an expectation:
\[
f(x) = \int_Z v(x, z) p(z) dz.
\qquad
\nabla_x f(x) \approx \sum_t \nabla_x v(x, z_t), 
\]
where $z_t \sim p(z)$ are samples from $p$.

* single agent problems with uncertainty
** Probability
*** Probability fundamentals
**** Probability measure $P$
- Defined on a universe $\Omega$
- $P : \Sigma \to [0,1]$ is a function of subsets of $\Omega$.
- A subset $A \subset \Omega$ is an *event* and $P$ measures its likelihood.
**** Axioms of probability
- $P(\Omega) = 1$
- For $A, B \subset \Omega$, if $A \cap B = \emptyset$ then $P(A \cup B) = P(A) + P(B)$.
**** Marginalisation
If $A_1, \ldots, A_n \subset \Omega$ are a partition of $\Omega$
\[
P(B) = \sum_{i = 1}^n P(B \cap A_i).
\]
** Conditional probability and independence
*** Conditional probability
**** Conditional probability
    :PROPERTIES:
    :BEAMER_env: definition
    :END:
The conditional probability of an event $A$ given an event $B$ is defined as 
\[
P(A | B) \defn \frac{P(A \cap B)}{P(B)}
\]
The above definition requires $P(B)$ to exist and be positive.

**** Conditional probabilities as a collection of probabilities
More generally, we can define conditional probabilities as simply a
collection of probability distributions:
\[
\cset{P_\param(A)}{\theta \in \Param},
\]
where $\Param$ is an arbitrary set. 

*** The theorem of Bayes
**** Bayes's theorem
    :PROPERTIES:
    :BEAMER_env: theorem
    :END:
\[
P(A | B) = \frac{P(B | A)}{P(B)} 
\]
#+BEAMER: \pause

**** The general case
If $A_1, \ldots, A_n$ are a partition of $\Omega$, meaning that they
are mutually exclusive events (i.e. $A_i \cap A_j = \emptyset$ for $i
\neq j$) such that one of them must be true (i.e. $\bigcup_{i=1}^n A_i =
\Omega$), then
\[
P(B) = \sum_{i=1}^n P(B | A_i) P(A_i)
\]
and 
\[
P(A_j | B) = \frac{P(B | A_j)}{\sum_{i=1}^n P(B | A_i) P(A_i)}
\]

*** Independence
**** Independent events
$A, B$ are independent iff $P(A \cap B) = P(A) P(B)$.
**** Conditional independence
 $A, B$ are conditionally independent given $C$ iff $P(A \cap B | C) = P(A | C) P(B | C)$.
** Random variables and expectation 
*** Random variables
A random variable $f : \Omega \to \Reals$ is a real-value function measurable with respect to the underlying probability measure $P$, and we write $f \sim P$.
**** The distribution of $f$
The probability that $f$ lies in some subset $A \subset \Reals$ is
\[
P_f(A) \defn P(\{\omega \in \Omega : f(\omega) \in A\}).
\]
**** Independence
Two RVs $f,g$ are independent in the same way that events are independent:
\[
P(f \in A \wedge g \in B) = P(f \in A) P(g \in B) = P_f(A) P_g(B).
\]
In that sense, $f \sim P_f$ and $g \sim P_g$.

*** Expectation
For any real-valued random variable $f: \Omega \to \Reals$, the expectation with respect to a probability measure $P$ is
\[
\E_P(f) = \sum_{\omega \in \Omega} f(\omega) P(\omega).
\]
**** Linearity of expectations
For any RVs $x, y$:
\[
\E_P(x + y) = \E_P(x) + \E_P(y)
\]
**** Independence
If $x,y$ are independent RVs then $\E_P(xy) = \E(x)\E(y)$.
**** Correlation
If $x,y$ are *not* correlated then $\E_P(xy) = \E(x)\E(y)$.
**** IID (Independent and Identically Distributed) random variables
A sequence $x_t$ of r.v.s is IID if $x_t \sim P$
$(x_1, \ldots, x_t, \ldots, x_T) \sim P^T$.

*** Conditional expectation
The conditional expectation of a random variable $f: \Omega \to \Reals$, with respect to a probability measure $P$ conditioned on some event $B$ is simply
\[
\E_P(f | B) = \sum_{\omega \in \Omega} f(\omega) P(\omega | B).
\]

** Statistical Decision Theory
*** Expected utility
**** Actions, outcomes and utility
In this setting, we obtain random outcomes that depend on our actions.
- Actions $a \in A$
- Outcomes $\omega \in \Omega$.
- Probability of outcomes $P(\omega \mid a)$
- Utility $U : \Omega \to \Reals$
**** Expected utility
The expected utility of an action is:
\[
\E_P[U \mid a] = \sum_{\omega \in \Omega} U(\omega) P(\omega \mid a).
\]

**** The expected utility hypothesis
We prefer $a$ to $a'$ if and only if
\[
\E_P[U \mid a] \geq \E_P[U \mid a']
\]

** Supervised learning
*** Supervised learning

** Markov decision processes
*** Markov decision process
- Action space $A$.
- State space $S$.
- Transition kernel $s_{t+1} = j \mid s_t = s, a_t = a \sim P_\mdp(j \mid s, a)$.
- Reward $r_t = \rho(s_t, a_t)$ (can also be random).
- Utility
\[
U_t = \sum_{k=t}^T r_t.
\]
*** Value functions
**** The state value function
For any given MDP $\mdp$ and policy $\pol$ we define
\[
V^\pol_{\mdp, t}(s) \defn \E^\pol_{\mdp, t} \left[ U_t ~\middle|~ s_t = s \right]
\]
**** The state-action value function
\[
Q^\pol_{\mdp, t}(s, a) \defn \E^\pol_{\mdp, t} \left[ U_t ~\middle|~ s_t = s, a_t = a \right]
\]
**** The optimal value functions
For an optimal policy $\pol^*$
\[
V^*_{\mdp, t}(s) \defn V^{\pol^*}_{\mdp, t}(s) \geq V^\pol_{\mdp, t}(s),
\qquad
Q^*_{\mdp, t}(s,a) \defn Q^{\pol^*}_{\mdp, t}(s,a) \geq V^\pol_{\mdp, t}(s,a) 
\]
*** The Bellman equations
**** State value function
\begin{align*}
V^\pol_{\mdp, t}(s)
& \defn \E^\pol_{\mdp}[U_{t}\mid s_t = s] \\
& = \E^\pol_{\mdp}[r_t + U_{t+1}\mid s_t = s] \\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \E^\pol_{\mdp}[U_{t+1} \mid s_t = s]\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} \E^\pol_{\mdp}[U_{t+1} \mid s_{t+1} = j] \Pr^\pol_\mdp(s_{t+1} = j \mid s_t = s)\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} V^\pol_{\mdp, t+1}(j)  \Pr^\pol_\mdp(s_{t+1} = j \mid s_t = s)\\
& = \E^\pol_{\mdp}[r_t \mid s_t = s] + \sum_{j \in S} V^\pol_{\mdp, t+1}(j) \sum_{a \in A} P_\mdp(j \mid s, a) \pol(a_t \mid s_t).
\end{align*}
**** State-action value function
\begin{align*}
Q^\pol_{\mdp, t}(s)
&= \rho(s,a) +  \sum_{j \in S} V^\pol_{\mdp, t+1}(j) P_\mdp(j \mid s, a)
\end{align*}

*** Optimal policies

**** Bellman optimality condition
The value function of the optimal policy satisfies this:
\begin{align*}
V^*_{\mdp, t}(s)
& = 
\max_{a}  [\rho(s,a) +  \sum_{j \in S} V^*_{\mdp, t+1}(j) P_\mdp(j \mid s, a)]
\end{align*}
**** Dynamic programming 
To find $V^*, Q^*$, first initialise $V^*_{\mdp, T}(s) = \max_a \rho(s,a)$. 
Then for $t = T-1, T-2, \ldots, 1$:
\begin{align*}
Q^*_{\mdp, t}(s,a) &= \rho(s,a) +  \sum_{j \in S} V^*_{\mdp, t+1}(j) P_\mdp(j \mid s, a).\\
V^*_{\mdp, t}(s) &= \max_a Q^*_{\mdp, t}(s,a).
\end{align*}
**** The optimal policy
The optimal policy is deterministic with:
\[
a_t = \argmax_a Q^*(s_t, a)
\]

* Multi-player Games
** Introduction
*** Multi-agent decision making
- *Two* versus $n$-player games
- *Co-operative* games
- *Zero-sum* games
- General-sum games
- *Stochastic* games
- Partial information games
*** Prisoner's Dilemma
*** Prisoner's Choice

** Two-Player zero-sum Games
*** Extensive-form alternating-move games
- At time $t$:
- Player chooses action $a_t$, which is revealed.
- Player chooses action $b_t$.
- Player $a$ receives $\rho(a_t, b_t)$ and $b$ receives $-\rho(a_t, b_t)$.
The utility for each player is 
$U = \sum_t \rho(a_t, b_t)$.
*** Backwards induction for ZSG 
\begin{algorithmic}
\FOR {$t=T, T-1, \ldots, 1$} 
\STATE x
\ENDFOR
\end{algorithmic}
*** Normal-form simultaneous-move games
- Player $a$ chooses action $a$ in secret.
- Player $b$ chooses action $b$ in secret.
- Players observe both actions
- Player $a$ receives $U(a,b)$, and $b$ receives $-U(a,b)$. 

*** Linear Programming
*** The linear programming problem
Linear programming is a constrained minimisation problem where the objective and the constraints are both linear.
\begin{align*}
\min_x~ & \theta^\top x\\
\textrm{s.t.~} & c^\top x \geq 0.
\end{align*}
We can have

